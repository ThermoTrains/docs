<h3>Software</h3>
<p>
  Die Software des Thermo-Scanners ist mit modernen Paradigmen des Softwareengineerings gebaut. Einfachheit,
  Stabilität, Geschwindigkeit und vor allem Erweiterbarkeit waren die Kriterien, die wir beim Design immer wieder
  überprüft und umgesetzt haben.
</p>

<h4>Software Landschaft</h4>
<p>
  Um die vielfältigen Anforderung an die Entwicklung der Software abzudecken, haben wir über den Verlauf dieser Arbeit
  um unsere Hardware eine ganze Software-Landschaft aufgebaut.
</p>

<h5>Continuous Integration</h5>
<p>
  Zu einem guten Entwicklungsprozess gehört Continuous Integration. Da es für die verwendeten Technologien und
  Frameworks keine Gratisdienste gibt und wir selber keine Server besitzen, die diese Aufgabe übernehmen können,
  haben wir den Thermo-Scanner selber als Continuous Integration Server eingesetzt. Startet die Thermobox, wird
  automatisch der HEAD commit aus dem Git repository auf dem master Branch ausgecheckt. Dann werden die NuGet
  Dependencies aktualisiert. Somit kann anschliessend der Release Build durchgeführt werden. Alle Assemblies erhalten
  die korrekte Versionsnummer die im Repository in der Datei <code>SharedAssemblyInfo.cs</code> eingecheckt ist. Weil
  wir aber nicht andauernd die Version erhöhen möchten, um diese bauen zu lassen, hängen wir auch den Commit Hash hinten
  an. Dieser String kann mit folgendem Git Befehl erzeugt werden:
</p>

<figure>
  <pre><code>git describe --long --tags</code></pre>
  <figcaption>Version mit Git Commit Hash erzeugen Befehl</figcaption>
</figure>

<p>
  Dies erzeugt einen String wie diesen: <code>1.0.0-52-ge078b31</code>. Diese Versionsnummer ist deswegen wichtig,
  weil wir in den Logs auf diese zurückgreifen. Dazu mehr im Kapitel <a href="#thermobox-logging">Logging</a>.
</p>

<h5>Unit Testing</h5>
<p>
  Um die verschiedenen Komponenten und insbesondere die Bildanalyse Algorithmen zu entwickeln, haben wir, wo es Sinn
  macht, TDD (Test Driven Development) eingesetzt. Seit Visual Studio 2017 Update 5 gibt es in der Enterprise Edition
  (welche wir über Microsoft Imagine via Schulaccount beziehen können) Live Unit Testing. Dieses nette Feature
  erkennt live, welche Unit Tests von der Code-Änderung betroffen sind und führt diese nach einem Build automatisch
  aus. Das Resultat wird gleich links neben den Zeilennummern im Editor angezeigt.
</p>

<figure>
  <img src="images/thermoscanner/screenshot-live-unit-testing.png">
  <figcaption>Screenshot Live Unit Testing</figcaption>
</figure>

<p>
  Zu beachten ist, dass Visual Studio dabei sämtlichen Dateiänderungen folgt. In unserem Build wurde anfänglich
  jedes Mal die Datei <code>SharedVersionInfo.cs</code> generiert, welche die aktuelle Versionsnummer enthält. Dies hat
  den Build jedes Mal neu getriggert. Somit hat Visual Studio non-Stopp gebuilded und die Unit Tests ausgeführt. Nach
  dem hilfreichen Feedback auf eine <a
  href="https://developercommunity.visualstudio.com/content/problem/152749/live-unit-testing-rebuilding-non-stop.html">Frage
  in der Visual Studio Developer Community</a>, konnten wir aber auch diesen Issue beheben, in dem wir den Build nur
  triggern, wenn es die Datei noch nicht gibt. Zudem löschen wir diese zu Beginn eines Builds, damit sie dann auch
  wirklich neu generiert wird.
</p>

<h5 id="thermobox-logging">Logging</h5>
<p>
  Um auch später nachvollziehen zu können was in der Software passiert, ist ein umfängliches Logging unumgänglich.
  Wir setzen dazu log4net ein. Die Konfiguration des Loggings ist für alle Konfigurationen dieselbe und wird
  geteilt. Somit müssen kleine Anpassungen an der Konfiguration nicht überall nachvollzogen werden. Die Log Dateien
  werden pro Komponente in eine Datei im Pfad <code>C:\Thermobox\logs\</code> abgelegt. Eine Zeile im Log enthält
  folgende Informationen:
</p>

<ul>
  <li>Datum</li>
  <li>Uhrzeit auf die Millisekunde genau</li>
  <li>Assembly Name</li>
  <li>Version</li>
  <li>Git Commit Hash</li>
  <li>Log Level</li>
  <li>Nachricht</li>
</ul>

<p>
  Eine Log Zeile sieht dann als Beispiel so aus:
</p>

<figure>
  <pre><code>2017-11-20 09:06:44,724 TemperatureReader.exe-1.0.0-1-gb652480
    [INFO] Received message on channel cmd:capture:start: 2017-11-20@09-06-44</code></pre>
  <figcaption>Beispiel Log Zeile</figcaption>
</figure>

<p>
  Wird eine Log Datei zu gross, wird diese automatisch umbenannt und es wird eine neue Datei begonnen. Wir haben ein
  Threshold von 10 MiB gewählt. Dieser wurde eigentlich nur von der VisibleLightReader-Komponente erreicht, weil wir
  dort Debug Logs einsetzen, um den Detektionsalgorithmus laufend zu verbessern. Gesamthaft haben wir über die zwei
  Testphasen ca. 154 MiB an Logdateien gesammelt.
</p>

<p>
  Auf einer Logzeile wird der Assembly Name aufgeführt. Dies mag redundant erscheinen, weil dieser ja auch bereits in
  der Logdatei enthalten ist. Allerdings starten mit dem Start Script alle Komponenten miteinander und lassen diese
  auch auf dieselbe Konsole schreiben, damit man nicht 6 verschiedene Fenster offen haben muss um diese zu überwachen.
  Im Normalbetrieb starten die Komponenten im Hintergrund. Somit hätte man eigentlich keine Möglichkeit die Logs
  aggregiert zu sehen. Man kann höchstens ein <code>tail</code> auf die einzelnen Dateien machen. Also haben wir ein
  PowerShell Script geschrieben, dass sämtlichen Logdateien folgt. Zudem färbt es die verschiedenen Loglevels auch
  gleich ein. Also erscheint eine Exception auch sofort rot. Die Entwicklung des Scripts war nicht ganz einfach, weil
  pro Logdatei ein Thread gestartet werden muss, der einer einzelnen Logdatei folgt und dessen neuen Logs zurück auf die
  Hauptkonsole schreibt. Diese Lösung könnte eventuell auch für andere Projekte interessant sein.
</p>

<p>
  Um das Logging muss man sich bei einem Wiederbetrieb der Kabine als erstes nicht kümmern. Logs werden nur lokal
  abgelegt. Auch die Scripts zur aggregierten Ansicht der Logs nehmen zusätzliche Komponenten Logs automatisch
  auf.
</p>

<h5>Remote Access</h5>
<p>
  Um aus der Ferne auf den Mini-PC und die Hardware zu zugreifen, haben wir einen Teamviewer Server installiert. Dieser
  ermöglicht es mit einer Partner ID und einem Passwort darauf zuzugreifen. Teamviewer bietet das beste
  Benutzererlebnis verschiedenster Remote Access Lösungen. Auf die Standard-Windows-Remote-Desktop-Lösung können wir
  nicht zugreifen, weil der Server direkt mit einer öffentlichen IP im Internet steht, welche nicht statisch ist.
  Teamviewer darf für nicht kommerzielle Zwecke gratis eingesetzt werden. Sobald die SBB das Projekt weiterführt,
  müsste ein Lizenz angeschafft werden, sofern noch kein besteht.
</p>

<h5>Hardware Monitoring</h5>
<p>
  Wir stellen unsere Hardware an einem Ort auf, an dem es externe Einflüsse gibt, die wir nicht unter Kontrolle haben.
  So kann z.B. jemand den Kasten ausstecken, Regen kann hineingelangen und einen Kurzschluss erzeugen oder er
  verliert die Netzwerkkonnektivität. Wir möchten also überwachen können, ob die Thermobox läuft. Also brauchen wir
  ein Tool zur Überwachung der Uptime.
</p>

<p>
  Weiter möchten wir die Temperatur der Hardware überwachen, um eventuelle konstruktionsbedingte Wärmestaus frühzeitig
  zu erkennen, bevor etwas anfängt zu schmelzen. Die Hardware ist draussen und kann nur in den vorgegebenen Temperaturen
  betrieben werden. Durch eine Fehlfunktion des Heizlüfters oder Thermostats könnte das Innere des Gehäuses auch
  überhitzen. Oder der Mini PC erzeugt so viel Hitze, dass diese nicht abgebaut werden kann. Einen Lüftungsschlitz nach
  aussen gibt es nämlich nicht. Der einzige undichte Ort ist der Kabelschlitz im Boden des Gehäuses. Also möchten wir
  die Temperatur des Mini-PCs überwachen können.
</p>

<p>
  Um Erkenntnisse über die Performance der Software zu erlangen und eventuell zu erkennen, dass das System
  überlastet ist, möchten wir auch die CPU Last sowie den Memory Verbrauch analysieren können. Die
  Performance-intensivsten Abläufe sind folgende:
</p>

<ul>
  <li>Bildanalyse ob ein Zug im Bild ist</li>
  <li>Aufnahme Full HD Farbvideo mit Live H.264 Kompression</li>
  <li>Aufnahme Infrarot Bilder</li>
  <li>Clamping Infrarot Bilder von 16 bits zu 8 bits</li>
  <li>Nachträgliche H.264 Kompression der Infrarot Bilder</li>
</ul>

<p>
  Diese Abläufe können sich auch in die Quere kommen, was wir erkennen möchten. Zudem möchten wir erfahren, wann der
  Arbeitsspeicher ausgeht und eventuell auf die Disk geswapped wird.
</p>

<p>
  Um diese Daten aufzunehmen, haben wir nach einer Monitoring-Lösung gesucht. Dazu gibt es diverse Self-Hosting
  Lösungen wie z.B. das alt bekannte Nagios, wovon es mittlerweile auch schon diverse Forks gibt. Diese Lösungen
  können wir allerdings nicht selber betreiben, weil uns die Server dazu fehlen. Ausserdem ist ein solches System
  dafür ausgelegt eine ganze Serverlandschaft zu überwachen und nicht nur einen einzigen. Deswegen ist die
  Installation und Konfiguration einer solchen Lösung sehr aufwändig. Diesen Aufwand wollten wir uns, wenn möglich,
  ersparen.
</p>

<p>
  Die andere Option: Eine SaaS Monitoring-Lösung verwenden. Das Problem hierbei ist, dass es keine Anbieter
  gibt, die das gratis machen. Oder jedenfalls nicht so, dass wir mehrere Monate lang auf den Server zugreifen können.
</p>

<p>
  Deswegen haben wir uns entschlossen, eine eigene Lösung zu schaffen, die wir auf dem Server von Sebastian Häni hosten
  können. Die entworfene Lösung ist so simpel gehalten wie möglich und ist nicht sehr erweiterbar. Die Funktionsweise
  ist so, dass die Thermobox alle 5 Minuten ein Datentupel im JSON Format mit folgenden Werten an den Server per HTTP
  POST schickt:
</p>

<ul>
  <li>CPU Temperaturen in Celsius pro Core</li>
  <li>CPU Auslastung in Prozent pro Core</li>
  <li>Memory Auslastung in Prozent</li>
  <li>Freier Festplattenspeicher in Bytes</li>
  <li>Hostname</li>
</ul>

<p>
  Der Server hängt das Datentupel an eine Datei. Das Frontend liest dann diese Datei aus und erzeugt eine HTML Seite
  die vier Graphen im Browser darstellt. Oben links wird dargestellt, ob die Box online ist. Oben rechts werden die
  CPU Auslastung sowie die CPU Temperatur überlagert dargestellt. Unten links sieht man die Memory Auslastung und
  unten rechts den Verlauf des freien Speicherplatzes.
</p>

<figure>
  <img src="images/thermoscanner/monitoring-screenshot.jpg">
  <figcaption>Screenshot Monitoring</figcaption>
</figure>

<p>
  Weil es ab und zu vorkam, dass die Software nicht so funktionierte wie gewünscht, hat sich die Festplatte gefüllt
  und es konnten keine Aufnahmen mehr gemacht werden. Bevor dies der Fall ist, schickt der Server eine E-Mail an
  uns mit der Information, dass weniger als 5 GiB freier Speicherplatz zur Verfügung steht.
</p>

<figure>
  <img src="images/thermoscanner/screenshot-monitoring-email.png">
  <figcaption>Screenshot Monitoring E-Mail-Benachrichtigung</figcaption>
</figure>

<p>
  Die Anforderungen an den Monitoring-Server sind folgende:
</p>

<ul>
  <li>PHP 7+</li>
  <li>Schreibberechtigung auf <code>data/stats.txt</code></li>
</ul>

<p>
  Um den Server in Betrieb zu nehmen, müssen nur die Sourcen auf den Webserver kopiert und die
  Schreibberechtigung eingerichtet werden. Im <code>ping.ps1</code> Script muss die richtige URL konfiguriert werden
  damit die Datentupel an den richtigen Ort gelangen. Es gibt keinen Mechanismus der die alten Daten löscht. Dazu muss
  manuell die Datei <code>stats.txt</code> geleert werden. Die Graphen stellen jeweils nur die letzten 48 Stunden
  dar. Details über ältere Posts können mit dem Button "Show more" angezeigt werden.
</p>

<p>
  Der Client, also in unserem Fall der Thermo-Scanner, führt das Script <code>ping.ps1</code>, welches ebenfalls im
  Repository zu finden ist, alle 5 Minuten aus. Dieser allgemein als Cron-Job bekannte Prozess kann auf dem Mini-PC,
  auf welchem Windows 10 läuft, mit dem Task Scheduler gesteuert werden. Folgend zwei Screenshots wie der Task
  konfiguriert wurde:
</p>

<figure>
  <img src="images/thermoscanner/monitoring-task-configuration-1.png">
  <figcaption>Konfiguration Monitoring Ping Task Generell</figcaption>
</figure>

<figure>
  <img src="images/thermoscanner/monitoring-task-configuration-2.png">
  <figcaption>Konfiguration Monitoring Ping Task Trigger</figcaption>
</figure>

<p>
  Im Register Actions wurde eine Action hinzugefügt. Diese ist vom Typ "Start a program". Das Programm ist
  <code>powershell</code> und die Argumente sind
  <code>-NoLogo -File C:\repos\thermotrains\src\main\csharp\Scripts\ping.ps1 -WindowStyle Hidden"</code>.
</p>

<p>
  Das Ping Script <code>ping.ps1</code> ist in PowerShell geschrieben. Es liest die CPU Temperaturen, CPU
  Auslastungen und die Memory Auslastung vom OpenHardwareMonitor aus. Um diese Angaben zuverlässig und einfach
  auslesen zu können, gibt es von Windows leider keine vorgegebenen Befehle oder Services. Das Auslesen des
  verbleibenden Speicherplatzes ist jedoch mit PowerShell Hausmitteln möglich.
</p>

<p>
  Eine generell funktionierende Lösung, um die Sensoren der Hardware auszulesen, gibt es nicht. Jeder Hersteller
  programmiert in seinen Treibern eigene Interfaces, um die Werte auszulesen. Die Software OpenHardwareMonitor versucht
  diese verschiedenen Interfaces zusammen zu tragen und stellt diese im einfachen Tool dar. Zusätzlich bietet
  OpenHardwareMonitor die Option an, sich beim Systemstart selber im Tray auszuführen. Dies haben wir auf dem Mini-PC
  so konfiguriert. Wenn OpenHardwareMonitor läuft, kann über die angebotenen WMI Objekte (Windows Management
  Instrumentation) auf die Sensorwerte mit simplen Queries zugegriffen werden. Als Beispiel, hier ein Befehl welcher
  alle CPU Kern Temperaturen in Kelvin ausliest:
</p>

<figure>
  <pre><code>$temp = Get-WmiObject -Namespace "root/OpenHardwareMonitor" Sensor `
  | Where-Object {$_.SensorType -eq 'Temperature' -and $_.Name -like 'CPU Core #*'} `
  | Select-Object Value `
  | ForEach-Object {$_.Value}</code></pre>
  <figcaption>CPU Temperatur Auslesen PowerShell Script</figcaption>
</figure>

<figure>
  <img src="images/thermoscanner/screenshot-openhardwaremonitor.png">
  <figcaption>Screenshot OpenHardwareMonitor</figcaption>
</figure>

<h5>Error Monitoring</h5>
<p>
  In den jeweiligen Komponenten können Errors, Warnings oder gar nicht abgefangene Exceptions
  auftreten. Um diese nicht umständlich im Log suchen zu müssen, sollen diese einfacher eingesehen werden können. Zudem
  sollte bei neuen Fehler gleich eine E-Mail-Benachrichtigung versendet werden.
</p>

<p>
  Dazu haben wir Sentry integriert. Sentry ist ein Dienst, welcher selber gehostet werden kann oder auf sentry.io
  betrieben wird. Da die Gratis-Lizenz mit ihren Einschränkungen für uns ausreicht, müssen wir keine Kosten
  aufwenden. Wenn eine Warnung oder Fehler auftritt, wird dieses Event an die Sentry API gemeldet. Sentry aggregiert
  dann die Events zusammen und schickt bei neuen unentdeckten Events eine E-Mail an uns.
</p>

<figure>
  <img src="images/thermoscanner/screenshot-sentry.png">
  <figcaption>Screenshot Sentry</figcaption>
</figure>

<p>
  Der Dienst ermöglicht es uns die Fehler sofort zu beheben nachdem sie aufgetreten sind. Zudem können wir die
  Warnungen dazu benutzen, um zu sehen wie häufig und wann gewisse Meldungen auftauchen. Zum Beispiel schicken wir
  immer eine Warnung, wenn ein Zug entdeckt, wurde dieser aber "anscheinenden" nach zu kurzer Zeit wieder aus dem Bild
  gefahren ist, dass das eigentlich gar nicht möglich ist. Wenn wir dann sehen, dass dies immer während der Dämmerung
  passiert ist das ein guter Hinweis, was womöglich mit der Bildanalyse schiefläuft.
</p>

<p>
  Um Sentry einzubinden, mussten wir einen eigenen log4net Appender bauen. Die existierenden Appender sind zu alt und
  werden nicht mehr gewartet. Sollte sich in naher Zukunft nichts mehr in diesen Projekten tun, erwägen wir unseren
  Appender selber als NuGet Package zu veröffentlich, damit andere auch davon profitieren können.
</p>

<p>
  Der Sentry Account müsste im kommerziellen Betrieb von der SBB neu erstellt werden. Danach kann im Projekt ein
  neuer DSN (Data Source Name) generiert werden. Dieser muss dann auf dem Mini PC in der Umgebungsvariable mit dem
  Namen <code>SENTRY_DSN</code> abgelegt werden.
</p>

<h4>Interne Kommunikation</h4>
<p>
  Durch die Anbindung von verschiedenen Komponenten und die verteilte Entwicklung sowie möglicher Integration von
  weiteren Komponenten je nach Projektverlauf haben wir uns für eine Microservice-Architektur entschieden. In einer
  solchen Architektur braucht es in der Regel einen Message Broker beziehungsweise eine Message Queue. Um dies
  umzusetzen, haben wir auf Redis gesetzt. Der Redis Server läuft ständig auf dem Mini PC und fährt automatisch hoch
  sobald Windows startet.
</p>

<p>
  Redis kann zwar mehr als nur Broker spielen, wir brauchen diese Features allerdings nicht. Redis zeichnet sich aus
  durch die Einfachheit sowie die grosse Anzahl an production ready Bibliotheken zur Integration in alle gängigen
  und zum Teil auch nicht gängigen Technologien. Ausserdem gibt es ein Command Line Interface mit welchem bequem
  manuell Nachrichten in die Queue geschrieben und diese auch ausgelesen werden können.
</p>

<p>
  Der Einsatz von Redis bringt also das Pattern PubSub (Publish/Subscribe) hervor. Die entwickelten Komponenten
  laufen als eigenständige Prozesse und wissen nicht welche anderen Prozesse laufen. Will eine Komponente mit einer
  anderen kommunizieren, muss sie die Nachricht in die Redis Message Queue schreiben und somit publishen. Will sie
  Nachrichten lesen, muss sie sich auf eine Queue subscriben.
</p>

<p>
  Um auf der Entwicklungsumgebung nicht den Redis Server installieren zu müssen, gibt es ein Redis Docker Image
  welches den Server in einem Container startet. Somit kann eine Instanz ohne Konfiguration on Demand gestartet werden.
  Auf dem Mini PC haben wir zuerst auch mit Docker den Redis Server eingerichtet, weil die Windows Version des Redis
  Servers nicht offiziell von Seite Redis unterstützt wird. Jedoch ist zum Zeitpunkt dieser Arbeit Docker immer noch
  nicht sehr stabil, um Container auf einer Serverplattform laufen zu lassen. Für eine Entwicklungsumgebung ist es ok,
  aber auch nicht ideal. Deshalb haben wir schliesslich den Redis Server, der Microsoft offiziell unterstützt, als
  Windowsdienst installiert.
</p>

<p>
  Während der Entwicklung der Komponenten haben wir uns schlussendlich auf folgende Menge an Befehlen geeinigt.
</p>

<ul>
  <li><code>capture:start {{timestamp}}</code></li>
  <li><code>capture:stop</code></li>
  <li><code>capture:abort</code></li>
  <li><code>capture:pause</code> (obsolet)</li>
  <li><code>capture:resume</code> (obsolet)</li>
  <li><code>delivery:upload {{filename}}</code></li>
  <li><code>delivery:compress {{filename}}</code></li>
  <li><code>kill</code></li>
</ul>

<p>
  Im Diagramm unten ist dargestellt, wie eine Aufnahme des Zuges auf der Kommunikationsebene abläuft. Es sind sämtliche
  Nachrichten ersichtlich. Wie man sieht, geht jede Nachricht immer zuerst an den Redis Server, welche sie dann weiter
  an die Komponenten, die sich darauf subscribed haben, weiterleitet. Die einzelnen Komponenten werden später erklärt.
</p>

<figure class="full">
  <img src="images/thermoscanner/aufnahme-sequenz-diagramm.png">
  <figcaption>Sequenzdiagramm Aufnahme</figcaption>
</figure>
<!--
To generate the diagram: Go to https://www.websequencediagrams.com/ and paste the following:

title Aufnahme Sequenz

Trigger->Redis: capture:start "1"
Redis->IRReader: capture:start "1"
Redis->VisibleLightReader: capture:start "1"
Redis->TemperatureReader: capture:start "1"

Trigger->Redis: capture:stop
Redis->IRReader: capture:stop
Redis->VisibleLightReader: capture:stop
Redis->TemperatureReader: capture:stop

IRReader->Redis: delivery:compress "1-ir.seq"
Redis->IRCompressor: delivery:compress "1-ir.seq"
IRCompressor->Redis: delivery:upload "1-ir.mp4"
VisibleLightReader->Redis: delivery:upload "1-visible.mp4"
TemperatureReader->Redis: delivery:upload "1-temperature.txt"

Redis->Uploader: delivery:upload "1-ir.mp4"
Redis->Uploader: delivery:upload "1-visible.mp4"
Redis->Uploader: delivery:upload "1-temperature.txt"
-->

<h4>Komponenten Architektur</h4>
<p>
  Damit es einfach ist neue Komponenten zu erstellen, haben wir ein Common-Modul erstellt welches Code bereitstellt,
  der für alle Komponenten derselbe ist. Im Common Modul gibt es eine abstrakte Klasse
  <code>ThermoBoxComponent</code> von welcher konkrete Komponenten erben können. Eine Komponente kann dann definieren,
  welchen Kanälen sie folgen möchte und welchen Handler sie anstossen möchte, wenn eine Nachricht eintritt. Zudem gibt
  es eine Funktion um Nachrichten zu publishen.
</p>

<p>
  Die gewählte Architektur hat sich sehr bewährt. Es ermöglicht den selektiven Betrieb einzelner Komponenten so, dass
  diese isoliert getestet werden können. Zudem kann man auch mal eine Komponente ausschalten, wenn man das Feature
  gerade nicht braucht. Zum Beispiel können wir den Uploader nicht starten, wenn wir nicht möchten, dass Dateien auf den
  Server hochgeladen werden. Es ist keine Konfigurationsänderung notwendig.
</p>

<p>
  Das einzige Problem das wir mit dieser Architektur festgestellt haben ist, dass wir Kameras haben, auf welche mit
  nur einem Windows Prozess zugegriffen werden kann. Die erste Idee war, dass wir eine Komponente haben, welche die
  Farbkamera ausliest, um zu erkennen, ob gerade ein Zug in die Waschstrasse fährt. Eine zweite Komponente greift auf
  dieselbe Kamera zu um die Bilder in ein Video aufzunehmen sobald der Zug erkannt wurde. Dies war leider nicht
  möglich. Also mussten wir in diesem konkreten und einzigen Fall vom Microservice-Pattern abweichen und beide
  Funktionalitäten in derselben Komponente implementieren.
</p>

<h4>Reader Komponenten</h4>
<p>
  Unter den Reader Komponenten verstehen sich Komponenten die eine Aufnahme tätigen oder eben einen Sensor auslesen.
  Die Reader Komponenten subscriben sich auf die Channels <code>capture:start</code>, <code>capture:stop</code> und
  <code>capture:abort</code>.</p>
<p>
  Beim Start Befehl wird ein Timestamp im Format <code>YYYY-MM-DD@HH-mm-ss</code>
  übergeben. Diesen Timestamp verwenden die Reader, um einen eindeutigen Dateinamen zu erzeugen. Wenn diese Dateinamen
  später asynchron über diverse Wege wieder zusammengetragen werden, können sie wieder zusammen gruppiert werden und
  man weiss zu welcher Aufnahme sie gehören. Würden die Reader selber den Timestamp erzeugen, ist durch die asynchrone
  Natur der Redis Message Queue nicht gegeben, dass dieselben Timestamps zum Zuge kommen.
</p>

<p>
  Beim Stopp Befehl beenden die Reader ihre Aufnahme und schicken die erzeugte Datei an den jeweiligen Channel weiter.
</p>

<p>
  Beim Abbruch-Befehl werden die Reader die Aufnahme abbrechen und alle angefallenen Dateien aufräumen. Ein Abbruch
  kann unter anderem stattfinden, wenn die Trigger Komponente sich dafür entscheidet die Aufnahme zu starten, weil sie
  glaubt, der Zug sei eingefahren. Später wird dann aber gemerkt, dass es ein Fehlalarm war und die Aufnahme
  abgebrochen werden sollte. Würde der Trigger warten bis er sich ganz sicher ist, könnte bereits ein Teil des Zuges
  durch gefahren sein ohne dass er aufgenommen wurde.
</p>

<p>
  Um manuelle Befehle zu auszuführen, haben wir ein PowerShell Script <code>Control.ps1</code> erstellt mit welchem
  interaktiv die möglichen Befehle abgesendet werden können.
</p>

<h5>IRReader</h5>
<ul>
  <li>
    <b>Subscription:</b>
    <ul>
      <li><code>capture:start</code></li>
      <li><code>capture:stop</code></li>
      <li><code>capture:abort</code></li>
      <li><code>capture:pause</code></li>
      <li><code>capture:resume</code></li>
      <li><code>kill</code></li>
    </ul>
  </li>
  <li>
    <b>Publish:</b>
    <ul>
      <li><code>delivery:compress</code></li>
      <li><code>delivery:upload</code></li>
    </ul>
  </li>
</ul>
<p class="affix">
  Die Aufgabe des IRReaders ist, die Bilder FLIR Kamera aufzunehmen. Bevor die eigentliche Komponente gestartet wird,
  wird zuerst die Kamera gefunden. Die Suche nach der Kamera geschieht asynchron. Um die Testingzyklen sowie
  grundsätzlich den Start der Komponente zu beschleunigen haben wir folgendes Verfahren entwickelt, um möglichst
  schnell mit der Kamera zu verbinden.
</p>

<p>
  Die Kommunikation zur Kamera handeln wir mit dem FLIR ATLAS SDK ab. Im Hintergrund verwendet das SDK einen GenICam
  Treiber für die Kommunikation. FLIR hat aber weitere proprietäre Parameter in die Kamera verbaut, weswegen wir nicht
  ein Standard GenICam-SDK verwenden.
</p>

<p>
  Zuerst wird ein Discovery Thread gestartet der in einem Event Handler auf gefundene Kameras reagiert. Wird die
  richtige Kamera gefunden, welche dem Suchstring enspricht, wird der originale Thread mit dem Resultat der Kamera
  beendet und retourniert. Die Kamera die gefunden werden soll, ist in der Konfiguration unter dem Key
  <code>IR_CAMERA_NAME</code> abgelegt. Für unsere Kamera lautet dieser "FLIR AX5". Wird die Kamera nicht innerhalb von
  5 Sekunden gefunden, verwenden wir die Emulator-Kamera, die das FLIR ATLAS SDK anbietet. Das heisst, das
  Discovery-Verfahren dauert maximal 5 Sekunden oder weniger.
</p>

<p>
  Die Emulator-Kamera ist hilfreich, wenn man die Komponente testen will ohne, dass man die Kamera im Netzwerk in
  Betrieb nimmt. Das erzeugte Bild ist ein Test-Infrarotbild bei welchem sich ein Balken auf und ab bewegt, sowie einige
  Noise-Pixel die sich zufällig verteilen. Diese Noise Pixel stellen sicher, dass jedes Frame andere Statistikwerte
  aufweist. So können Entwickler testen ob Minimum, Maximum, Median und so weiter für jedes Frame neu und richtig
  berechnet werden. Diese Werte sind wichtig um das Bild dem Betrachter mit einer sinnvollen Temperaturspanne anzeigen
  zu können. Macht die anzeigende Software keine laufende Anpassung der Spanne sind die Noise-Pixel nicht immer
  sichtbar.
</p>

<figure>
  <img src="images/thermoscanner/kamera-emulator.jpg">
  <figcaption>Bild des FLIR ATLAS SDK Kameraemulators</figcaption>
</figure>

<figure>
  <pre><code>2018-01-11 10:51:40,520 IRReader.exe [INFO] Discovering cameras
2018-01-11 10:51:45,640 IRReader.exe [WARN] Fallback to emulator camera
2018-01-11 10:51:47,125 IRReader.exe [INFO] Stopping discovery
2018-01-11 10:51:47,125 IRReader.exe [INFO] Connecting to camera: Camera Emulator
2018-01-11 10:51:48,229 IRReader.exe [INFO] Connecting to redis on localhost
2018-01-11 10:51:48,893 IRReader.exe [INFO] Connected to redis
2018-01-11 10:51:48,894 IRReader.exe [INFO] Subscribing to cmd:capture:start
2018-01-11 10:51:48,904 IRReader.exe [INFO] Subscribing to cmd:capture:stop
2018-01-11 10:51:48,905 IRReader.exe [INFO] Subscribing to cmd:capture:abort
2018-01-11 10:51:48,905 IRReader.exe [INFO] Subscribing to cmd:capture:pause
2018-01-11 10:51:48,906 IRReader.exe [INFO] Subscribing to cmd:capture:resume
2018-01-11 10:51:48,907 IRReader.exe [INFO] Subscribing to cmd:kill</code></pre>
  <figcaption>IRReader Startup Log</figcaption>
</figure>

<p>
  Die Infrarot Kamera wird in der Regel eine Betriebstemperatur von ca. 40 - 50° C einnehmen. Die Kamera hat eine
  interne Mappingtabelle sowie ein Kalibrierungsverfahren, um die Eigentemperatur zu kompensieren damit diese keine
  Einflüsse auf das aufgenommene Bild haben. FLIR nennt den Prozess NUC (Non Uniformity Correction). Ein NUC dauert
  ca. 0.2 - 0.5 Sekunden und blockiert die Aufnahme. Zeichnet man Bilder auf, hat man während eines NUCs ein schwarzes
  Bild. Die Standardkonfiguration der Kamera ist, dass alle 4 Minuten ein NUC gemacht wird um eventuelle sich
  veränderte Temperaturen zu kompensieren. Zusätzlich startet die Kamera den NUC, wenn sich die Eigentemperatur um
  einen einstellbaren Threshold verändert hat. Gerade wenn man die Kamera in einem Büro in Betrieb nimmt, wird sie
  anfänglich ca. jede Minute einen NUC durchführen bis sie die normale Betriebstemperatur erreicht hat. Der NUC ist
  übrigens hörbar, weil der Verschluss mechanisch betätigt wird. Es hört sich wie ein Klicken an.
</p>

<p>
  Wir möchten in unseren Aufnahmen keine schwarzen Frames haben. Deshalb wird unmittelbar bevor eine Aufnahme
  gestartet wird ein NUC durchgeführt. Dann können wir davon ausgehen, sofern die Kamera länger als ein paar Minuten in
  Betrieb ist, dass der nächste NUC erst in 4 Minuten durchgeführt wird. Der anfängliche NUC dauert auch nicht zu lange,
  so, dass wir eventuell sogar den Anfang verpassen würden. Die NUC Einstellungen können via die Parameter
  <code>NUCMode</code>, <code>NUCTempDelta</code> und <code>NUCIntervalFrames</code> eingerichtet werden.
</p>

<p>
  Die Aufnahme steuern wir über den FLIR ATLAS SDK Recorder. Dieser kann Videos von der Kamera aufnehmen. Diese
  werden im FLIR proprietären Format SEQ abgespeichert. Das SEQ Format ist nichts anderes als aneinandergehängte TIFF
  Dateien, welche mit Rohdaten der Kamera befüllt sind. Zusätzlich gibt es EXIF Metadaten. Da die Kamera mit 14 Bit
  aufnimmt und keine Kompression stattfindet, sind die resultierenden Dateien sehr gross. Es gibt keine Möglichkeit
  die Bilder sofort zu komprimieren.
</p>

<figure>
  <img src="images/thermoscanner/seq-filesize-formula.png">
  <figcaption>Formel zur Berechnung der SEQ-Dateigrösse</figcaption>
</figure>

<p>
  Nehmen wir in der vollen Auflösung und Framerate auf, werden die Dateien schnell so gross, dass sie gar nicht mehr
  auf der Disk Platz hätten. Deshalb haben wir die Framerate auf ein Frame pro Sekunde verringert. Die Auflösung zu
  verringern macht keinen Sinn, weil wir dann Bilddetails verlieren.
</p>

<p>
  Die Kontrolle der Kamera über das FLIR ATLAS SDK ist etwas umständlich. Die Control-Library ist nicht sehr stabil. Es
  kann zu zufälligen Exceptions kommen, wenn ein Befehl abgesetzt wird. Um Abhilfe zu schaffen, haben wir eine Retry
  Methode gebaut, der ein Callback zur Befehlsausführung mitgegeben wird sowie ein Callback zur Überprüfung ob es
  erfolgreich war. Diese Retry Methode führt den Befehl so lange wiederholt aus, bis keine Exception geworfen wird
  und der zweite Callback sagt, dass es funktioniert hat. Leider können wir nicht in den Code hineinschauen, weil er
  proprietär und obfuscatet ist.
</p>

<figure>
  <pre><code>/// &lt;summary&gt;
/// Tries to execute an action. If an exception is thrown, it tries again until a
/// threshold is reached.
/// &lt;/summary&gt;
/// &lt;param name="action"&gt;Action with potential exception thrown&lt;/param&gt;
/// &lt;param name="testSuccess"&gt;Function to test the success of the action&lt;/param&gt;
private static void Retry(Action action, Func&lt;bool&gt; testSuccess)
{
  var tries = 0;
  const int maxTries = 5;

  while (tries &lt; maxTries)
  {
    try
    {
      action.Invoke();

      if (testSuccess.Invoke())
      {
        return;
      }

      throw new Exception("Camera state was not as expected");
    }
    catch (Exception ex)
    {
      Log.Warn("Exception while executing camera command", ex);
    }

    tries++;
    Log.Warn($"Failed executing camera command (try {tries} of {maxTries})");
    Thread.Sleep(10);
  }

  Log.Error($"Could not execute command after {tries} tries.");
}</code></pre>
  <figcaption>FLIR ATLAS SDK Code zur wiederholten Befehlsausführung</figcaption>
</figure>

<p>
  Wird die Aufnahme gestoppt, erzeugt der IRReader drei Dateien.
</p>

<ul>
  <li>SEQ Datei: Video-Rohdaten</li>
  <li>JPG Datei: Snapshot des ersten Frames des Videos</li>
  <li>JSON Datei: Aktuelle Device Parameter</li>
</ul>

<p>
  Die SEQ Datei wird via <code>delivery:compress</code> an den IRCompressor übergeben, welcher die Datei komprimiert,
  damit dieser übertragen werden kann. Das Übertragen der Rohdaten ist erstens zu langsam, weil Züge schneller
  vorbeifahren als wir hochladen könnten und zweitens haben wir nicht genügend Serverspeicherplatz im die Dateien
  abzuspeichern. Zu beachten ist, dass die SEQ Datei nur die Sensorwerte enthält. Es sind noch keine Temperaturwerte
  erzeugt worden. Um diese zu berechnen müssen weitere Parameter hinzugenommen werden.
</p>

<p>
  Die JPG Datei dient uns zum Vergleich des komprimierten Videos. Damit sehen wir, ob der Kompressionsalgorithmus
  keine offensichtlichen Fehler beinhaltet. Weiter dauern das Kompressionsverfahren sowie der Upload einige Zeit.
  Möchten wir trotzdem schon sehen was aufgenommen wurde, müssten wir warten. Dieser Snapshot kann allerdings sehr
  schnell und ohne langwierige Kompression hochgeladen werden. So sehen wir direkt, ob und welcher Zug sich im Bild
  befindet.
</p>

<p>
  Die JSON Datei enthält die 315 verfügbaren Geräteparameter der FLIR A65 Kamera die zur Zeit der Aufnahme
  konfiguriert waren. Je nach Einstellung resultieren andere Bilder. So gibt es zum Beispiel einen High Gain oder Low
  Gain Modus. Diese Angaben dienen uns als Kontext bei der Interpretation und Weiterverarbeitung der aufgenommenen
  Bilder. Wir speichern diese Informationen bei jeder Aufnahme mit ab. Enthalten sind auch Sensorinformationen der
  Kamera, wie z.B. Gehäuse- oder Sensortemperatur. Zur Serialisierung der Kamera-Parameter haben wir eigene Modelklassen
  geschrieben die wir befüllen und dann mit <a href="#ref-json-net">Json.NET</a> serialisieren.
</p>

<figure>
  <pre><code>{
  "Parameters": [{
    "Name": "SensorTemperature",
    "Description": "FPA temperature in Celsius",
    "Value": "28.4"
  }, ...]
}</code></pre>
  <figcaption>Ausgelesen Device Parameter in JSON Format</figcaption>
</figure>

<p>
  Hier ein Auszug aus den Parametern die für uns zum Teil relevant sind:
</p>

<figure>
  <code>DeviceVendorName, DeviceModelName, DeviceManufacturerInfo, DeviceVersion, DeviceID, DeviceUserID,
    DeviceScanType, DeviceTemperatureSelector, DeviceTemperature, CameraFirmwareVersion,
    SensorFirmwareVersion, SensorResolution, SensorFocalLength, SensorFrameRate, SensorTemperature,
    HousingTemperature, CameraOptions, SensorWidth, SensorHeight, SensorDigitizationTaps, WidthMax, HeightMax, Width,
    Height, OffsetX, OffsetY, BinningHorizontal, BinningVertical, DecimationHorizontal, DecimationVertical, PixelFormat,
    PixelCoding, PixelSize, PixelColorFilter, AcquisitionMode, AcquisitionFrameCount, LineSelector,
    LineMode, UserOutputSelector, LineDebounceFactor, SyncMode, SensorGainMode, NUCMode, NUCTempDelta,
    NUCIntervalFrames, BaudRate, ShutterPosition, CorrectionMask, R, Spot, B, F, O, FNumber,
    Transmission, OInt, TemperatureLinearMode, TemperatureLinearResolution, RThg, J1, OAShift, ObjectEmissivity,
    ReflectedTemperature, WindowTransmission, WindowTemperature, AtmosphericTransmission, AtmosphericTemperature,
    SensorVideoStandard, ImageAdjustMethod, VideoOrientation, PlateauLevel, MaxAGCGain</code>
</figure>

<h5>VisibleLightReader</h5>
<ul>
  <li>
    <b>Subscription:</b>
    <ul>
      <li><code>capture:start</code></li>
      <li><code>capture:stop</code></li>
      <li><code>capture:abort</code></li>
      <li><code>capture:pause</code></li>
      <li><code>capture:resume</code></li>
      <li><code>kill</code></li>
    </ul>
  </li>
  <li>
    <b>Publish:</b>
    <ul>
      <li><code>capture:start</code></li>
      <li><code>capture:stop</code></li>
      <li><code>capture:abort</code></li>
      <li><code>delivery:upload</code></li>
    </ul>
  </li>
</ul>
<p class="affix">
  Aus technischen Limitierungen erfüllt der VisibleLightReader zwei Aufgaben was gegen das Microservice Prinzip
  verstösst. Die Komponente liest zum einen den Bildstream der Basler Kamera, um festzustellen, ob sich ein Zug in das
  Bild hineinbewegt hat. Hat sie einen Zug detektiert, meldet sie mit <code>capture:start</code> dies an alle
  Komponenten weiter so wie auch sich selber. Selber startet sie nämlich dann auf den Befehl die Aufnahme in eine
  Videodatei. Der Name der Komponente kommt davon, dass wir damit die für das menschliche Auge sichtbaren Wellenlängen
  aufnehmen. Dass Licht und Wellenlängen nicht dasselbe ist, ist uns bewusst. Der Name ist also wissenschaftlich nicht
  ganz korrekt.
</p>

<p>
  Die Komponente benutzt zur Kommunikation das Basler Pylon SDK. Da die Firma Basler einer der führenden Hersteller
  von Industriekameras ist, ist ihr SDK auch weit aus durchdachter und stabiler als das Pendant von FLIR.
  Man merkt, dass FLIR einige Patterns vom Pylon SDK abgekupfert hat.
</p>

<p>
  Die technische Limitierung ist, dass man nur von einem Windows Prozess auf die über USB angeschlossene Basler
  Kamera zugreifen kann. Der Prozess blockiert die Kamera und kein zweiter kann mehr darauf zugreifen. Das heisst, wir
  mussten den Image Grab Loop so schreiben, dass die Bilder zum einen vom Erkennungsalgorithmus verarbeitet
  und zum anderen in eine Videodatei geschrieben werden. Das setzt natürlich voraus, dass die
  Erkennungsphase nicht länger dauert als die Aufnahme eines Frames. Ansonsten verlieren wir entweder Frames oder der
  Buffer wird unendlich gross.
</p>

<p>
  Das folgende Flow Chart erklärt, wie wir das gelöst haben. In der Implementation haben wir eine Klasse
  <code>Recorder</code> geschrieben, welche intern einen State hat, ob sie am Aufnehmen ist oder nicht. Der Image Grab
  Loop schickt die Bilder dann an eine Instanz der Klasse. Der Recorder weiss dann selber, ob er das Bild verwerfen
  oder aufnehmen soll.
</p>

<p>
  Die Prozesse, die gelb umrandet sind, werden später in einem weiteren Flow Chart erklärt. Den gesamten Flow in einem
  einzigen Flow Chart darzustellen, ist leider zu unübersichtlich.
</p>

<figure>
  <img src="images/thermoscanner/detection/flowchart-grab-loop.png">
  <figcaption>VisibleLightReader: Grab Loop Flow Chart</figcaption>
</figure>

<p>
  Zuerst finden wir die richtige Kamera mithilfe des Namens, welcher in der Konfiguration unter dem Key
  <code>VISIBLE_LIGHT_CAMERA_NAME</code> abgelegt ist. Dazu mussten wir glücklicherweise keinen asynchronen
  Discover-Service bauen. Im Pylon Framework gibt es einen synchronen Aufruf, der das Kameraobjekt liefert. Nach dem
  Öffnen konfigurieren wir die Kameraparameter für uns passend. Wir stellen ein, dass nur 1 Frame pro Sekunde
  aufgezeichnet werden kann. Das machen wir, weil wir ansonsten zu grosse Dateien erhalten, für welche wir kein
  Speicherplatz haben. Ausserdem haben wir in der Nacht sowieso nicht mehr als 1 FPS, weil die Beleuchtungszeit so
  hoch wird. Den Image Grabber stellen wir so ein, dass er immer das aktuellste Frame der Kamera holt. Somit baut
  sich kein versteckter Buffer unendlich auf. Den Grab Loop entwickeln wir wie erwähnt selber, weil wir mehr Kontrolle
  brauchen.
</p>

<p>
  In der Methode <code>HandleStateChange</code> verarbeiten wir Befehle die asynchron von extern hineingekommen sind.
  Das sind also Befehle der Message Queue.
</p>

<figure>
  <img src="images/thermoscanner/detection/flowchart-handle-state-change.png">
  <figcaption>VisibleLightReader: Handle State Change Flow Chart</figcaption>
</figure>

<p>
  Es geht nun weiter in den Detektionsalgorithmus. Dies ist eine eigene Klasse. Die 4 übergebenen Bilder von vorher
  sind bereits vorverarbeitet. Sie sind in Graustufen, verkleinert und enthalten nur den Ausschnitt des Bildes, auf
  welchem sich die Räder des Zuges befinden.
</p>

<figure>
  <img src="images/thermoscanner/detection/background.jpg">
  <figcaption>Gecroppter Background mit Gleis</figcaption>
</figure>

<figure>
  <img src="images/thermoscanner/detection/snapshot.jpg">
  <figcaption>Einfahrt Zug aufgenommen von Basler Kamera</figcaption>
</figure>

<p>
  Das Erkennen des Zuges mit dem ganzen Bild ist sehr schwierig, wie auch unnötig. Der Ausschnitt verändert sich nicht
  solange die Kabine nicht bewegt wird. Im gesamten Hintergrund gibt es ca. 10 weitere Gleise auf welchen Zügen
  vorbeifahren können. Zu Beginn haben wir den Ausschnitt nicht ausgeschnitten. Dass wir dann mehr aufnehmen, war uns
  klar. Da wussten wir aber noch nicht, dass wir an einer neuralgischen Stelle aufnehmen werden, wo jede Minute
  mindestens ein Zug im Hintergrund vorbeifährt.
</p>

<p>
  In der Detektionsklasse ist das Hintergrundbild abgelegt. Also das Bild der Kamera, auf welchem sich kein Zug
  befindet. Wir nennen es von nun an Background. Der Background wird beim Start sowie in regelmässigen Abständen immer
  wieder neu abgelegt. Wenn wir den Background ablegen, führen wir zuerst einen Blurfilter aus. Danach berechnen wir die
  arithmetische durchschnittliche Helligkeit. Wir speichern also nur einen Wert zwischen 0 und 255.
</p>

<p>
  Um auf dem Bild immer etwas zu sehen, müssen wir periodisch die Beleuchtungszeit korrigieren. In der Nacht
  haben wir Beleuchtungszeiten von einer Sekunde und am Tag von wenigen Millisekunden. Aber auch mit der
  Beleuchtungszeit von einer Sekunde haben wir, je nach eingeschalteten Flutlichtern, zu wenig gesehen. Deswegen weisen
  wir die Kamera an den Gain automatisch zu korrigieren. Diese automatische Korrektur dauert im Extremfall bis
  zu zwei Sekunden. Wir geben den Befehl zur automatischen Korrektur selber. Würden wir es der Kamera überlassen
  dies periodisch zu tun, könnten wir im falschen Moment eine Background-Momentaufnahme machen, was dann fälschlicherweise einen Zug detektiert.
</p>

<p>
  Auf dem folgenden Diagramm sieht man die von uns in der ersten Testphase aufgezeichneten Beleuchtungszeiten
  (links) und Gains (rechts) aggregiert auf einen Tagesverlauf.
</p>

<figure>
  <img src="images/thermoscanner/detection/exposure-gain.png">
  <figcaption>Basler Kamera Belichtungszeit und Gain</figcaption>
</figure>

<p>
  Haben wir alle diese State-Geschichten hinter uns, können wir die eigentlichen Bilder evaluieren.
  Nachfolgend das Flow Chart, mit den bis dort durchlaufenen Schritten:
</p>

<figure>
  <img src="images/thermoscanner/detection/flowchart-detection.png">
  <figcaption>VisibleLightReader: Detektion Flow Chart</figcaption>
</figure>

<p>
  Bisher haben wir also einen Background-Durchschnittswert und neu auch die Durchschnittswerte der aktuellen
  Bilder. Wir können jetzt vergleichen, ob der aktuelle Wert dunkler ist als der Backgroundwert. Zugräder sind nämlich
  dunkler als der Schotter und produzieren einen deutlich sichtbaren Einfall der Bildhelligkeit. Auf dieses einfache Verfahren
  erst mit der Zeit gekommen, nachdem viele andere Ansätze fehl schlugen. Die Umstände der echten Welt sind viel abstruser, als
  man sich das im Labor vorstellt. Der Kasten ist diversen Umständen ausgeliefert und muss damit umgehen können.
</p>

<ul>
  <li>Sonnenaufgang und Sonnenuntergang</li>
  <li>Regen und Schnee erzeugen Noise im Bild, welcher potentiell detektiert wird</li>
  <li>Wenn Schnee liegt, sieht das Bild anders aus</li>
  <li>Personen auf dem Fussweg vor der Kabine</li>
  <li>Regen kann an der Scheibe Tropfen bilden</li>
  <li>Nebel kann die Sicht verschlechtern</li>
  <li>Reflexionen vom Prime Tower können Kaustiken bilden</li>
  <li>In der Nacht vorbeifahrende Züge werfen Licht auf den Boden</li>
  <li>Unterschiedliche Fahrtrichtungen</li>
</ul>

<p>
  Unser erstes Verfahren war mit dem Backgroundbild die sich veränderten Pixel zu tracken und festzustellen, ob von
  links oder rechts ein genug grosses Objekt hineinfährt. Das hat schon gar nicht funktioniert, wenn man den ganzen
  Bildbereich analysiert. Man könnte zwar meinen man muss einfach einen genug hohen Raum festlegen. Anders gesagt
  die Bounding Box müsste eine gewisse Höhe haben. Nur steht im Hintergrund das Bahnviadukt von Zürich. Wenn
  gleichzeitig ein Zug auf einem der unteren Gleise und einem auf dem oberen Gleis durchfährt haben wir eine Bounding
  Box die genau gleich hoch ist wie ein Zug der durch die Waschstrasse fährt. Jetzt kann man sich fragen, wie häufig
  das vorkommt. Wir mussten feststellen, dass das ca. alle 5 Minuten passiert. Ansonsten hätte dieses Verfahren sehr
  gut funktioniert soweit wir es getestet haben.
</p>

<p>
  Nachdem wir den Analyseausschnitt auf das untere Gleis gelegt haben, mussten wir feststellen,
  dass in der Nacht vorbeifahrende Züge einen starken Lichteinfall auf den Boden werfen.
  Dies hat den Detektor eine Aufnahme auslösen lassen.
</p>

<figure>
  <img src="images/thermoscanner/detection/night-light.jpg">
  <figcaption>Von anderen Zügen einfallendes Licht</figcaption>
</figure>

<p>
  Wir mussten also ein einfacheres Verfahren entwickeln, welches nur die Helligkeit zum Background vergleicht.
  Mit fleissiger Überwachung des Detektionsalgorithmus und ständigem Nachschrauben sowie den praktischen Tools, die wir
  geschaffen haben, konnten wir die Thresholds ständig verbessern. Das nachfolgende Flow Chart zeigt auf was passiert,
  wenn der Threshold erreicht wurde oder nicht. Zur Übersicht wurde der End-State weggelassen. Wenn kein Pfeil von einem
  Kasten weggeht, endet der Flow dort.
</p>

<figure>
  <img src="images/thermoscanner/detection/flowchart-evaluate.png">
  <figcaption>VisibleLightReader: Evaluierungs-Flow Chart</figcaption>
</figure>

<p>
  Die Detektorklasse meldet die Befehle über C# Events an die Klasse die den Grab Loop beherbergt. Diese erzeugt dann
  asynchron den Befehl welchen sie an die Message Queue weiterleitet.
</p>

<p>
  Um den Detektionsalgorithmus herum entstanden eine ganze Menge an Konstanten die wir nach besten Kenntnissen
  vorkonfigurierten und dann während der Testphase ständig nachkorrigierten. Die aufgeführten Werte sind die
  schlussendlichen Resultate davon.
</p>

<figure>
  <table>
    <thead>
    <tr>
      <th>Konstante</th>
      <th>Beschreibung</th>
      <th>Einheit</th>
      <th>Wert</th>
    </tr>
    </thead>
    <tbody>
    <tr>
      <td>NoBoundingBoxBackgroundThreshold</td>
      <td>Solange sich nichts bewegt, wird der Background neu initialisiert</td>
      <td>Sekunden</td>
      <td>30</td>
    </tr>
    <tr>
      <td>MinTimeAfterExit</td>
      <td>Minimale Dauer zwischen zwei Aufnahmen</td>
      <td>Sekunden</td>
      <td>60</td>
    </tr>
    <tr>
      <td>MinTimeAfterEntry</td>
      <td>Minimale Dauer einer Aufnahme. Ist sie kürzer wird sie abgebrochen</td>
      <td>Sekunden</td>
      <td>60</td>
    </tr>
    <tr>
      <td>MaxRecordingDuration</td>
      <td>Maximale Dauer einer Aufnahme</td>
      <td>Minuten</td>
      <td>45</td>
    </tr>
    <tr>
      <td>AutoExposureTimeout</td>
      <td>Zeit, die die Kamera braucht, um die Belichtung einzustellen</td>
      <td>Sekunden</td>
      <td>2</td>
    </tr>
    <tr>
      <td>ExitThreshold</td>
      <td>Anzahl Bestätigungen, dass kein Zug mehr da ist</td>
      <td>Einheit</td>
      <td>8</td>
    </tr>
    <tr>
      <td>ExitScheduleTime</td>
      <td>Dauer des Auslaufens der Aufnahme (damit das Video das Wegfahren des Zuges aufnimmt)</td>
      <td>Sekunden</td>
      <td>20</td>
    </tr>
    </tbody>
  </table>
  <figcaption>Konstanten des Detektionsalgorithmus</figcaption>
</figure>

<p>
  Insgesamt sind wir mit der Erkennungsrate des Verfahrens zufrieden. Es detektiert zwar eindeutig zu viele Bilder als
  Zug, jedoch wird kein einziger Zug verpasst. Wenn man folgende Beispiele anschaut, ist es aber erstaunlich, wie gut es
  bei schlechten Sichtverhältnissen funktioniert.
</p>

<figure>
  <img src="images/thermoscanner/detection/rain-no-train.jpg">
  <figcaption>Fälschlicherweise Aufnahme gestartet</figcaption>
</figure>

<figure>
  <img src="images/thermoscanner/detection/rain-train.jpg">
  <figcaption>Aufnahme wurde gestartet</figcaption>
</figure>

<p>
  Die Erkennungsrate lag am Ende der ersten Testphase mit verbessertem Algorithmus bei 99%.
  27 Aufnahmen wurden gestartet und hochgeladen, auf welchen kein Zug war. In der zweiten Testphase
  wurden 178 von 178 Zügen erkannt. Die Anzahl falsch erkannter Züge lag bei 49. Als Grund dafür sehen wir das
  schlechte Wetter sowie die Wassertropfen, die sich auf dem Sichtglas festgesetzt haben. In einer
  neuen Version der Kabine wäre ein Glas, welches keine Tropfen ansetzt, sicher besser. Allenfalls würde auch
  ein kleines Dach Abhilfe schaffen (welches wir eigentlich ursprünglich bei Almatec bestellt hatten).
</p>

<p>
  Die Sensitivität liegt also bei 0.78, trotz dem schlechten Wetter mit Sturm Burglind. Die Spezifität ist
  schwierig zu beziffern, weil wir künstlich etwa ein Zeitintervall für richtig negativ erkannte Bilder festlegen
  müssten. Oder es müssten in sämtlichen Bildern Features als richtig negativ erkannt und gezählt werden, was unmöglich ist.
</p>

<p>
  Nachdem wir die erste Durchlaufwaschstrasse besucht haben, bemerkten wir, dass Züge zuerst an die Waschstrasse
  heranfahren und dann dort ein paar Minuten verweilen. Während dieser Zeit kommunizieren Sie mit dem
  Waschstrassenleiter oder bedienen die Steueranlage, je nach Typ der Anlage. Für uns heisst das, dass auf dem Video
  der Zug zu Beginn einfach steht und sich nicht bewegt. Das erzeugt unnötig grosse Videodateien. Deshalb hatten wir
  die Idee das Video zu pausieren und fortzusetzen, wenn sich der Zug in Bewegung setzt. Das hat bei gutem
  Wetter gut funktioniert. Wir haben die Bewegung auf dem Bild mit dem vorherigen Bild analysiert und konnten so sagen, ob sich etwas bewegt hat.
  Leider funktionierte das Verfahren bei Nacht nicht, da diverse Lichtquellen Bewegung
  verursachen. Dasselbe bei schlechtem Wetter, wenn Regentropfen oder Schnee ins Bild kommen. Deshalb haben
  wir die Pausierung und Fortsetzung wieder entfernt. Die Reader selber sind aber noch in der Lage zu pausieren und
  fortzusetzen, wenn ein manueller Befehl eintrifft.
</p>

<h5>TemperatureReader</h5>
<ul>
  <li>
    <b>Subscription:</b>
    <ul>
      <li><code>capture:start</code></li>
      <li><code>capture:stop</code></li>
      <li><code>capture:abort</code></li>
      <li><code>kill</code></li>
    </ul>
  </li>
  <li>
    <b>Publish:</b>
    <ul>
      <li><code>delivery:upload</code></li>
    </ul>
  </li>
</ul>
<p class="affix">
  Um unseren TEMPer2 Temperatursensor auszulesen, mussten wir das USB HID (Human Interface Device) konsumieren. Vom
  Hersteller gibt es nur eine Bedienersoftware ohne Code Beispiele oder Support für Entwickler. Glücklicherweise
  haben wir ein .NET Codebeispiel gefunden, welches jedoch etwas veraltet ist. Wir haben die Konstanten, die der
  Autor gefunden hat übernommen und den Code neu geschrieben. Im Artikel
  <a href="#ref-temper-in-csharp">Code Project: Temper USB Thermometer in C#</a> wurde allerdings von einem älteren
  TEMPer Gerät ausgegangen, welches nicht zwei Sensoren hat. Deswegen haben wir anfangs nicht gemerkt, dass wir
  eigentlich den falschen Sensor auslesen.
</p>

<p>
  Beim Befehl <code>capture:start</code> macht der TemperatureReader nichts anderes als sich den übermittelten
  Timestamp merken. Würde er nämlich bereits jetzt die Temperatur auslesen und mit <code>delivery:upload</code>
  weiterschicken, hat ein späterer Abbruch durch <code>capture:abort</code> keine Folgen mehr und die Datei wäre
  bereits auf dem Server. Erst bei <code>capture:stop</code> zeichnet der Sensor die Temperatur in eine Datei auf die
  er dann weiter schickt.
</p>

<h5>WeatherReader</h5>
<ul>
  <li>
    <b>Subscription:</b>
    <ul>
      <li><code>capture:start</code></li>
      <li><code>capture:stop</code></li>
      <li><code>capture:abort</code></li>
      <li><code>kill</code></li>
    </ul>
  </li>
  <li>
    <b>Publish:</b>
    <ul>
      <li><code>delivery:upload</code></li>
    </ul>
  </li>
</ul>
<p class="affix">
  In der zweiten Aufnahmephase haben wir uns entschieden den TemperatureReader nicht mehr zu verwenden, weil die
  Messdaten nicht brauchbar sind. Um die aufgenommenen IR Bilder trotzdem auswerten zu können haben wir den
  WeatherReader implementiert der aktuellen Wetterdaten von <a href="#ref-openweathermap">OpenWeatherMap</a>
  herunterlädt.
</p>

<p>
  OpenWeatherMap bietet eine gratis API an, um das aktuelle Wetter abzufragen. Um alte Daten abzufragen, müsste man
  bezahlen. Deswegen haben wir uns entschieden die Daten jeweils dann abzufragen, wenn die Aufnahme gemacht wird.
  Durch das müssen wir nicht bezahlen. Die Daten können über einen HTTP GET Request abgefragt werden:
</p>

<figure class="affix">
  <pre><code>http://api.openweathermap.org/data/2.5/weather?q=Zürich,CH&appid={openWeatherMapApiKey}</code></pre>
  <figcaption>OpenWeatherMap API Request URL</figcaption>
</figure>

<p>
  Die Antwort, die zurück kommt sieht dann ungefähr so aus:
</p>

<figure class="affix">
  <pre><code>{
  "coord": {"lon": 8.54, "lat": 47.37},
  "weather": [{
    "id": 802,
    "main": "Clouds",
    "description": "scattered clouds",
    "icon": "03n"
  }],
  "main": {
    "temp": 280.06,
    "pressure": 1016,
    "humidity": 71,
    "temp_min": 276.15,
    "temp_max": 284.15
  },
  "visibility": 10000,
  "wind": {"speed": 7.2, "deg": 240},
  "clouds": {"all": 40},
  "dt": 1514697600,
  "sys": {
    "sunrise": 1514704386,
    "sunset": 1514735106
  }
}</code></pre>
  <figcaption>OpenWeatherMap API Response</figcaption>
</figure>

<p>
  Der WeatherReader setzt eine Internetverbindung voraus. In der Zeit als das LTE Modul ausgefallen ist, hatten wir
  somit keine Wetterdaten sammeln können. Leider werden bei der kostenlosen Variante die Wetterdaten nur alle 2 Stunden aktualisiert.
  Etwas unschön ist, dass die Wetterdaten nur alle 2 Stunden aktualisiert werden, wenn man nicht
  zahlt.
</p>

<p>
  Der API Key wird auf dem Mini PC in der Umgebungsvariable <code>OPEN_WEATHER_MAP_API_KEY</code> abgelegt, damit
  dieser nicht in einer im Repository eingecheckten Datei gespeichert werden muss.
</p>

<h4>Delivery Komponenten</h4>

<h5>IRCompressor</h5>
<ul>
  <li>
    <b>Subscription:</b>
    <ul>
      <li><code>delivery:compress</code></li>
      <li><code>kill</code></li>
    </ul>
  </li>
  <li>
    <b>Publish:</b>
    <ul>
      <li><code>delivery:upload</code></li>
    </ul>
  </li>
</ul>
<p class="affix">
  Die Bilder von der FLIR Infrarotkamera können mithilfe des FLIR ATLAS SDKs leider nur als SEQ Datei abgespeichert werden.
  Diese Dateien sind zu gross, um sie in vernünftiger Zeit über eine Mobilfunkantenne zu übertragen.
  Eventuell käme es dann zu einem nicht aufholbaren Rückstand in der
  Übertragungsqueue. Der andere Grund ist der begrenzte Speicherplatz des Servers, auf den die Dateien übertragen werden.
  Dieser verfügt nur über 50 GiB Speicherplatz. Dies ist jedoch ein Soft-Limit wie wir bemerkt
  haben. Wir möchten also die SEQ Dateien in ein anderes Format komprimieren, um die Daten zu übertragen. Da es sich um
  aufeinander folgende Bilder handelt, würde sich ein Videoformat anbieten. Das proprietäre Format von FLIR SEQ mussten
  wir selber reverse engineeren, um die Inhalte zu verstehen und auslesen zu können. FLIR hat kein Interesse diese
  Innereien zu dokumentieren oder Gratissoftware anzubieten, die diese konvertieren kann. Es gibt nämlich Bezahltools von
  FLIR, die aber mehrere zehntausend Franken kosten. Deswegen haben wir es selber entwickelt.
</p>

<p>
  Das SEQ Format ist eigentlich nichts anderes als mehrere rohe TIFF Bilder hintereinander gehängt. Die FLIR Kamera
  nimmt Bilder mit 14 Bit Auflösung pro Pixel auf. Diese Pixel werden mit zwei Null-bits mit 16 Bit abgespeichert.
  Wir können mit dem FLIR ATLAS SDK auf diese rohen Daten lesend zugreifen. Allerdings erhalten wir ein Byte-Array.
  Das heisst, wir müssen dieses in ein ushort-Array umkonvertieren und jeweils zwei Bytes mergen. Das müssen wir für
  jedes Bild im Video machen. Jetzt haben wir Bilder mit 16 Bit pro Pixel in einem Format das wir weiterverarbeiten
  können. Das Problem ist nun, wenn wir aus diesen Graustufen Bildern ein Video machen möchten, müssten wir einen Codec
  finden der 16 bit Graustufen unterstützt. Das gibt es leider nicht in einem breit unterstützten Codec. Apple hat die
  <a href="#ref-apple-prores">ProRes Codec Familie</a> die bis zu 12 Bit sampeln kann. Wir müssen also die 16
  respektive 14 Bits auf die gängige Samplerate von 8 bits herunterbringen.
</p>

<p>
  Die Rohdaten aus der SEQ Datei sind absolute und linear verteilte Sensorwerte. Das heisst der Wert 0 entspricht dem
  niedrigsten Sensorwert und 16'384 (2<sup>14</sup>) entspricht dem höchsten. Wenn wir ein Zug aufnehmen, geht dessen
  Temperaturspanne nicht von 0 bis 16'384, sondern bewegt sich im Bereich von wenigen Celsius. Die kältesten Objekte
  sind so kalt wie die Aussentemperatur und die heissesten Objekte, die Bremsen und Achsen, bis zu 40°C. Züge fahren
  idealerweise nicht mit Vollgas auf die Waschstrasse zu machen eine Vollbremsung, was die Bremsen
  zum Glühen bringen würde.
</p>

<p>
  Mit dieser Erkenntnis haben wir ausprobiert, was es heissen würde, wenn wir die minimalen maximalen Pixelwerte auslesen
  und die Werte mit dem Offset "clampen". Nach dem Clamping nehmen wir die ganze Range und skalieren sie auf 8 Bit.
  Dabei können wir auch gleich den Präzisionsverlust berechnen.
</p>

<figure>
  <pre><code>Präzisonsverlust = Math.Max(0, (1 - (256f / (max - min))) * 100)</code></pre>
  <figcaption>Formel zur Berechnung des Präzisionsverlusts in Prozent</figcaption>
</figure>

<p>
  Sämtliche Verluste haben wir in diesem Histogramm beider Testphasen aufgezeigt. Die Y-Achse stellt die Anzahl Bilder
  dar.
</p>

<figure class="affix">
  <img src="images/thermoscanner/compression-precision-loss.png">
  <figcaption>IRCompressor Histogramm Präzisionsverlust in Prozent</figcaption>
</figure>

<p>
  Man sieht, dass der Verlust in der Regel zwischen 78% und 88% liegt. Wir gewinnen also nicht viel
  dabei. Man muss allerdings folgendes berücksichtigen. Die Kamera liefert im HighGainMode eine Temperaturgenauigkeit
  von 0.01°C (Temperaturspanne von 160 (-25° bis 135°C) geteilt durch 16'384). Bei einem Verlust von 100% können wir
  die Temperatur mit einer Genauigkeit von 0.625°C angeben. Bei einem Verlust von 80% bei <strong>0.125°C</strong>.
  Für unsere Auswertungen später ist diese Genauigkeit ausreichend.
</p>

<p>
  Nachdem wir die Bilder zu 8 Bit Graustufen konvertiert haben, können wir sie mit h.264 encodieren und erhalten somit
  eine sehr kleine Datei die auch direkt im Browser im Cockpit angeschaut werden kann. Den Offset- und
  Skalierungsfaktor den wir zum Clamping benutzt haben speichern wir in den Metadaten der MP4 Datei damit die SEQ
  Datei wieder rekonstruiert werden kann wenn auch mit Verlust.
</p>

<p>
  Ein Problem ergab sich während dem Betrieb. Wenn die aufgenommen SEQ Datei sehr gross war, hat der IRCompressor die
  CPU zu 100% ausgelastet um das Video zu encodieren. Das hatte zur Folge, dass der VisibleLightReader keine
  Aufnahmen mehr machen konnte, weil er nicht mehr genug schnell seine eigene Aufnahme encodieren konnte. Somit
  endeten diese Videos als schwarze Frames. Der Bildanalyseteil hat aber wie bis anhin funktioniert, weil das
  Betriebssystem dem Prozess genug Priorität gegeben hat. Das Encoding passiert jedoch in einem separaten Thread der
  über EMGU.CV zu OpenCV von FFmpeg gesteuert wird. Somit hatten wir keine Kontrolle über die Priorität dieser Threads.
  Als Gegenmassnahme haben wir zum einen mit dem C# Profiler den Code soweit wie möglich optimiert. Dies ergab ein
  merkbarer aber vernichtend kleinen Performance Boost. Wir haben uns dann entschlossen, die Framerate der IR Videos
  auf 1 Frame zu setzen. Danach konnten wir die Videos innerhalb einer Minute konvertieren. Da wir das Video sowieso
  nur zum Stitching verwenden, spielt dies keine Rolle. Wir haben es nicht von Beginn an so gemacht. Denn 30 FPS Videos
  bei Projektpartnern zu zeigen kommt besser an als ein Video mit 1 FPS. Die Hardwaelimitation hat uns also davon
  abgehalten.
</p>

<h5>Uploader</h5>
<ul>
  <li>
    <b>Subscription:</b>
    <ul>
      <li><code>delivery:upload</code></li>
      <li><code>kill</code></li>
    </ul>
  </li>
  <li>
    <b>Publish:</b>
    <ul>
      <li>keine</li>
    </ul>
  </li>
</ul>
<p class="affix">
  Die Uploader Komponente hört auf Nachrichten im Kanal <code>delivery:upload</code>. Die Dateien werden auf einen
  FTP Server hochgeladen und anschliessend in den Papierkorb verschoben. Die Zugangsdaten zum FTP Server sind in der
  Umgebungsvariable <code>THERMOBOX_FTP</code> abgelegt in der Form <code>username:password@server</code>.
</p>
<p>
  Die Dateien werden anschliessend nicht einfach gelöscht, sondern in den Papierkorb verschoben. Damit bietet sich
  die Möglichkeit Möglichkeit nachzuschauen, welche Dateien auf dem Mini PC existiert haben, falls vielleicht etwas mit
  dem Upload schiefgegangen ist. Auf Windows kann dann eingestellt werden, wie gross der Inhalt des Papierkorbs maximal
  sein darf. Ist das Limit überschritten, leert Windows automatisch die ältesten Dateien. So können wir den
  Festplattenspeicherplatz optimal ausnutzen und trotzdem sicherstellen, dass sie nie vollläuft. Ausser ein Programm
  erstellt eine einzige Datei, die den verfügbaren Speicherplatz alleine ausnutzt.
</p>

<p>
  Um Dateien von einer .NET Applikation in den Papierkorb zu verschieben, gibt es bedauerlicherweise kein einfaches
  Interface. Zum einen kann man ein Shell32 Objekt erstellen, welches auf den Papierkorb zeigt und die Datei dort hin
  verschieben. Damit das klappt, muss man aber zuerst in einem STA Thread (single-threaded apartment) sein, welches
  unsere Komponenten nicht sind.
</p>

<p>
  Die auf den FTP Server hochgeladenen Dateien lesen wir im Thermobox Cockpit aus und zeigen sie im Browser an.
  So kann live mitverfolgt werden, was gerade aufgenommen wurde.
</p>

<h4>CLI Utils</h4>
<p>
  Während der Projektarbeit sind wir immer wieder auf sich wiederholende Arbeiten gestossen, die wir automatisiert
  haben. Diese kleinen CLI Tools haben wir auch in die .NET Solution integriert damit diese auch vom gemeinsamen Code
  profitieren können.
</p>

<h5>SeqConverter</h5>
<p>
  Das SeqConverter CLI Programm dient zur Konvertierung von FLIR SEQ Dateien zu MP4 oder umgekehrt. Zudem kann es
  schwarzweisse MP4 Videos in Falschfarben Videos konvertieren.
</p>

<p>
  Im Modus <strong>SEQ zu MP4</strong> macht das CLI Tool genau dasselbe wie der IRCompressor.
</p>

<p>
  Im Modus <strong>MP4 zu SEQ</strong> werden die Metainformationen der MP4 Datei ausgelesen um die SEQ Datei zu
  rekonstruieren. Dies dient dazu, dass wir verifizieren können, ob unser Kompressionsverfahren funktioniert.
  Vorher und nachher sollten dieselben Temperaturwerte berechnet werden. Auch können wir so visualisieren, was der
  Präzisionsverlust effektiv ausmacht, indem wir die zwei Dateien in den FLIR Tools vergleichen können. Das FLIR ATLAS
  SDK bietet keine Möglichkeit an, SEQ oder auch nur einzelne Bilder mit Signaldaten zu erstellen, an. Das heisst
  wir mussten die SEQ Datei bit für bit selber erstellen. Damit wir das konnten mussten wir zuerst die essentiellen Tags
  der Metadaten finden, damit die SEQ Datei valide ist und von den FLIR Tools erkannt und gelesen wird. Mithilfe von
  ExifTool und einem Hex Editor und einer langen Nacht konnten wir das nötige Verständnis aufbauen.
</p>

<p>
  Als Einstiegspunkt diente das veraltete <a href="#ref-flir-interface">Spezifikationsdokument</a> von FLIR für einzelne
  Bilder mit angehängten Signaldaten. Das fehlende Puzzlestück war die Framerate des Videos. Ist diese gesetzt,
  erkennt FLIR Tools die Datei als Video. Fehlt sie, wird nur das erste Frame angezeigt. Die Kenntnis, welches Tag
  Feld und welchen Datentyp die Framerate hat, haben wir zurück ins ExifTool fliessen lassen. Phil Harvey, der
  Maintainer von ExifTool, hat kurz später eine neue Version erstellt die nun die Framerate von SEQ Dateien auslesen
  kann. Sebastian Häni wurde dankend in den Release Notes erwähnt.
</p>

<p>
  Das Verfahren zur Erstellung der SEQ Datei ist nun jedoch sehr einfach geraten nachdem wir das Format verstanden
  haben. Der Header der SEQ Datei enthält wichtige Informationen die von der Kamera respektive durch das FLIR ATLAS
  SDK geschrieben werden. Jedoch sind diese Informationen immer dieselben bei gleichbleibender Kamera. Das einzige
  was sich ändert sind dynamische Anzeigeparameter und Temperaturberechnungsparameter die in den FLIR Tools durch den
  Klick auf "Auto" sowieso neu berechnet werden. Wir können also getrost immer den binär identischen Header
  verwenden. Danach können wir jedes Frame einzeln an den Binärstream anhängen nachdem wir sie durch den
  Skalierungsfaktor geteilt und den Offset dazu addiert haben.
</p>

<p>
  Im Modus <strong>MP4 zu MP4</strong> muss ein weiterer Parameter zur Farbpalette angegeben werden. Der SeqConverter
  akzeptiert ein Graustufenvideo und wendet eine Falschfarbenpalette an. Innerhalb von OpenCV gibt es bereits eine
  ganze Reihe von <a href="#ref-opencv-color-map">Paletten</a> welche aber nicht wirklich gut aussehen. Deswegen
  unterstützen wir davon nur die Palette Hot. Neben dieser haben wir die Palette die schöne Palette von FLIR namens
  Iron implementiert. Weil es keine zu 100% genaue mathematische Definition der Palette gibt haben wir die 256
  Farbwerte in einer Map abgelegt. Somit können wir die nicht so ansehbaren Graustufenvideos in mit Farben versehen
  um diese an unsere Partner weiterleiten zu können damit sie diese in Präsentation aufnehmen können. Zudem können
  wir die Falschfarbenbilder auch weiter an das Thermoboard schicken.
</p>

<figure class="affix">
  <img src="images/thermoscanner/iron-colormap.png">
  <figcaption>FLIR Iron Color Palette</figcaption>
</figure>

<h5>ExtractFrames</h5>
<p>
  Das ExtractFrames diente uns während der Nachbearbeitung und zur Erstellung von kurzen Testvideos anhand unseres
  Testmaterials. Dieses Tool kann schnell n Frames aus einem Video extrahieren und in einem Ordner ablegen.
</p>

<h4>Thermobox Cockpit</h4>
<p>
  Die vom Uploader hochgeladenen Dateien möchten wir mit einfachen Mitteln einsehen können, um nachvollziehen zu
  können, ob die Datenakquise richtig funktioniert. Die Dateien werden asynchron auf den Server geladen was heisst,
  dass man partielle Aufnahmen bereits ansehen kann. Das Cockpit ist eine Webapplikation, die im Browser links eine
  Liste aller Artefakte darstellt. Diese werden zuerst nach Datum und dann nach Zeitpunkt zusammengefasst. Dies kann
  anhand der Dateinamen der Artefakte gemacht werden. Klickt man ein Artefakt an wird es rechts angezeigt.
</p>

<p>
  Es gibt eine simple REST API die die Artefakte auf die auf dem Dateisystem verfügbar sind sammelt und zurückgibt.
  Mit Javascript bauen wir dann die Liste der Artefakte auf. Auf ein Framework haben wir bewusst verzichtet, weil das
  Cockpit eigentlich nie mehr sein soll als ein Debug Tool.
</p>

<p>
  Um den Datenschutzanforderungen der SBB Folge zu leisten, haben wir die Webapplikation hinter einen simplen
  Kennwortschutz gestellt. Auf den Aufnahmen sind zum Teil auch Personen erkennbar die im Zug sitzen. Zudem sind
  die Zugnummern lesbar. Jemand könnte also eine Inventarliste aller Fahrzeuge der SBB aufbauen. Die Passwörter
  werden in einer Konfigurationsdatei abgelegt.
</p>

<figure class="affix">
  <img src="images/thermoscanner/screenshot-thermobox-cockpit.png">
  <figcaption>Screenshot Thermobox Cockpit</figcaption>
</figure>

<!-- References -->

<p id="ref-json-net" class="reference-item">
  <span class="ref">Json.NET: .NET API für JSON</span>
  <a href="https://www.newtonsoft.com/json" target="_blank" rel="noopener">https://www.newtonsoft.com/json</a>
  <span class="retrieved">11. Januar 2018</span>
</p>

<p id="ref-temper-in-csharp" class="reference-item">
  <span class="ref">Code Project: Temper USB Thermometer in C#</span>
  <a href="https://www.codeproject.com/Tips/1007522/Temper-USB-Thermometer-in-Csharp" target="_blank" rel="noopener">https://www.codeproject.com/Tips/1007522/Temper-USB-Thermometer-in-Csharp</a>
  <span class="retrieved">12. Januar 2018</span>
</p>

<p id="ref-openweathermap" class="reference-item">
  <span class="ref">OpenWeatherMap: Wetter API</span>
  <a href="http://openweathermap.org" target="_blank" rel="noopener">http://openweathermap.org</a>
  <span class="retrieved">10. Januar 2018</span>
</p>

<p id="ref-apple-prores" class="reference-item">
  <span class="ref">Apple ProRes</span>
  <a href="https://en.wikipedia.org/wiki/Apple_ProRes" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Apple_ProRes</a>
  <span class="retrieved">13. Januar 2018</span>
</p>

<p id="ref-flir-interface" class="reference-item">
  <span class="ref">FLIR Interface Requirements Specification for FLIR TIFF File Format</span>
  <a href="http://www.flir.com/uploadedFiles/sUAS/Products/Vue-Pro/FLIR-Interface-Requirements-TIFF.pdf" target="_blank"
     rel="noopener">http://www.flir.com/uploadedFiles/sUAS/Products/Vue-Pro/FLIR-Interface-Requirements-TIFF.pdf</a>
  <span class="retrieved">14. Januar 2018</span>
</p>

<p id="ref-opencv-color-map" class="reference-item">
  <span class="ref">OpenCV Color Maps</span>
  <a
    href="https://docs.opencv.org/3.4.0/d3/d50/group__imgproc__colormap.html#gga9a805d8262bcbe273f16be9ea2055a65aef83628a5b46e23bf823000ff0270304"
    target="_blank" rel="noopener">https://docs.opencv.org/3.4.0/d3/d50/group__imgproc__colormap.html</a>
  <span class="retrieved">14. Januar 2018</span>
</p>
