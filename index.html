<!DOCTYPE html>
<html lang="de">
<head>
  <meta charset="utf-8">
  <title>Automatische Detektion von Isolationsschäden an Zügen</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Sebastian Häni, Raphael Laubscher">
  <link rel="stylesheet" href="normalize.css/normalize.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans">
  <link rel="stylesheet" href="main.css">
  <script src="jquery/dist/jquery.min.js"></script>
  <script src="doc.js"></script>
</head>
<body>

<div class="front">
  <p>
    Berner Fachhochschule<br/>
    Bachelor of Science in Computer Science<br/>
    Bachelor Thesis
  </p>
  <h1>
    Thermo-Scanner:<br>
    Automatische Detektion von Isolationsschäden an Zügen
  </h1>
  <p class="cover">
    <img src="images/cover.jpg" width="505" height="106" alt="Cover"/>
  </p>

  <table>
    <tbody>
    <tr>
      <td>Studenten:</td>
      <td>Sebastian Häni, <a href="mailto:haeni.sebastian@gmail.com" target="_blank" rel="noopener">haeni.sebastian@gmail.com</a></td>
    </tr>
    <tr>
      <td></td>
      <td>Raphael Laubscher, <a href="mailto:raphael.laubscher@gmail.com" target="_blank" rel="noopener">raphael.laubscher@gmail.com</a></td>
    </tr>
    <tr>
      <td>Betreuer:</td>
      <td>Marcus Hudritsch, <a href="mailto:marcus.hudritsch@bfh.ch" target="_blank" rel="noopener">marcus.hudritsch@bfh.ch</a></td>
    </tr>
    <tr>
      <td>Experte:</td>
      <td>Dr. Harald Studer, <a href="mailto:harald.studer@optimo-medical.com" target="_blank" rel="noopener">harald.studer@optimo-medical.com</a></td>
    </tr>
    <tr>
      <td>Version:</td>
      <td class="version">2.0.0, <span>20.01.2018</span></td>
    </tr>
    </tbody>
  </table>
</div>

<div class="chapter">
  <p>
    Diese Arbeit wurde als Teil eines Bachelor Studiums an der Berner Fachhochschule BFH geschrieben.
  </p>

  <p>
    Sämtliche Projektinformationen wie diese Dokumentation, Präsentationen, Videos und der Sourcecode sind auf der
    Projektwebsite unter <a href="https://thermotrains.github.io" target="_blank" rel="noopener">
    https://thermotrains.github.io</a> öffentlich zugänglich.
  </p>

  <p>
    Alle Quellen sind nummeriert [<em>n</em>] und im Quellenverzeichnis im Appendix aufgelistet. Grundwissen in
    Informatik wird zum Verständnis dieser Arbeit vorausgesetzt. Die wichtigsten Abkürzungen und Konzepte sind im
    Glossar, das ebenfalls im Appendix gefunden werden kann, erklärt.
  </p>

  <p>
    Dieses Dokument wurde in HTML geschrieben und mit <a href="#ref-prince">Prince</a> zu einer PDF Datei konvertiert.
    Die verwendete Schrift ist <em>Open Sans</em>, entworfen von Steve Matteson im Auftrag von Google.
  </p>
</div>


<p id="ref-prince" class="reference-item">
  <span class="ref">YesLogic Pty. Ltd.: Prince</span>.
  <a href="http://www.princexml.com/" target="_blank" rel="noopener">http://www.princexml.com/</a>
  <span class="retrieved">20. März 2017</span>
</p>

<div class="chapter">

  <h2 class="break nonr">Abstract</h2>

  <p>
    Die Vision des Projekts ist das automatische Erkennen von fehlerhaften Isolationen an Zügen. Eine defekte Isolation
    soll mit detaillierten Informationen an ein Servicecenter gemeldet werden. Weil dadurch der genaue Zustand bekannt
    ist, lässt sich der optimale Zeitpunkt einer Wartung kosteneffektiv festlegen. Solch vorausschauende Wartung
    (Predictive Maintenance) ist der nächste Schritt in der Industrie, um noch effizienter zu wirtschaften.
    Zeit- oder distanzabhängige Wartungen entfallen.
  </p>
  <p>
    Mit Hilfe einer Wärmebildkamera sollen die Fahrzeuge auf Wärmeverlust untersucht werden. Die Aufnahmen machen
    defekte Wärmebrücken und Isolationen sichtbar. Sie liefern die Erkenntnisse, ob und wie eine Klassifizierung möglich
    ist.
  </p>
  <p>
    Aus der Vorstudie, welche die erste Phase unseres Projektes bildet, liegt ein positiver Machbarkeitsentscheid,
    eine evaluierte Wärmebildkamera und der Prototyp des Post-Processings vor.
  </p>
  <p>
    Das Resultat ist eine Gesamtlösung, die alle Teile eines Predictive Maintenance Systems abdeckt.
  </p>
  <ul>
    <li><strong>Daten-Akquise</strong>: Thermo-Scanner, der Züge aufnimmt während sie vorbeifahren.</li>
    <li><strong>Daten-Verarbeitung</strong>: Aufnahmen werden entzerrt und aufbereitet.</li>
    <li><strong>Daten-Analyse</strong>: Mit der Cloud Applikation "Thermoboard" kann ein Experte die strukturierten Daten analysieren.</li>
  </ul>
  <p>
    Bisher existierten keine vergleichbaren Aufnahmen von Zügen. Leider konnten wir in der kurzen Zeit keine Züge mit
    Defekten filmen, was eine Klassifizierung schwierig macht.
  </p>
  <p>
    Als angehende Ingenieure forderte uns diese Arbeit in allen Disziplinen, von der Planung, über die Lösung
    organisatorischer und technischer Herausforderungen, bis hin zum Handwerklichen. Ein Projekt also, wie es in der
    realen Welt existiert. Deshalb sind wir sehr zufrieden, mit unserer Arbeit die Grundlage für vielversprechende
    Optimierungen in der Energieeinsparung und der Wartung geschaffen zu haben. Unsere Auftraggeberin, die SBB,
    stimmen die Resultate positiv und sie ist sehr interessiert das Projekt weiter zu führen. Wir haben gezeigt, dass
    Wärmebildanalysen, dank strategisch gewählter Aufnahmeorte, im grossen Stil möglich sind. Ausserdem liefern die
    Aufnahmen neue Erkenntnisse, wie die Züge im Betrieb aussehen. Wir sind also zuversichtlich, dass in Zukunft eine
    Erkennung von Isolationsschäden oder anderen Defekten aufgrund von Wärmebildern möglich wird.
  </p>
</div>

<div class="chapter">
  <h2 class="break nonr">Inhaltsverzeichnis</h2>
  <ol id="toc" class="allow-page-break"></ol>
</div>


<div class="chapter">
  <h2 class="break">Einleitung</h2>

  <h3>Ausgangslage</h3>

  <p>
    Die Abteilung des SBB Personenverkehrs möchte bis 2025 im Rahmen ihres Energieeinsparungsprojekts 600 GWh pro Jahr
    einsparen. Das AKW Mühleberg produziert im Vergleich dazu knapp 3'000 GWh pro Jahr. Das Energie- und direkt damit
    verbundene Kosteneinsparungspotential ist dementsprechend enorm. Eine Massnahme aus diesem Projekt ist,
    Schienenfahrzeuge automatisch auf fehlerhafte Isolationen zu prüfen. Durch die Bedingungen, denen die Materialien im
    täglichen Verkehr ausgesetzt sind, kommt es vor, dass sich Isolationsteile, vor allem im Dachbereich, lösen oder
    aufgrund von Materialfehlern oder Wasser- und Feuchtigkeitseintritt Schaden nehmen und zerfallen.
  </p>

  <h3>Vision</h3>

  <p>
    Die Vision des Gesamtprojekts der SBB ist das automatische Erkennen von fehlerhaften Zügen. Ein fehlerhafter Zug
    soll mit detaillierten Informationen (Zug X, n-ter Wagon von vorne, betroffener Bereich, etc.) an ein Servicecenter
    gemeldet werden. Weil dadurch der genaue Zustand bekannt ist, lässt sich der optimale Zeitpunkt einer Wartung
    kosteneffektiv festlegen. Predictive Maintenance ist meist günstiger, weil Schäden noch nicht enstanden oder
    kleiner sind. Dies trifft bei unentdeckten Wasserschäden besonders zu.
  </p>

  <h3>Projektidee</h3>

  <p>
    Mit Hilfe einer Wärmebildkamera sollen die Fahrzeuge auf Wärmeverlust untersucht werden. So kann man defekte
    Wärmebrücken und Isolationen sichtbar machen. Zusätzlich soll ein Farbvideo aufgezeichnet werden. Das ermöglicht die
    Defekte zu lokalisieren und Rückschlüsse auf die Ursache zu ziehen. Die beiden Kameras sollen in einem
    wettertauglichen Gehäuse automatisch vorbeifahrende Züge aufnehmen. Die Aufnahmen werden danach zu einem Zugpanorama
    zusammengesetzt und für die Analyse vorbereitet (entzerrt / rektifiziert).
  </p>

  <h3>Resultate der Vorstudie</h3>

  <ol>
    <li>
      <strong>Positive Machbarkeitsstudie:</strong>​ Die Machbarkeit des Vorhabens ist gegeben. Im Projekt 2 stellten
      wir fest, dass das Projekt mit der heutigen Hardware, Umgebung und Prozessen realisierbar ist.
    </li>
    <li>
      <strong>Evaluation Hardware:</strong>​ Mit der FLIR A65 Infrarotkamera haben wir eine Kamera gefunden mit relativ
      hoher Auflösung (640x512px) und einem grossen Field of View (45°). Sie erlaubt qualitativ gute Aufnahmen von Zügen
      ab einer Distanz von 5m und max. 30km/h (Schritttempo empfohlen). Die weitere Hardware für den Thermo-Scanner ist
      zu evaluieren.
    </li>
    <li>
      <strong>Prototyp Post-processing:</strong>​ Der Prototyp des Post-processings ist fertig und ist in der Lage
      aus einem Farbvideo das entzerrte und rektifizierte Panorama eines Zuges automatisch zu erstellen.
    </li>
    <li>
      <strong>Evaluation Standorte:</strong> ​Wir haben einen ersten Standort vor einer Waschanlage in Zürich Herdern
      gewählt bei der wir unseren Thermo-Scanner aufstellen können. In einer 2-wöchigen Testphase sollen hier möglichst
      viele Daten gesammelt werden.
    </li>
  </ol>

  <h3>Ziele Bachelor Thesis</h3>

  <p>In der zweiten Phase wollen wir uns vor allem um die Fertigstellung unseres Thermo-Scanners kümmern.</p>

  <ol>
    <li>
      <strong>Daten-Akquise​:</strong> Bau und Entwicklung des Thermo-Scanners.
      Der Thermo-Scanner nimmt automatisch vorbeifahrende Züge auf und lädt diese in die Cloud.
      Dabei ist er wetterfest und benötigt nur einen Stromanschluss.
    </li>
    <li>
      <strong>Daten-Verarbeitung:</strong>​ Im Post-processing lassen sich automatisiert aus den Videos Panoramas von
      ganzen Zügen erstellen. Die Bilder sind entzerrt und rektifiziert.
    </li>
    <li>
      <strong>Daten-Analyse:</strong>​ Die Spezialisten der SBB können die aufbereiteten Zugpanoramas analysieren
      und feststellen ob Defekte vorhanden sind. Es kann abgeschätzt werden, ob mit diesem Verfahren die Ziele der
      SBB im Bereich Predictive Maintenance und der Energieeinsparung näher rücken.
    </li>
    <li>
      <strong>Klassifizierung (optional):</strong> ​Falls wirklich Schäden durch Experten der SBB erkannt werden können,
      soll ein Klassifizierungsverfahren zur Detektion dieser Schäden entwickelt werden.
    </li>
  </ol>

  <h3>Problemstellung</h3>

  <p>
    Da wir aber zu Beginn des Projektes feststellen mussten, dass noch gar keine Infrarotaufnahmen von Isolationsschäden
    vorhanden sind, haben wir den Inhalt der Arbeit angepasst. Der Fokus der Arbeit hat sich von der automatischen
    Klassifizierung von Isolationsschäden auf das Sammeln und Aufbereiten von Infrarotaufnahmen verschoben. Neu ist
    das Ziel Erkenntnisse darüber zu gewinnen ob und wie Isolationsschäden mit Wärmebildkameras sichtbar sind.
  </p>
  <p>
    Die Problemstellung ist also: Wie kann man automatisiert Infrarotaufnahmen von Zügen erstellen und diese
    für eine Analyse aufbereiten?
  </p>

  <p>
    Wie bereits erwähnt können verrutschte Isolationsmatten und Wassereinbruch Gründe für Wärmeverlust sein.
    Jedoch gibt es auch viele andere Wärmebrücken, wie auf der Abbildung unten gut sichtbar ist. Die folgenden Bilder
    sind bei einem Test der SBB in der <a href="#ref-sbb-thermography">Klimakammer Olten</a> entstanden. In dieser
    Klimakammer können verschiedene Aussenbedingungen simuliert werden. Ausserdem kann das Fahrzeug auch eingeschaltet
    werden und somit die Betriebsbedingungen wie Heizung / Kühlung simulieren.<br>
    Weil es an diversen Orten Wärmebrücken geben kann und nicht klar ist, wie sich verrutschte Isolationen und
    Wassereinbrüche auf einer Thermographie äussern, haben wir beschlossen den ganzen Zug zu analysieren und
    uns nicht auf einzelne Bereiche einzuschränken.
  </p>

  <figure class="full">
    <img src="images/sbb_ic2000.jpg">
    <figcaption>Thermographie Test SBB</figcaption>
  </figure>

  <p>
    Das obige Bild zeigt gut, wie ein intakter Zug auf einer Thermographie aussieht. Die auffallenden roten
    Flächen auf beiden Seiten der Türe sind die Stahlträger des Wagons, durch die auch die Warmluft geführt wird.
    Weitere auffallende Wärmebrücken sehen wir im Bereich der Notausstiegfenster und natürlich der Lüftungsschlitze.
  </p>

  <p>
    Folgende Bilder zeigen diese Stellen noch genauer.
  </p>

  <figure class="affix">
    <img src="images/aircon.png">
    <figcaption>Zuluftkanal (Klimaanlage)</figcaption>
  </figure>

  <figure>
    <img src="images/window.png">
    <figcaption>Notausstiegsfenster</figcaption>
  </figure>

  <figure>
    <img src="images/ventilation.png">
    <figcaption>Lüftungsschlitze</figcaption>
  </figure>

  <p>
    Solche Erkenntnisse, wie wir sie aus obigen Testaufnahmen gewinnen konnten, sind wichtig, um später normal
    auftretende Wärmebrücken von ungewollten Wärmebrücken unterscheiden zu können.
  </p>

  <p>
    Neben der Problematik der Datenbeschaffung in Form von Videos ist auch der Aufnahmestandort ein wichtiger Faktor.
    Es nützt nichts, wenn wunderbare Aufnahmen vorhanden sind, diese aber nur einen Bruchteil der Flotte der SBB
    abdecken. Deshalb ist ein strategischer Aufnahmestandort wichtig. Der Aufnahmestandort soll in einem viel
    frequentierten Gebiet liegen, durch welches jeder Zug regelmässig durchfährt. Mit Aufnahmen an wenigen solchen
    Standorten soll ein Grossteil der Flotte gefilmt werden können. Die Anforderungen an den Standort sind:
  </p>
  <ul>
    <li>langsam fahrende Züge (max. 30km/h)</li>
    <li>min. 5m Abstand der Kamera zum Gleis</li>
    <li>Stromanschluss und Mobilfunkempfang</li>
    <li>Züge frequentieren den Standort häufig</li>
  </ul>

  <h3>Machbarkeitsanalyse</h3>

  <h4>Probeaufnahmen</h4>

  <p>
    Die ersten Aufnahmen mit der FLIR SC620 (altes Modell, von der BFH zur Verfügung gestellt) haben gezeigt,
    dass die Videos noch zu wünschen übriglassen:
  </p>

  <ul>
    <li>
      Bei zu schnellen Zügen sind die Aufnahmen auf Grund der Sensorauslesegeschwindigkeit der Kamera verzerrt.
      Die Kamera liest die Pixel von oben nach unten aus (Rolling-Shutter-Effekt).
    </li>
    <li>
      Der Bild-Auslöser reagiert mit ca. 3 Sekunden Verzögerung.
    </li>
    <li>
      Die Belichtungszeit ist nicht einstellbar.
    </li>
    <li>
      Videoaufnahme ist umständlich (Composite Video Kabel über Konverter an PC anschliessen und dort mit einer
      Applikation das Video anzeigen und mit einer anderen Software aufnehmen)
    </li>
    <li>
      Die Temperaturspannen hingegen scheinen nicht ein Problem zu werden.
      Die Kamera ist genügend sensitiv um auch bei einer grossen Spanne unterschiedliche Wärmeflächen zu erkennen.
    </li>
    <li>
      Die SC620 führt in unregelmässigen Abständen eine automatische Rekalibrierung durch.
      Dies blockiert den Videokanal für 1-2 Sekunden was unpraktisch ist, wenn gerade ein Zug vorbeifährt.
    </li>
    <li>
      Die Züge reflektieren warme Objekte relativ stark.
      Bei unseren Aufnahmen von der Seite waren zwei reflektierte Masten auf den Zügen zu erkennen.
    </li>
  </ul>

  <figure>
    <img src="images/sample-pictures/testaufnahmen.jpg">
    <figcaption>IR Testaufnahmen</figcaption>
  </figure>

  <figure>
    <img src="images/sample-pictures/making-of-wankdorf.jpg">
    <figcaption>Making of IR Testaufnahmen</figcaption>
  </figure>

  <p>
    Mit der neu evaluierten FLIR A65 aus der Vorstudie können wesentlich bessere Aufnahmen realisiert werden.
    Die sorgfältige Evaluation hat die gewünschten Resultate geliefert.
  </p>

  <ul>
    <li>
      Die grundlegende Technologie die auf Microbolometer basiert ist zwar die gleiche wie bei der FLIR SC620,
      aber die Aufnahmen der FLIR A65 sind trotzdem weniger schnell verschwommen und man könnte Züge bis 30km/h filmen.
    </li>
    <li>
      Die Kamera ist vollständig automatisierbar, nur fokussieren muss man manuell. Die Belichtungszeit, automatische
      Rekalibrierung, Framerate usw. können über eine API gesteuert werden.
    </li>
    <li>
      Angeschlossen wird die Kamera über PoE (Power over Ethernet) über ein Standardprotokoll.
    </li>
    <li>
      Designt für den Dauerbetrieb.
    </li>
  </ul>

  <figure>
    <img src="images/sample-pictures/train.png" style="height: 400px">
    <figcaption>Testaufnahmen Bern, Stöckacker</figcaption>
  </figure>

  <figure>
    <img src="images/sample-pictures/test-aufnahme-stoeckacker.jpg" style="height: 400px">
    <figcaption>Making of Testaufnahmen Bern, Stöckacker</figcaption>
  </figure>

  <h4>Mögliche Aufnahmestandorte</h4>

  <p>
    Als möglichen Aufnahmestandort eignen sich die Waschanlagen der SBB, welche unter anderem in Zürich, Basel und Genf
    stationiert sind.
  </p>

  <figure>
    <img src="images/DLR_altstetten.jpg">
    <figcaption>Waschanlage Zürich Altstetten</figcaption>
    <span class="source">Quelle: <a href="#ref-sbb-pietro">Pietro Loderer, SBB</a></span>
  </figure>

  <p>Die Waschstrassen eignen sich aus folgenden Gründen:</p>

  <ul>
    <li>
      <strong>Züge fahren langsam:</strong> Die Aufnahmen mit der IR-Kamera drohen also nicht zu verschmieren
      und wir erhalten scharfe Bilder.
    </li>
    <li>
      <strong>Wöchentliche Reinigung:</strong> Die Züge werden normalerweisen einmal pro Woche gereinigt und
      durchlaufen somit die Waschanlage häufig.
    </li>
    <li>
      <strong>Identifizierung der Wagons:</strong> Alle Züge, ja sogar jeder Wagon ist mit einem RFID-Chip
      ausgestattet. In der Waschanlage wird dieser ausgelesen. Somit wäre die Identifikation der einzelnen Wagons
      sehr einfach.
    </li>
  </ul>

</div>

<!-- References -->

<p id="ref-sbb-thermography" class="reference-item">
  <span class="ref">Thermographie SBB</span>
  <span class="sub-title">Klimakammer, Bericht Thermographie-Aufnahmen IC-2000, Schweizerische Bundesbahnen SBB, Hilfikerstr. 1, 3000 Bern 65</span>
</p>

<p id="ref-sbb-pietro" class="reference-item">
  <span class="ref">Waschanlage Zürich Altstetten</span>
  <span class="sub-title">Pietro Loderer, Systemingenieur - Bremse & Antrieb, SBB</span>
</p>

<div class="chapter">
  <h2 class="break">Projektmanagement</h2>

  <h3>Mittel und Methoden</h3>
  <h4>Methoden, Ressourcen, Sachmittel</h4>

  <ul>
    <li>Agile Projektmethode mit 2-wöchigem Intervall (Sprint)</li>
    <li>Projektplan mit grober Planung und Meilensteinen</li>
    <li>Detailplanung pro Intervall</li>
    <li>Statusupdates aller Stakeholder min. 1-mal pro Intervall</li>
    <li>Frühzeitiges Einfordern von Ressourcen seitens Auftraggeber (Spezialisten SBB)</li>
  </ul>

  <h4>Hardware / Software</h4>

  <h5>Hardware</h5>
  <ul>
    <li>Videoaufnahme-Box deren Komponenten zu evaluieren sind</li>
    <li>Server für Upload der Aufnahmen</li>
    <li>Server für Analysesoftware</li>
    <li>Werkzeug</li>
  </ul>

  <h5>Software</h5>
  <ul>
    <li>Zu entwickelnde Aufnahmesoftware</li>
    <li>Erweiterung Post-processing</li>
    <li>Zu entwickelnde Analysesoftware</li>
    <li>Visual Studio (C# IDE)</li>
    <li>IntelliJ IDE (JAVA IDE)</li>
    <li>Open CV</li>
    <li>Matlab</li>
    <li>Diverse weitere Libraries (erwähnt in jeweiligen Kapiteln)</li>
  </ul>

  <h4>Technische Voraussetzungen</h4>

  <p>
    Die technischen Voraussetzungen wurden in der Vorphase des Projekts erarbeitet.
  </p>

  <h3>Projektorganisation</h3>
  <h4>Organigramm</h4>

  <figure class="affix">
    <img src="images/projectmanagement/organigramm.svg">
    <figcaption>Organigramm</figcaption>
  </figure>

  <h4>Partner</h4>

  <figure>
    <table>
      <thead>
      <tr>
        <th>Partner</th>
        <th>Beschreibung</th>
      </tr>
      </thead>
      <tbody>
      <tr>
        <td>ID-Systems AG, Sägeweg 7, 2557 Studen</td>
        <td>Bezugsquelle FLIR Kamera, Beratung und Ausbildung Infrarotkameras</td>
      </tr>
      <tr>
        <td>Klimakammer Olten, Industriestrasse 153, 4600 Olten</td>
        <td>Experten Klimatechnik, Erfahrung mit Thermographieaufnahmen von Zügen</td>
      </tr>
      <tr>
        <td>SBB AG, Zürich-Altstetten</td>
        <td>Chef Technik DPZ und Betreiber Waschanlage Altstetten (Aufnahmestandort)</td>
      </tr>
      <tr>
        <td>Almatec AG, Industriestrasse 6, 6170 Schüpfheim</td>
        <td>Hersteller Gehäuse Thermo-Scanner</td>
      </tr>
      <tr>
        <td>FUJIFILM (Switzerland) AG, Niederhaslistrasse 12, 8157 Dielsdorf</td>
        <td>Lieferant Basler Farbkamera</td>
      </tr>
      <tr>
        <td>Spectra (Schweiz) AG, Flugplatzstrasse 5, 8404 Winterthur</td>
        <td>Lieferant Mini-PC</td>
      </tr>
      <tr>
        <td>Vilant Systems AG, Könizstrasse 161, 3097 Liebefeld</td>
        <td>Zusammenarbeit abgebrochen, Lieferant RFID Reader</td>
      </tr>
      </tbody>
    </table>
  </figure>

  <h3>Meetings</h3>

  <p>
    Folgend eine Übersicht über abgehaltene Meetings mit unseren Partnern und Kontaktpersonen bei der SBB.
    Wichtige Entscheidungen sind in einem Meeting Protokoll festgehalten oder wurden zeitnah umgesetzt.
  </p>

  <figure>
    <table>
      <thead>
      <tr>
        <th>Datum</th>
        <th>Teilnehmer</th>
        <th>Thema</th>
      </tr>
      </thead>
      <tbody>
      <tr>
        <td>17.11.2016</td>
        <td>Auftraggeber, Dozent, Projektteam</td>
        <td>Kick-Off Meeting</td>
      </tr>
      <tr>
        <td>07.03.2017</td>
        <td>Dozent, Projektteam, ID-Systems</td>
        <td>Vorstellung Projekt bei den Infrarotkamera-Spezialisten von ID-Systems</td>
      </tr>
      <tr>
        <td>05.04.2017</td>
        <td>Projektteam, Klimaspezialisten</td>
        <td>Vorstellung Projekt bei den Spezialisten der Klimakammer Olten</td>
      </tr>
      <tr>
        <td>20.06.2017</td>
        <td>Auftraggeber, Dozent, Projektteam</td>
        <td>Status Rapport Vorphase</td>
      </tr>
      <tr>
        <td>30.08.2017</td>
        <td>Projektteam, Almatec</td>
        <td>Besprechung Sonderanfertigung Gehäuse für Thermo-Scanner</td>
      </tr>
      <tr>
        <td>06.09.2017</td>
        <td>ID-Systems, Dozent, Vertreter SBB, Projektteam</td>
        <td>Schulung FLIR Infrarotkamera bei ID-Systems. Einführung in Bedienung Kamera und FLIR Tools (Viewer und
          Analysesoftware)
        </td>
      </tr>
      <tr>
        <td>11.09.2017</td>
        <td>Vilant, Projektteam</td>
        <td>Skype Meeting mit RFID Lieferant betreffend Ausleihe Demogerät und Unterstützung durch Entwickler</td>
      </tr>
      <tr>
        <td>18.09.2017</td>
        <td>Projektteam</td>
        <td>Startveranstaltung Bachelor-Thesis an der BFH</td>
      </tr>
      <tr>
        <td>19.10.2017</td>
        <td>Projektteam, Almatec</td>
        <td>Übergabe Heizlüfter / Thermostat</td>
      </tr>
      <tr>
        <td>10.11.2017</td>
        <td>Experte, Dozent, Projektteam</td>
        <td>Treffen mit Experte Dr. Harald Studer, Stand Projekt</td>
      </tr>
      <tr>
        <td>15.11.2017</td>
        <td>Projektteam, Waschanlage Zürich Herdern</td>
        <td>Treffen mit Leiter Serviceanlage Herdern. Besichtigung Aufnahmestandort für 2-wöchige Aufnahmen.</td>
      </tr>
      <tr>
        <td>20.11.2017</td>
        <td>Betreuer SBB, Projektteam, Waschanlage Zürich Altstetten</td>
        <td>Aufstellen Thermo-Scanner für 2-wöchige Aufnahmephase.</td>
      </tr>
      <tr>
        <td>30.11.2017</td>
        <td>Auftraggeber (Unternehmensentwicklung), Projektteam</td>
        <td>Predictive Maintenance Event der FFHS</td>
      </tr>
      <tr>
        <td>05.12.2017</td>
        <td>Betreuer SBB, Klimaexperte, Chef Technik, Projektteam</td>
        <td>Durchsicht der ersten Resultate mit den Spezialisten der SBB in Yverdon. Besichtigung des ICE während
          Wartung.
        </td>
      </tr>
      <tr>
        <td>19.12.2017</td>
        <td>Projektteam, Waschanlage Zürich Altstetten</td>
        <td>Aufstellen Thermo-Scanner für zweite Aufnahmephase bis 07.01.2018.</td>
      </tr>
      </tbody>
    </table>
  </figure>

  <h4>FLIR Schulung</h4>

  <p>
    Im Kaufpreis der FLIR Kamera ist auch eine Schulung von ID-Systems inbegriffen. Am 06.09.2017 durften wir zusammen
    mit den Vertretern der SBB die Schulung in den Räumlichkeiten von ID-Systems besuchen. Die Inhalte der Schulung
    waren:
  </p>

  <ul>
    <li>Einführung Thermographie</li>
    <li>Technologie Detektoren</li>
    <li>Praxis (Dos and Don'ts)</li>
    <li>FLIR Software</li>
  </ul>

  <p>
    Der vermittelte Stoff half uns, die Kamera besser zu verstehen resp. zu bedienen und wir erhielten Klarheit über die
    von FLIR bereitgestellten Hilfsmittel. Für die Klimaexperten der SBB und den Projektleiter war es zudem sehr
    interessant und im Hinblick auf eine Übergabe unserer Arbeit auch sinnvoll. Die SBB weiss nun auch wie die
    Infrarotkamera zu bedienen ist.
  </p>

  <h4>Sichtung Aufnahmen bei SBB</h4>

  <p>
    Nach der ersten zweiwöchigen Testphase haben wir unsere Resultate den Verantwortlichen der SBB präsentiert. Dabei
    waren der Projektleiter seitens SBB, der Chef Technik (Zuständig bei Isolationsschäden) und ein Klimatechniker der
    Klimakammer Olten. Wir haben die Panoramas der gefilmten Züge vorgestellt und auch einige Videos in den FLIR Tools
    genauer analysiert. Die SBB ist sehr interessiert an den Aufnahmen und sieht grosses Potential für die Verwendung in
    verschiedenen Bereichen. Die Resultate wurden auch SBB intern an Teammeetings und an den Auftraggeber weitergegeben.
    Wärmebildaufnahmen existierten bis anhin nur von einzelnen Wagons unter kontrollierten Bedingungen und nicht im
    täglichen Betrieb. Ausserdem konnte die Qualität der Aufnahmen überzeugen, denn es waren viele Details sichtbar wie
    beispielsweise das feine Wagon-Skelet. Die Experten konnten auf den Aufnahmen aber keine Schäden feststellen, somit
    haben wir leider keinen schadhaften Zug erwischt. Der Chef Technik versprach aber, uns beim nächsten
    Isolationsschaden sofort zu informieren, damit wir den Zug filmen können.
  </p>

  <h4>Predictive Maintenance Event</h4>

  <p>
    Am 30. November fand an der FFHS (Fernfachhochschule Schweiz) ein Predictive Maintenance Event mit dem Titel
    "Predictive Maintenance als zentraler Baustein der Industrie 4.0" statt. Einer der Referenten war unser Auftraggeber
    Urs Gehrig, Senior Consultant Unternehmensentwicklung. In der Präsentation wurde auch unser Projekt vorgestellt. Die
    SBB sieht unsere Arbeit also als strategisch wichtig an. Detaillierte Informationen zum Event finden sich auf <a
    href="#ref-predictive-maintenance">der Website der FFHS</a>.
  </p>

  <figure>
    <img src="images/projectmanagement/pm_event.png">
    <figcaption>Auszug Predictive Maintenance Präsentation Urs Gehrig</figcaption>
  </figure>

  <h4>Statusmeeting mit Dozent</h4>

  <p>
    Mindestens alle zwei Wochen fand ein Status Meeting mit unserem Dozent Marcus Hudritsch statt.
    Besprochene Punkten waren:
  </p>
  <ul>
    <li>Erledigte Arbeiten</li>
    <li>aktuelle Herausforderungen</li>
    <li>weiteres Vorgehen</li>
  </ul>
  <p>
    Die Meetings waren sehr konstruktiv und haben es uns erlaubt auf Kurs zu bleiben.
  </p>

  <h3>Zeitplan</h3>

  <p>
    Die Grobplanung haben wir mit Hilfe eines Zeitplans erstellt. Dort sind Deadlines und Meilensteine eingetragen.
    Mit dieser Planung können wir die Eckpunkte und Zeitfenster unserer Arbeit abstecken und kontrollieren,
    ob wir noch gut in der Zeit liegen.
  </p>

  <figure>
    <img src="images/projectmanagement/projectplan_1.png">
    <figcaption>Projektplan Seite 1</figcaption>
  </figure>
  <figure>
    <img src="images/projectmanagement/projectplan_2.png">
    <figcaption>Projektplan Seite 2</figcaption>
  </figure>

  <h3>Projektwebsite</h3>
  <p>
    Um den diversen Interessensgruppen unserer Arbeit diese Dokumentation und andere Ressource einfach zugänglich zu
    machen, haben wir eine Projektwebsite erstellt. Diese beinhaltet das Video, diese Dokumentation, die Slides sowie
    ein Link zum Source Code. Das Hosting ist gratis von GitHub bereitgestellt. Die Dokumentation und Slides Unterseiten
    sind in einem eigenen Git-Repository gehalten und werden bei einem Push in den Master Branch automatisch auf Travis
    gebaut und ausgeliefert.
  </p>

  <p>
    Die Projektewebsite kann unter <a href="https://thermotrains.github.io" target="_blank" rel="noopener">thermotrains.github.io</a>
    gefunden werden.
  </p>

</div>

<!-- References -->
<p id="ref-predictive-maintenance" class="reference-item">
  <span class="ref">Predictive Maintenance Event FFHS</span>
  <a
    href="https://www.ffhs.ch/fuer-unternehmen/seminare-workshops/vergangene-seminare-workshops/predictive-maintenance-als-zentraler-baustein-der-industrie-40-928"
    target="_blank" rel="noopener">Predictive Maintenance als zentraler Baustein der Industrie 4.0</a>
  <span class="retrieved">5. Januar 2018</span>
</p>

<div class="chapter">
  <h2 class="break">System Architektur</h2>

  <p>
    Dieses Kapitel soll einen Überblick über die geschaffene Lösung bieten. Die Lösung entspricht dem typischen
    Muster eines kompletten Systems im Feld von Predictive Maintenance. Um seine Aufgabe zu erfüllen sind folgende
    Schritte durchzuführen:
  </p>

  <ol>
    <li>Datenakquise</li>
    <li>Datenverarbeitung</li>
    <li>Datenanalyse</li>
  </ol>

  <p>
    Nebst den eigentlichen Schritten müssen diese überwacht werden können. Dazu haben wir in einer zweiten Dimension
    einen Maintenance Layer gebaut.
  </p>

  <p>
    In dem jeweiligen Kapitel wird im Detail auf die drei Subsysteme eingegangen.
  </p>

  <figure class="full affix">
    <img src="images/master-diagram.png">
    <figcaption>System Architektur</figcaption>
  </figure>

  <h3>Datenakquise</h3>
  <p>
    Die Datenakquise passiert auf der Thermo-Scanner-Hardware. Die gesammelten Artefakte werden dann in das
    Processing System übergeben in dem diese auf einen Dateiserver geladen werden. Die Akquise der Daten ist in
    unserem Projekt der wichtigste sowie auch herausforderndste Faktor. Die Hardware sowie auch Software wurde komplett
    durch uns zusammengebaut und programmiert.
  </p>

  <p>
    Das System haben wir mit einer eigens gebauten Monitoring-Lösung überwacht. Aufgetretene Fehler haben wir an ein
    externes Tool "Sentry" geschickt, auf welchem wir diese auswerten konnten. Den Zugang zum System haben wir mit
    Teamviewer eingerichtet.
  </p>

  <p>
    Das Akquise-Subsystem liefert folgende Daten an das Datenverarbeitungs-Subsystem weiter:
  </p>

  <ul>
    <li>Infrarot Video</li>
    <li>Sichtbare Lichtwellen Video</li>
    <li>Aussentemperaturinformationen</li>
    <li>Wetterinformationen</li>
    <li>Metainformationen</li>
  </ul>

  <h3>Datenverarbeitung</h3>
  <p>
    Die Datenverarbeitung hat keine dedizierte Hardware. Stattdessen haben wir die Software auf unseren privaten
    Rechnern laufen lassen. Zum einen wären solche Server ziemlich teuer im Betrieb und zum anderen ist das System
    nicht ganz ausgereift, um es autonom zu betreiben. Die Image Processing Pipeline ist in einer
    plattformunabhängigen Sprache geschrieben und könnte bei Bedarf auf irgendwelcher Hardware automatisch betrieben
    werden.
  </p>

  <p>
    Das Datenverarbeitungs-Subsystem liefert folgende Daten an das Analyse-Subsystem weiter:
  </p>

  <ul>
    <li>Infrarot Bild mit ganzem Zug</li>
    <li>Infrarot Bild mit einzelnen Wagen</li>
    <li>Sichtbare Lichtwellen mit einzelnen Wagen</li>
    <li>Sichtbare Lichtwellen Bild mit ganzem Zug</li>
    <li>Aussentemperaturinformationen</li>
    <li>Wetterinformationen</li>
    <li>Metainformationen</li>
  </ul>

  <h3>Datenanalyse</h3>
  <p>
    Dieses Subsystem dient einem Experten, der sich mit den Wagenbaustrukturen auskennt, als Plattform zur manuellen
    Auswertung der aufbereiteten Daten. Er hat die Möglichkeit in einer angenehmen Umgebung die Daten genauer
    anzuschauen und sie zu vergleichen. Er kann zum Beispiel dasselbe Fahrzeug über die Betriebsdauer vergleichen und
    sehen ob sich irgendwo ein Schaden abzeichnet. Oder er kann denselben Fahrzeugtyp unter einander vergleichen und
    Unterschiede feststellen. Die Metainformationen, die ihm dazu angezeigt werden, helfen ihm zu einem Schluss zu
    kommen.
  </p>

  <h3>Technologien</h3>
  <p>
    Sämtlicher Sourcecode der im Umfang dieser Arbeit geschrieben wurde, ist auf GitHub verfügbar. Da wir für
    unterschiedliche Technologien und Betriebsumgebungen Software geschrieben haben, haben wir diese in verschiedene
    Git Repositories aufgeteilt und unter einer GitHub Organisation angelegt:
    <a href="https://github.com/ThermoTrains" target="_blank" rel="noopener">https://github.com/ThermoTrains</a>.
  </p>
  <p>
    Die Repositories <code>docs</code>, <code>slides</code> und <code>thermoboard</code> werden bei
    einem Push auf den Master Branch oder einem Pull Request auf Travis CI gebaut. Die Builds sind unter
    <a href="https://travis-ci.org/ThermoTrains" target="_blank" rel="noopener">https://travis-ci.org/ThermoTrains</a>
    verfügbar.
  </p>

  <figure class="full">
    <table>
      <thead>
      <tr>
        <th>Repository</th>
        <th>Typ</th>
        <th>Sprachen</th>
        <th>Frameworks</th>
      </tr>
      </thead>
      <tbody>
      <tr>
        <td><code><a href="https://github.com/ThermoTrains/thermobox" target="_blank" rel="noopener">thermobox</a></code></td>
        <td>Windows Applikation</td>
        <td>C#</td>
        <td>.NET, FLIR ATLAS, Basler Pylon, EMGU.CV</td>
      </tr>
      <tr>
        <td><code><a href="https://github.com/ThermoTrains/post-processing" target="_blank" rel="noopener">post-processing</a></code></td>
        <td>Konsolenapplikation</td>
        <td>Java</td>
        <td>OpenCV</td>
      </tr>
      <tr>
        <td><code><a href="https://github.com/ThermoTrains/thermoboard" target="_blank" rel="noopener">thermoboard</a></code></td>
        <td>Webapplikation</td>
        <td>PHP, TypeScript</td>
        <td>Laravel, Angular</td>
      </tr>
      <tr>
        <td><code><a href="https://github.com/ThermoTrains/thermobox-cockpit" target="_blank" rel="noopener">thermobox-cockpit</a></code></td>
        <td>Webapplikation</td>
        <td>PHP, HTML</td>
        <td>keine</td>
      </tr>
      <tr>
        <td><code><a href="https://github.com/ThermoTrains/supersimplemonitoring" target="_blank" rel="noopener">supersimplemonitoring</a></code></td>
        <td>Webapplikation</td>
        <td>PHP, HTML</td>
        <td>Chart.JS</td>
      </tr>
      <tr>
        <td><code><a href="https://github.com/ThermoTrains/research" target="_blank" rel="noopener">research</a></code></td>
        <td>Forschungscode-Snippets</td>
        <td>Matlab</td>
        <td>keine</td>
      </tr>
      <tr>
        <td><code><a href="https://github.com/ThermoTrains/docs" target="_blank" rel="noopener">docs</a></code></td>
        <td>Dokumentation</td>
        <td>HTML</td>
        <td>PrinceXML</td>
      </tr>
      <tr>
        <td><code><a href="https://github.com/ThermoTrains/slides" target="_blank" rel="noopener">slides</a></code></td>
        <td>Slides zum Projekt</td>
        <td>HTML</td>
        <td>Reveal.js</td>
      </tr>
      <tr>
        <td><code><a href="https://github.com/ThermoTrains/thermotrains.github.io" target="_blank" rel="noopener">thermotrains.github.io</a></code></td>
        <td>Projektwebsite</td>
        <td>HTML</td>
        <td>keine</td>
      </tr>
      <tr>
        <td><code><a href="https://github.com/ThermoTrains/classification" target="_blank" rel="noopener">classification</a></code></td>
        <td>Klassifikationsspielereien</td>
        <td>Python, C#</td>
        <td>Tensorflow</td>
      </tr>
      </tbody>
    </table>
    <figcaption>Übersicht Repositories</figcaption>
  </figure>

  <p>
    Das Repository <code>classification</code> diente als Spielwiese und enthält keinen lebenswichtigen Code für das
    Projekt. Wir haben damit die weitere Stossrichtung der automatischen Klassifikation erprobt.
  </p>

</div>

<div class="chapter">
  <h2 class="break">Akquise: Thermo-Scanner</h2>

  <figure>
    <img src="images/thermoscanner/20171120_100306.jpg" style="height: 300px"/>
    <figcaption>Fertig installierter Thermo-Scanner</figcaption>
  </figure>

  <figure>
    <img src="images/thermoscanner/20171120_084011.jpg" style="height: 300px"/>
    <figcaption>Installation Thermo-Scanner</figcaption>
  </figure>
  <p>
    Der Thermo-Scanner verkörpert die Hard- und Software, um alle nötigen Rohdaten für die Analyse der Züge zu
    beschaffen. Sie soll am Gleisrand aufgestellt werden und alle vorbeifahrenden Züge automatisch aufnehmen. Die
    einzige Voraussetzung für den Betrieb ist ein Stromanschluss. Ansonsten soll der Thermo-Scanner autonom
    funktionieren.
  </p>

  <h3>Anforderungen</h3>

  <ul class="affix">
    <li>Die Box ist wetterfest.</li>
    <li>Benötigt nur einen Stromanschluss, funktioniert nach dem Einstecken automatisch.</li>
    <li>Die Aufnahme startet automatisch beim Einfahren eines Zuges.</li>
    <li>Ein Infrarot- und ein Farbvideo wird aufgenommen</li>
    <li>Metadaten zur Aufnahme werden aufgenommen</li>
    <li>Die Wagonnummer wird mittels RFID ausgelesen um den Zug zu identifizieren</li>
    <li>Die Videos und Metadaten werden auf einen Server hochgeladen.</li>
    <li>Die Box kann remote gesteuert und gewartet werden.</li>
    <li>Kann langsam fahrende Züge aufnehmen (Schritttempo).</li>
  </ul>

  <p>
    Die folgenden zwei Kapiteln dokumentieren im Detail, wie die dafür nötige Hard- und Software aufgebaut ist.
    Ein solch autonomes System ist aufwendig in der Realisation. Die Software ist eigens von uns für diesen
    Zweck entwickelt. Auch die Hardware, den Thermo-Scanner, bauten wir selber. Es versteht sich also,
    dass dies viel aufwendiger ist, als ein paar von Hand gefilmte Wärmebildaufnahmen im Labor auszuwerten.
    Dafür ist ein einsatzbereites System entstanden, dass, am Gleisrand aufgestellt und eingesteckt, den Betrieb
    von alleine aufnimmt.
  </p>

  <h3>Hardware</h3>

<p>
  Dieses Kapitel beschreibt wie der Thermo-Scanner beschaffen ist, die Funktionsweise wird im Kapitel
  Software beschrieben.
</p>

<h4>Architektur</h4>

<figure>
  <img src="images/thermoscanner/kabine_blueprint.png" width="700"/>
  <figcaption>Blueprint Kabine</figcaption>
  <span class="source">Quelle: <a href="#ref-almatec-offerte">Offerte Almatec</a></span>
</figure>

<p>
  Die gesamte Hardware ist in einem wetterfesten Gehäuse nach IP55 Standard untergebracht (Staub- / Wasserschutz). Das
  Gehäuse ist eine Spezialanfertigung mit einem Sichtfenster für die Farbkamera und einer Öffnung für die
  Infrarotkamera. Im Boden des Gehäuses sind Öffnungen für die Stromzufuhr und den Anschluss der externen GSM-Antenne
  sowie des RFID Readers vorhanden. Der Thermo-Scanner steht aufrecht und kann, dank der soliden Füsse, bei Bedarf auch
  mit Sandsäcken stabil montiert werden.
</p>

<p>
  Das Innenleben ist wie folgt aufgebaut (von unten nach oben):
</p>

<ul>
  <li>Temperatursensor für die Aussentemperatur (hängt unten heraus)</li>
  <li>Steckerleiste für die Stromversorgung aller Komponenten</li>
  <li>Heizlüfter um Kondenswasserbildung zu verhindern</li>
  <li>Mini PC für Betrieb der Software</li>
  <li>Tablare für die Kameras</li>
  <li>3D schwenkbare Stative für Kameras</li>
  <li>FLIR Infrarotkamera</li>
  <li>Basler Full-HD Farbkamera</li>
  <li>Thermostat, welchen den Heizlüfter steuert (ein/aus)</li>
  <li>GSM-Antenne (ausserhalb Gehäuse) für Kommunikation</li>
</ul>

<figure>
  <img src="images/thermoscanner/innenleben.jpg" style="height: 500px;"/>
  <figcaption>Innenleben Thermo-Scanner</figcaption>
</figure>

<figure>
  <img src="images/thermoscanner/front_nahe.jpg" style="width: 60%;"/>
  <figcaption>Front Thermo-Scanner</figcaption>
</figure>

<h4>Komponenten</h4>

<h5>Gehäuse</h5>

<figure>
  <img src="images/thermoscanner/front_ganz.jpg" style="height: 500px;"/>
  <figcaption>Thermo-Scanner Gehäuse</figcaption>
</figure>

<p>
  Das Gehäuse ist eine Spezialanfertigung, die wir bei der Firma Almatec in Auftrag gegeben haben. Es ist wetterfest und
  für das Betreiben von Elektronik im Freien geeignet. Oben befinden sich die Öffnungen für die beiden Kameras. Die
  Farbkamera schaut durch ein Plexiglas und die Infrarotkamera durch ein Loch. Die Infrarotkamera darf nicht durch ein
  Glas filmen, da sonst die Infrarotwellen gefiltert werden. Spezialgläser existieren zwar, sind aber extrem teuer. Um
  dennoch einigermassen gegen Wassereintritt geschützt zu sein, ist die Öffnung mit einer Gummitülle abgedichtet. Auf
  die Hardware im Inneren kann von beiden Seiten über eine Türe zugegriffen werden.
</p>

<h5>FLIR Infrarotkamera</h5>

<figure>
  <img src="images/thermoscanner/camera.png" width="700"/>
  <figcaption>FLIR A65</figcaption>
  <span class="source">Quelle: <a href="#ref-flir-a65">FLIR A65 User Manual</a></span>
</figure>

<p>
  Die FLIR A65 Infrarotkamera wurde in der Voranalyse des Projektes evaluiert. Aufgrund des grossen FOV, der
  verhältnismässig grossen Auflösung, der Möglichkeit zur automatisierten Steuerung und der Bezahlbarkeit ist es die
  ideale Kamera für unser Projekt. Die Kamera ist auf einem schwenkbaren Stativ montiert und kann so ideal ausgerichtet
  werden.
</p>

<p>Die konkreten Anforderungen sind:</p>

<figure class="affix">
  <table>
    <thead>
    <tr>
      <th>Nr.</th>
      <th>Name</th>
      <th>Beschreibung</th>
    </tr>
    </thead>
    <tbody>
    <tr>
      <td>1</td>
      <td>Field Of View (FOV)</td>
      <td>
        Die Kamera muss ein genügend grosses FOV besitzen. Der Zug soll in der Höhe komplett auf das Bild
        passen. Der IC2000 ist ca. 4m hoch.
      </td>
    </tr>
    <tr>
      <td>2</td>
      <td>Auflösung</td>
      <td>
        Je höher die Auflösung desto besser lassen sich die Aufnahmen bearbeiten und analysieren. Die Auflösung von
        Infrarotkameras sind technologiebedingt klein, deshalb sollte eine grösstmögliche Auflösung gewählt werden.
      </td>
    </tr>
    <tr>
      <td>3</td>
      <td>Automatisierbar</td>
      <td>
        Die Kamera muss automatisierbar sein, d.h. sie muss per API gesteuert und ausgelesen werden können.
        Ein Netzwerkanschluss ist wünschenswert, so kann die Kamera einfach remote gesteuert werden.
      </td>
    </tr>
    <tr>
      <td>4</td>
      <td>Framerate</td>
      <td>
        Die Framerate muss genügend gross sein, um ausreichend Bilder des vorbeifahrenden Zuges aufzunehmen.
        Zwei aufeinander folgende Frames sollten sich dabei zu min. 25% überlappen, um ein Stitching zu ermöglichen.
        Bis zu einer Geschwindigkeit von 30km/h soll dies möglich sein. Die Limitierung auf 30km/h rührt von der
        Trägheit des Detektionsmaterials her (bei Microbolometer).
      </td>
    </tr>
    <tr>
      <td>5</td>
      <td>Preis</td>
      <td>
        Die SBB übernimmt die Kosten für die Anschaffung und Wartung der Kamera. Deshalb müssen die Kosten im
        Rahmen des Budgets der SBB liegen.
      </td>
    </tr>
    </tbody>
  </table>
  <figcaption>Anforderungen IR-Kamera</figcaption>
</figure>

<p>Evaluation</p>

<p>
  Schlussendlich haben wir zwei passende Kameras gefunden. Da der Markt nicht sehr gross ist und wir unsere
  Anforderungen ziemlich genau kannten, war die engere Auswahl schnell getroffen. Auch die beiden Experten von ID
  Systems, die uns bei der Evaluation unterstützt haben, schlugen ähnliche Modelle vor. Aufgrund des Budgets kamen
  keine gekühlten Kameras in Frage.
</p>

<figure>
  <table>
    <thead>
    <tr>
      <th></th>
      <th>FLIR A65</th>
      <th>Optris PI 640</th>
    </tr>
    </thead>

    <tbody>
    <tr>
      <th>Auflösung</th>
      <td>640 x 512px</td>
      <td>640 x 480px</td>
    </tr>
    <tr>
      <th>Field Of View</th>
      <td>45° HFOV & 37° VFOV</td>
      <td>60° HFOV & 45° VFOV</td>
    </tr>
    <tr>
      <th>Framerate</th>
      <td>30 Hz</td>
      <td>32 Hz</td>
    </tr>
    <tr>
      <th>Preis</th>
      <td>8'359.85 CHF</td>
      <td>7'462.80 USD</td>
    </tr>
    </tbody>
  </table>
  <figcaption>FLIR A65 vs. Optris PI 640</figcaption>
</figure>

<p>
  Wie aus der Tabelle ersichtlich unterscheiden sich die beiden Modelle in den wichtigsten Punkten nicht gross.
  Den Ausschlag für die FLIR Kamera hat schlussendlich die kostenlose Unterstützung bei der Inbetriebnahme gegeben.
  Somit haben wir mit der FLIR A65 eine IR-Kamera gefunden, die all unseren Anforderungen entspricht. Die
  Einschränkungen die wir beachten müssen sind:
</p>

<ul>
  <li>Züge dürfen höchstens 30km/h fahren, ansonsten verschmieren die Bilder.</li>
  <li>Ein minimaler Abstand von 5m muss für die Aufnahmen gewährleistet sein.</li>
</ul>

<h5>Basler Farbkamera</h5>

<figure>
  <img src="images/thermoscanner/basler.jpg" width="700"/>
  <figcaption>Basler acA1920-25uc</figcaption>
  <span class="source">Quelle: <a href="#ref-basler-ace">Basler Website</a></span>
</figure>

<p>
  Die Farbkamera nimmt dasselbe Bild wie die FLIR Kamera auf. Diese zusätzlichen Informationen helfen später bei der
  Analyse. Ausserdem wird sie zum Triggern der Aufnahme (Start/Stopp) verwendet, indem wir im Video einfahrende Züge
  erkennen. Auch die Farbkamera ist auf einem schwenkbaren Stativ montiert, was ein optimales Ausrichten erlaubt.
</p>

<p>Anforderungen</p>
<figure class="affix">
  <table>
    <thead>
    <tr>
      <th>Nr.</th>
      <th>Name</th>
      <th>Beschreibung</th>
    </tr>
    </thead>
    <tbody>
    <tr>
      <td>1</td>
      <td>Field Of View (FOV)</td>
      <td>
        Wie bei der Infrarotkamera muss auch hier das FOV genügend gross sein um den Zug in ganzer Höhe aufzunehmen.
        Das Sichtfeld sollte sogar noch ein bisschen grösser sein als bei der IR-Kamera, damit für das Triggern der
        Aufnahme genug Zeit bleibt.
      </td>
    </tr>
    <tr>
      <td>2</td>
      <td>Auflösung</td>
      <td>
        Je höher die Auflösung desto besser lassen sich die Aufnahmen bearbeiten und analysieren.
        Deshalb sollte eine grösstmögliche Auflösung gewählt werden.
      </td>
    </tr>
    <tr>
      <td>3</td>
      <td>Automatisierbar</td>
      <td>
        Die Kamera muss automatisierbar sein, d.h. sie muss per API gesteuert und ausgelesen werden können.
        Ausserdem soll sie für den Dauerbetrieb ausgelegt sein.
      </td>
    </tr>
    <tr>
      <td>4</td>
      <td>Framerate</td>
      <td>
        Die Framerate muss genügend gross sein, um ausreichend Bilder des vorbeifahrenden Zuges aufzunehmen.
        Zwei aufeinander folgende Frames sollten sich dabei zu min. 25% überlappen, um ein Stitching zu ermöglichen.
      </td>
    </tr>
    <tr>
      <td>5</td>
      <td>Preis</td>
      <td>
        Die SBB übernimmt die Kosten für die Anschaffung und Wartung der Kamera. Deshalb müssen die Kosten im
        Rahmen des Budgets der SBB liegen.
      </td>
    </tr>
    </tbody>
  </table>
  <figcaption>Anforderungen Farbkamera</figcaption>
</figure>

<p>Basler acA1920-25uc</p>

<p>
  Mit der Basler ace Industriekamera haben wir eine geeignete Farbkamera gefunden. Hier eine Übersicht über die
  Spezifikation:
</p>

<figure>
  <table>
    <thead>
    <tr>
      <th></th>
      <th>Basler acA1920-25uc</th>
    </tr>
    </thead>

    <tbody>
    <tr>
      <th>Auflösung</th>
      <td>Full-HD 1080p</td>
    </tr>
    <tr>
      <th>Field Of View</th>
      <td>76° HFOV & 58° VFOV</td>
    </tr>
    <tr>
      <th>Framerate</th>
      <td>26 Hz</td>
    </tr>
    <tr>
      <th>Anschluss</th>
      <td>USB. Stromversorgung ebenfalls über USB.</td>
    </tr>
    <tr>
      <th>Preis</th>
      <td>756.85 CHF</td>
    </tr>
    </tbody>
  </table>
  <figcaption>Spezifikation Basler Farbkamera</figcaption>
</figure>

<p>
  Basler ist ein bekannter Hersteller von Industriekameras und geniesst einen guten Ruf. Deshalb und weil sie alle
  unsere Anforderungen erfüllt, sind wir sicher die richtige Wahl getroffen zu haben.
</p>

<h5>Mini PC</h5>

<figure>
  <img src="images/thermoscanner/minipc.jpg" width="700"/>
  <figcaption>Spectra PowerBox 31A0</figcaption>
  <span class="source">Quelle: <a href="#ref-spectra-powerbox">Spectra Website</a></span>
</figure>

<p>
  Der Mini PC bildet die zentrale Steuereinheit auf der unsere Software läuft. Die beiden Kameras werden über den
  PC gesteuert und beziehen auch den Strom von dort. Weiter sind der Temperatursensor und die externe GSM-Antenne
  an den Mini PC angeschlossen.
</p>

<p>Anforderungen</p>
<figure class="affix">
  <table>
    <thead>
    <tr>
      <th>Nr.</th>
      <th>Name</th>
      <th>Beschreibung</th>
    </tr>
    </thead>
    <tbody>
    <tr>
      <td>1</td>
      <td>Leistung</td>
      <td>
        <ul>
          <li>Verarbeitung Videostreams von zwei Kameras möglich</li>
          <li>Komprimierung FLIR Rohdaten (16Bit Video) zu MP4 läuft performant</li>
          <li>min. 16GB Arbeitsspeicher</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>2</td>
      <td>LTE Modul</td>
      <td>
        Wir brauchen Remotezugang zum Thermo-Scanner für Wartungen und den Upload der Aufnahmen.
        Der Mini-PC sollte also ein LTE Modul enthalten.
      </td>
    </tr>
    <tr>
      <td>3</td>
      <td>Anschlüsse</td>
      <td>
        <ul>
          <li>PoE um FLIR Kamera zu betreiben</li>
          <li>USB mit Stromversorgung für Basler Kamera</li>
          <li>Antennenanschluss LTE-Modul</li>
          <li>USB Anschluss für Temperatursensor</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td>4</td>
      <td>Dauerbetrieb</td>
      <td>
        Der Mini-PC muss für den Dauerbetrieb ausgelegt sein. Möglichst wenig bewegliche Teile, robuste Komponenten und
        ein gutes Klimamanagement sind nötig.
      </td>
    </tr>
    <tr>
      <td>5</td>
      <td>Montage</td>
      <td>
        Möglichkeit zum Verschrauben des PC sollte vorhanden / vorgesehen sein.
      </td>
    </tr>
    <tr>
      <td>6</td>
      <td>Preis</td>
      <td>
        Die SBB übernimmt die Kosten für die Anschaffung und Wartung des Thermo-Scanners. Deshalb müssen die Kosten im
        Rahmen des Budgets der SBB liegen.
      </td>
    </tr>
    </tbody>
  </table>
  <figcaption>Anforderungen Mini PC</figcaption>
</figure>

<p>Spezifikation Mini-PC</p>

<figure>
  <table>
    <thead>
    <tr>
      <th></th>
      <th>Spectra PowerBox 31A0</th>
    </tr>
    </thead>
    <tbody>
    <tr>
      <th>Prozessor</th>
      <td>Intel i7-6700TE</td>
    </tr>
    <tr>
      <th>RAM</th>
      <td>16GB</td>
    </tr>
    <tr>
      <th>Harddisk</th>
      <td>120GB SSD</td>
    </tr>
    <tr>
      <th>Anschlüsse</th>
      <td>PoE, USB, Antennenkabel, DisplayPort, ...</td>
    </tr>
    <tr>
      <th>Kühlung</th>
      <td>passiv</td>
    </tr>
    <tr>
      <th>Weiteres</th>
      <td>integriertes LTE Modul mit Daten SIM-Karte von Sunrise</td>
    </tr>
    <tr>
      <th>Preis</th>
      <td>2'764.80 CHF</td>
    </tr>
    </tbody>
  </table>
  <figcaption>Spezifikation Spectra Powerbox</figcaption>
</figure>

<p>
  Für das LTE Modul musste noch eine Daten-SIM Karte organisiert werden. Von der SBB konnten wir leider keine geeignete
  SIM Karte beziehen, diese waren alle auf 10GB / Monat limitiert. Für den Testbetrieb reichte uns das leider nicht, da
  wir auch Rohdaten vom Mini PC runterladen wollten. Deshalb schlossen wir bei der Sunrise ein Abo für eine unlimitierte
  Daten-SIM ab.
</p>


<figure>
  <table>
    <thead>
    <tr>
      <th></th>
      <th>Daten SIM Sunrise</th>
    </tr>
    </thead>
    <tbody>
    <tr>
      <th>Datenvolumen</th>
      <td>unlimitiert</td>
    </tr>
    <tr>
      <th>Übertragungsgeschwindigkeit</th>
      <td>Unlimitiert 4G+ High Speed</td>
    </tr>
    <tr>
      <th>Grundgebühr pro Monat</th>
      <td>49.- CHF</td>
    </tr>
    <tr>
      <th>Aktivierungsgebühr</th>
      <td>49.- CHF</td>
    </tr>
    <tr>
      <th>Vertragslaufzeit</th>
      <td>3 Monate. 14.11.2017 - 14.02.2017.</td>
    </tr>
    </tbody>
  </table>
  <figcaption>Spezifikation Daten-SIM Sunrise</figcaption>
</figure>


<h5>LTE Antenne</h5>

<figure>
  <img src="images/thermoscanner/lte_antenne.jpg">
  <figcaption>Outdoortaugliche Kreuzpol LTE Antenne</figcaption>
  <span class="source">Quelle: <a href="#ref-brack-lte">Brack: LTE-Antenne, 2x SMA Stecker</a></span>
</figure>

<p>
  Der Mini PC verfügt über ein LTE Modul mit zwei Antennenanschlüssen. Die mitgelieferten Antennen sind direkt am Mini
  PC montiert und somit innerhalb des Gehäuses. Im Gehäuse selber gibt es überhaupt keinen Empfang. Bis zu dieser
  Erkenntnis hat es lange gedauert, da zuerst die Antennen nicht geliefert wurden und wir dachten, dass LTE Modul
  funktioniere nicht. Wir nahmen an, die Antennen befinden sich innerhalb des Mini PC. Als die Antennen nachgeliefert
  wurden, hat jemand sie aus dem Briefkasten gestohlen. Sobald wir die Antennen dann endlich hatten, mussten wir
  feststellen, dass es im Gehäuse keinen Empfang gibt.
</p>
<p>
  Somit war nun klar, dass eine externe LTE Antenne benötigt wird. Die Anforderungen sind:
</p>

<figure class="affix">
  <table>
    <thead>
    <tr>
      <th>Nr.</th>
      <th>Name</th>
      <th>Beschreibung</th>
    </tr>
    </thead>
    <tbody>
    <tr>
      <td>1</td>
      <td>Outdoortauglich</td>
      <td>
        Antennen ist für den Einsatz draussen gedacht und wetterfest.
      </td>
    </tr>
    <tr>
      <td>2</td>
      <td>langes Kabel</td>
      <td>
        Min. 2m langes Kabel notwendig um Antenne aus dem Gehäuse führen zu können.
      </td>
    </tr>
    <tr>
      <td>3</td>
      <td>guter Empfang</td>
      <td>
        Möglichst guter Empfang auch in empfangsschwachen Gebieten.
      </td>
    </tr>
    </tbody>
  </table>
  <figcaption>Anforderungen LTE Antenne</figcaption>
</figure>

<p>
  Die evaluierte LTE Antenne erfüllt alle unsere Anforderungen und ist dank 90°-Kreuz-Polarisierung empfangsstark. Dank
  dieser spielt es keine Rolle, wie die Antenne zum Signal ausgerichtet ist. Das lange Antennenkabel führen wir unten
  aus dem Gehäuse (zusammen mit Strom usw.) und montieren die Antenne mit starkem Klebeband auf dem Dach des
  Thermo-Scanners.
</p>

<figure>
  <img src="images/thermoscanner/kreuz_pol.jpg">
  <figcaption>Antennenausrichtung</figcaption>
</figure>

<p class="affix">Links: sehr schlechter Empfang, Mitte: idealer Empfang, Rechts: Platzierung spielt keine Rolle</p>

<p>Spezifikation LTE Antenne</p>

<figure class="affix">
  <table>
    <thead>
    <tr>
      <th></th>
      <th>USL LTE-Antenne</th>
    </tr>
    </thead>

    <tbody>
    <tr>
      <th>Frequenzband</th>
      <td>800 Mhz bis 2.7 Ghz</td>
    </tr>
    <tr>
      <th>Polarisierung</th>
      <td>90°-Kreuz-Polarisierung</td>
    </tr>
    <tr>
      <th>Mobilfunktechnologien</th>
      <td>Unterstützt LTE, 3G und 2G</td>
    </tr>
    <tr>
      <th>Anschluss</th>
      <td>2 x SMA, 2m Antennenkabel</td>
    </tr>
    <tr>
      <th>Abstrahlcharakteristik</th>
      <td>Rundstrahl</td>
    </tr>
    <tr>
      <th>Weiteres</th>
      <td>outdoortauglich</td>
    </tr>
    <tr>
      <th>Preis</th>
      <td>62.00 CHF</td>
    </tr>
    </tbody>
  </table>
  <figcaption>Spezifikation USL LTE-Antenne</figcaption>
</figure>

<h5>Thermostat / Heizlüfter</h5>

<p>
  Um Kondenswasserbildung zu verhindern ist im Gehäuse ein Heizlüfter und ein Thermostat verbaut. Bei Temperaturen unter
  15°C schaltet der Thermostat den Heizlüfter ein, bis wieder 15°C herrschen. Wichtig dabei ist, dass der Heizlüfter
  unten im Gehäuse und der Thermostat möglichst weit entfernt oben im Gehäuse platziert ist. Ansonsten ist man nicht
  sicher, dass der ganze Innenraum genügend aufgewärmt ist. Wichtig ist auch, dass über dem Heizlüfter genügend Platz
  ist, damit nichts zu heiss wird oder Feuer fangen könnte. Dank des grossen Gehäuses konnten wir alle
  Sicherheitsabstände einhalten.
</p>

<figure>
  <img src="images/thermoscanner/heizluefter-thermostat.jpg">
  <figcaption>Links: Heizlüfter, Rechts: Thermostat</figcaption>
</figure>

<h5>Temperatursensor</h5>

<p>
  Um die Aussentemperatur zu messen, wurde ein USB-Temperatursensor <a href="#ref-temper2">TEMPer2 von PCSensor</a>
  verbaut. Wir brauchen die Aussentemperatur, um aus den Infrarotaufnahmen die genaue Temperatur berechnen zu können.
  Der Sensor wird durch ein Loch im Bodenblech aus dem Gehäuse geführt. In der Praxis hat sich leider gezeigt, dass der
  Sensor nicht zuverlässig funktioniert. Es müsste das gesamte Kabel aus dem Gehäuse rausschauen nicht nur das Endstück,
  um eine richtige Messung zu erhalten. Wir greifen deshalb auf die Temperaturdaten einer nahe gelegenen Wetterstation
  zu. Die Ungenauigkeiten, die sich dadurch ergeben, sind für unseren Fall vernachlässigbar.
</p>

<figure>
  <img src="images/thermoscanner/temper-sensor.jpg">
  <figcaption>Temperatursensor</figcaption>
</figure>

<p>
  Gegen Ende des Projektes haben wir eine neue Website von PCSensor gefunden, die den Sensor besser beschreibt. Zum
  Zeitpunkt der Implementierung waren englische Information leider nur über ältere Gerätetypen vorhanden. TEMPer2 war
  nur auf Chinesisch beschrieben. Auch gibt es seit 22. November 2017 eine neue Version der TEMPer Software. Wir haben
  nun herausgefunden, dass die 2 in TEMPer2 dafürsteht, dass das Gerät zwei Sensoren hat. Einer im Gehäuse und einer
  über ein Kabel.
</p>
<p>
  Da unsere Messungen, wie man unten sieht, fast nie unter 14°C gelangen, gehen wir davon aus, dass wir die
  Innentemperatur gemessen haben. Man sieht gut, wie die Temperatur in der Nacht konstant bei ca. 14°C verharrt, im
  Verlaufe des Tages allerdings ansteigt, wenn die Aussentemperatur über 14°C war. Das auf und ab gegen Ende der ersten
  Testphase deuten wir als eine Aussentemperatur, die stark unter 14°C lag, so dass der Thermostat den Heizlüfter über
  längere Intervalle eingeschaltet lies. Die Zeitstempel zu denen wir die Temperaturen gemessen haben, sind leider nicht
  schön linear verteilt, weil nur bei Zugdurchfahrt eine Aufnahme gespeichert wird. Die Züge fahren nicht regelmässig
  durch die Waschstrasse.
</p>

<figure class="full">
  <img src="images/thermoscanner/temper-histogram.png">
  <figcaption>Temperatursensor Histogramm</figcaption>
</figure>

<h5>RFID Reader</h5>
<p>
  Alle Waggons der SBB sind mit einem RFID Chip ausgestattet, über den sie eindeutig identifizierbar sind. Ein
  detektierter Schaden könnte somit eindeutig einem Waggon oder Zug zugeordnet werden. Der Einsatz eines RFID Readers
  ist also wünschenswert.
</p>
<p>
  <b>Anforderungen</b>
</p>
<ul class="affix">
  <li>Extern anschliessbar an Thermo-Scanner</li>
  <li>Outdoor tauglich</li>
  <li>Auslesen von Zügen die mit max. 30km/h fahren</li>
  <li>Brauchbare API / unterstützte Protokolle für die Anbindung an Thermo-Scanner SW</li>
</ul>
<p>
  Die SBB hat bereits RFID Reader im Einsatz, z.B. in den Waschstrassen. Wir haben deshalb beim Hersteller Vilant AG
  nachgefragt, ob sie uns bei unserem Projekt unterstützen und wir einen RFID Reader kaufen können. Vilant bietet RFID
  Reader für professionelle Anwendungen in der Industrie wie bspw. in Lagern, Logistikzentren und eben auch im
  Bahnsektor. Die Produktpalette erstreckt sich von Handlesegeräten bis Hochleistungsleser, die Züge bei voller
  Geschwindigkeit auslesen können. Das richtige Produkt für unsere Zwecke war also vorhanden. Nach einem ersten Gespräch
  mit Vilant wurde uns ein Testgerät inklusive Unterstützung durch Entwickler in Aussicht gestellt. Die SBB war
  aufgrund andere Projekte auch an einem Kauf eines Readers interessiert, deshalb haben wir auch eine Offerten-Anfrage
  gestellt. Leider hat Vilant nie auf unsere Anfragen mit konkreten Ergebnissen reagiert, sondern uns immer wieder auf
  später vertröstet und noch nicht einmal eine Offerte erstellt. Wir haben eine letzte Deadline für die gewünschte
  Offerte und einen Workshop mit Testgerät gestellt, welche Vilant verstreichen liess. Deshalb haben wir uns entschieden
  die Zusammenarbeit abzubrechen und den RFID Reader im Projekt vorerst nicht weiter zu berücksichtigen.
</p>
<p>
  Es ist schade, dass wir viel Zeit in die Abklärungen für einen RFID Reader gesteckt haben ohne ein Resultat zu
  erzielen. Für das erwähnte Projekt der SBB, das auch einen RFID Reader benötigt, wird derzeit bei einem anderen
  Anbieter eine Offerte eingeholt. Evtl. ergibt sich da etwas. Auf jeden Fall ist unsere Software so gebaut, dass
  weitere Sensoren problemlos eingebunden werden können. Und unser Aufnahmestandort ist an einer Waschstrasse, in
  welcher alle RFID-Chips der Waggons ausgelesen werden. Möglicherweise kann man die Daten aus der Waschstrasse mit
  unseren Aufnahmen matchen (z.B. über Timestamps). Dies verfolgen wir momentan aber nicht aktiv weiter und fokussieren
  uns auf die Aufnahmen und deren Analyse.
</p>

<!-- References -->

<p id="ref-almatec-offerte" class="reference-item">
  <span class="ref">Offerte Almatec</span>
  <span class="retrieved">25. September 2017</span>
</p>

<p id="ref-flir-a65" class="reference-item">
  <span class="ref">FLIR A65 User Manual</span>
  <a href="http://support.flir.com/DocDownload/Assets/dl/t559770-en-us_a4.pdf" target="_blank" rel="noopener">
    http://support.flir.com/DocDownload/Assets/dl/t559770-en-us_a4.pdf</a>
  <span class="retrieved">16. Juni 2017</span>
</p>

<p id="ref-basler-ace" class="reference-item">
  <span class="ref">Basler Website</span>
  <a href="https://www.baslerweb.com/de/produkte/kameras/flaechenkameras/ace/aca1920-25uc" target="_blank"
     rel="noopener">https://www.baslerweb.com/de/produkte/kameras/flaechenkameras/ace/aca1920-25uc</a>
  <span class="retrieved">17. Dezember 2017</span>
</p>

<p id="ref-spectra-powerbox" class="reference-item">
  <span class="ref">Spectra Website</span>
  <a href="https://www.spectra.de/produkte/industrie-pc-systeme/mini-pc-systeme/1-slot/spectra-powerbox-31a0.html"
     target="_blank" rel="noopener">https://www.spectra.de/produkte/industrie-pc-systeme/mini-pc-systeme/1-slot/spectra-powerbox-31a0.html</a>
  <span class="retrieved">17. Dezember 2017</span>
</p>

<p id="ref-brack-lte" class="reference-item">
  <span class="ref">USL LTE-ANTENNE, 2X SMA STECKER</span>
  <a href="https://www.brack.ch/usl-lte-antenne-237340" target="_blank" rel="noopener">https://www.brack.ch/usl-lte-antenne-237340</a>
  <span class="retrieved">12. Januar 2017</span>
</p>

<p id="ref-temper2" class="reference-item">
  <span class="ref">TEMPer2 von PCSensor</span>
  <a href="http://www.pcsensor.com/usb-thermometer/temper2.html" target="_blank" rel="noopener">http://www.pcsensor.com/usb-thermometer/temper2.html</a>
  <span class="retrieved">11. Januar 2017</span>
</p>

  <h3>Software</h3>
<p>
  Die Software des Thermo-Scanners ist mit modernen Paradigmen des Softwareengineerings gebaut. Einfachheit,
  Stabilität, Geschwindigkeit und vor allem Erweiterbarkeit waren die Kriterien, die wir beim Design immer wieder
  überprüft und umgesetzt haben.
</p>

<h4>Software Landschaft</h4>
<p>
  Um die vielfältigen Anforderung an die Entwicklung der Software abzudecken, haben wir über den Verlauf dieser Arbeit
  um unsere Hardware eine ganze Software-Landschaft aufgebaut.
</p>

<h5>Continuous Integration</h5>
<p>
  Zu einem guten Entwicklungsprozess gehört Continuous Integration. Da es für die verwendeten Technologien und
  Frameworks keine Gratisdienste gibt und wir selber keine Server besitzen, die diese Aufgabe übernehmen können,
  haben wir den Thermo-Scanner selber als Continuous Integration Server eingesetzt. Startet die Thermobox, wird
  automatisch der HEAD commit aus dem Git repository auf dem master Branch ausgecheckt. Dann werden die NuGet
  Dependencies aktualisiert. Somit kann anschliessend der Release Build durchgeführt werden. Alle Assemblies erhalten
  die korrekte Versionsnummer die im Repository in der Datei <code>SharedAssemblyInfo.cs</code> eingecheckt ist. Weil
  wir aber nicht andauernd die Version erhöhen möchten, um diese bauen zu lassen, hängen wir auch den Commit Hash hinten
  an. Dieser String kann mit folgendem Git Befehl erzeugt werden:
</p>

<figure>
  <pre><code>git describe --long --tags</code></pre>
  <figcaption>Version mit Git Commit Hash erzeugen Befehl</figcaption>
</figure>

<p>
  Dies erzeugt einen String wie diesen: <code>1.0.0-52-ge078b31</code>. Diese Versionsnummer ist deswegen wichtig,
  weil wir in den Logs auf diese zurückgreifen. Dazu mehr im Kapitel <a href="#thermobox-logging">Logging</a>.
</p>

<h5>Unit Testing</h5>
<p>
  Um die verschiedenen Komponenten und insbesondere die Bildanalyse Algorithmen zu entwickeln, haben wir, wo es Sinn
  macht, TDD (Test Driven Development) eingesetzt. Seit Visual Studio 2017 Update 5 gibt es in der Enterprise Edition
  (welche wir über Microsoft Imagine via Schulaccount beziehen können) Live Unit Testing. Dieses nette Feature
  erkennt live, welche Unit Tests von der Code-Änderung betroffen sind und führt diese nach einem Build automatisch
  aus. Das Resultat wird gleich links neben den Zeilennummern im Editor angezeigt.
</p>

<figure>
  <img src="images/thermoscanner/screenshot-live-unit-testing.png">
  <figcaption>Screenshot Live Unit Testing</figcaption>
</figure>

<p>
  Zu beachten ist, dass Visual Studio dabei sämtlichen Dateiänderungen folgt. In unserem Build wurde anfänglich
  jedes Mal die Datei <code>SharedVersionInfo.cs</code> generiert, welche die aktuelle Versionsnummer enthält. Dies hat
  den Build jedes Mal neu getriggert. Somit hat Visual Studio non-Stopp gebuilded und die Unit Tests ausgeführt. Nach
  dem hilfreichen Feedback auf eine <a
  href="https://developercommunity.visualstudio.com/content/problem/152749/live-unit-testing-rebuilding-non-stop.html">Frage
  in der Visual Studio Developer Community</a>, konnten wir aber auch diesen Issue beheben, in dem wir den Build nur
  triggern, wenn es die Datei noch nicht gibt. Zudem löschen wir diese zu Beginn eines Builds, damit sie dann auch
  wirklich neu generiert wird.
</p>

<h5 id="thermobox-logging">Logging</h5>
<p>
  Um auch später nachvollziehen zu können was in der Software passiert, ist ein umfängliches Logging unumgänglich.
  Wir setzen dazu log4net ein. Die Konfiguration des Loggings ist für alle Konfigurationen dieselbe und wird
  geteilt. Somit müssen kleine Anpassungen an der Konfiguration nicht überall nachvollzogen werden. Die Log Dateien
  werden pro Komponente in eine Datei im Pfad <code>C:\Thermobox\logs\</code> abgelegt. Eine Zeile im Log enthält
  folgende Informationen:
</p>

<ul>
  <li>Datum</li>
  <li>Uhrzeit auf die Millisekunde genau</li>
  <li>Assembly Name</li>
  <li>Version</li>
  <li>Git Commit Hash</li>
  <li>Log Level</li>
  <li>Nachricht</li>
</ul>

<p>
  Eine Log Zeile sieht dann als Beispiel so aus:
</p>

<figure>
  <pre><code>2017-11-20 09:06:44,724 TemperatureReader.exe-1.0.0-1-gb652480
    [INFO] Received message on channel cmd:capture:start: 2017-11-20@09-06-44</code></pre>
  <figcaption>Beispiel Log Zeile</figcaption>
</figure>

<p>
  Wird eine Log Datei zu gross, wird diese automatisch umbenannt und es wird eine neue Datei begonnen. Wir haben ein
  Threshold von 10 MiB gewählt. Dieser wurde eigentlich nur von der VisibleLightReader-Komponente erreicht, weil wir
  dort Debug Logs einsetzen, um den Detektionsalgorithmus laufend zu verbessern. Gesamthaft haben wir über die zwei
  Testphasen ca. 154 MiB an Logdateien gesammelt.
</p>

<p>
  Auf einer Logzeile wird der Assembly Name aufgeführt. Dies mag redundant erscheinen, weil dieser ja auch bereits in
  der Logdatei enthalten ist. Allerdings starten mit dem Start Script alle Komponenten miteinander und lassen diese
  auch auf dieselbe Konsole schreiben, damit man nicht 6 verschiedene Fenster offen haben muss um diese zu überwachen.
  Im Normalbetrieb starten die Komponenten im Hintergrund. Somit hätte man eigentlich keine Möglichkeit die Logs
  aggregiert zu sehen. Man kann höchstens ein <code>tail</code> auf die einzelnen Dateien machen. Also haben wir ein
  PowerShell Script geschrieben, dass sämtlichen Logdateien folgt. Zudem färbt es die verschiedenen Loglevels auch
  gleich ein. Also erscheint eine Exception auch sofort rot. Die Entwicklung des Scripts war nicht ganz einfach, weil
  pro Logdatei ein Thread gestartet werden muss, der einer einzelnen Logdatei folgt und dessen neuen Logs zurück auf die
  Hauptkonsole schreibt. Diese Lösung könnte eventuell auch für andere Projekte interessant sein.
</p>

<p>
  Um das Logging muss man sich bei einem Wiederbetrieb der Kabine als erstes nicht kümmern. Logs werden nur lokal
  abgelegt. Auch die Scripts zur aggregierten Ansicht der Logs nehmen zusätzliche Komponenten Logs automatisch
  auf.
</p>

<h5>Remote Access</h5>
<p>
  Um aus der Ferne auf den Mini-PC und die Hardware zu zugreifen, haben wir einen Teamviewer Server installiert. Dieser
  ermöglicht es mit einer Partner ID und einem Passwort darauf zuzugreifen. Teamviewer bietet das beste
  Benutzererlebnis verschiedenster Remote Access Lösungen. Auf die Standard-Windows-Remote-Desktop-Lösung können wir
  nicht zugreifen, weil der Server direkt mit einer öffentlichen IP im Internet steht, welche nicht statisch ist.
  Teamviewer darf für nicht kommerzielle Zwecke gratis eingesetzt werden. Sobald die SBB das Projekt weiterführt,
  müsste ein Lizenz angeschafft werden, sofern noch kein besteht.
</p>

<h5>Hardware Monitoring</h5>
<p>
  Wir stellen unsere Hardware an einem Ort auf, an dem es externe Einflüsse gibt, die wir nicht unter Kontrolle haben.
  So kann z.B. jemand den Kasten ausstecken, Regen kann hineingelangen und einen Kurzschluss erzeugen oder er
  verliert die Netzwerkkonnektivität. Wir möchten also überwachen können, ob die Thermobox läuft. Also brauchen wir
  ein Tool zur Überwachung der Uptime.
</p>

<p>
  Weiter möchten wir die Temperatur der Hardware überwachen, um eventuelle konstruktionsbedingte Wärmestaus frühzeitig
  zu erkennen, bevor etwas anfängt zu schmelzen. Die Hardware ist draussen und kann nur in den vorgegebenen Temperaturen
  betrieben werden. Durch eine Fehlfunktion des Heizlüfters oder Thermostats könnte das Innere des Gehäuses auch
  überhitzen. Oder der Mini PC erzeugt so viel Hitze, dass diese nicht abgebaut werden kann. Einen Lüftungsschlitz nach
  aussen gibt es nämlich nicht. Der einzige undichte Ort ist der Kabelschlitz im Boden des Gehäuses. Also möchten wir
  die Temperatur des Mini-PCs überwachen können.
</p>

<p>
  Um Erkenntnisse über die Performance der Software zu erlangen und eventuell zu erkennen, dass das System
  überlastet ist, möchten wir auch die CPU Last sowie den Memory Verbrauch analysieren können. Die
  Performance-intensivsten Abläufe sind folgende:
</p>

<ul>
  <li>Bildanalyse ob ein Zug im Bild ist</li>
  <li>Aufnahme Full HD Farbvideo mit Live H.264 Kompression</li>
  <li>Aufnahme Infrarot Bilder</li>
  <li>Clamping Infrarot Bilder von 16 bits zu 8 bits</li>
  <li>Nachträgliche H.264 Kompression der Infrarot Bilder</li>
</ul>

<p>
  Diese Abläufe können sich auch in die Quere kommen, was wir erkennen möchten. Zudem möchten wir erfahren, wann der
  Arbeitsspeicher ausgeht und eventuell auf die Disk geswapped wird.
</p>

<p>
  Um diese Daten aufzunehmen, haben wir nach einer Monitoring-Lösung gesucht. Dazu gibt es diverse Self-Hosting
  Lösungen wie z.B. das alt bekannte Nagios, wovon es mittlerweile auch schon diverse Forks gibt. Diese Lösungen
  können wir allerdings nicht selber betreiben, weil uns die Server dazu fehlen. Ausserdem ist ein solches System
  dafür ausgelegt eine ganze Serverlandschaft zu überwachen und nicht nur einen einzigen. Deswegen ist die
  Installation und Konfiguration einer solchen Lösung sehr aufwändig. Diesen Aufwand wollten wir uns, wenn möglich,
  ersparen.
</p>

<p>
  Die andere Option: Eine SaaS Monitoring-Lösung verwenden. Das Problem hierbei ist, dass es keine Anbieter
  gibt, die das gratis machen. Oder jedenfalls nicht so, dass wir mehrere Monate lang auf den Server zugreifen können.
</p>

<p>
  Deswegen haben wir uns entschlossen, eine eigene Lösung zu schaffen, die wir auf dem Server von Sebastian Häni hosten
  können. Die entworfene Lösung ist so simpel gehalten wie möglich und ist nicht sehr erweiterbar. Die Funktionsweise
  ist so, dass die Thermobox alle 5 Minuten ein Datentupel im JSON Format mit folgenden Werten an den Server per HTTP
  POST schickt:
</p>

<ul>
  <li>CPU Temperaturen in Celsius pro Core</li>
  <li>CPU Auslastung in Prozent pro Core</li>
  <li>Memory Auslastung in Prozent</li>
  <li>Freier Festplattenspeicher in Bytes</li>
  <li>Hostname</li>
</ul>

<p>
  Der Server hängt das Datentupel an eine Datei. Das Frontend liest dann diese Datei aus und erzeugt eine HTML Seite
  die vier Graphen im Browser darstellt. Oben links wird dargestellt, ob die Box online ist. Oben rechts werden die
  CPU Auslastung sowie die CPU Temperatur überlagert dargestellt. Unten links sieht man die Memory Auslastung und
  unten rechts den Verlauf des freien Speicherplatzes.
</p>

<figure>
  <img src="images/thermoscanner/monitoring-screenshot.jpg">
  <figcaption>Screenshot Monitoring</figcaption>
</figure>

<p>
  Weil es ab und zu vorkam, dass die Software nicht so funktionierte wie gewünscht, hat sich die Festplatte gefüllt
  und es konnten keine Aufnahmen mehr gemacht werden. Bevor dies der Fall ist, schickt der Server eine E-Mail an
  uns mit der Information, dass weniger als 5 GiB freier Speicherplatz zur Verfügung steht.
</p>

<figure>
  <img src="images/thermoscanner/screenshot-monitoring-email.png">
  <figcaption>Screenshot Monitoring E-Mail-Benachrichtigung</figcaption>
</figure>

<p>
  Die Anforderungen an den Monitoring-Server sind folgende:
</p>

<ul>
  <li>PHP 7+</li>
  <li>Schreibberechtigung auf <code>data/stats.txt</code></li>
</ul>

<p>
  Um den Server in Betrieb zu nehmen, müssen nur die Sourcen auf den Webserver kopiert und die
  Schreibberechtigung eingerichtet werden. Im <code>ping.ps1</code> Script muss die richtige URL konfiguriert werden
  damit die Datentupel an den richtigen Ort gelangen. Es gibt keinen Mechanismus der die alten Daten löscht. Dazu muss
  manuell die Datei <code>stats.txt</code> geleert werden. Die Graphen stellen jeweils nur die letzten 48 Stunden
  dar. Details über ältere Posts können mit dem Button "Show more" angezeigt werden.
</p>

<p>
  Der Client, also in unserem Fall der Thermo-Scanner, führt das Script <code>ping.ps1</code>, welches ebenfalls im
  Repository zu finden ist, alle 5 Minuten aus. Dieser allgemein als Cron-Job bekannte Prozess kann auf dem Mini-PC,
  auf welchem Windows 10 läuft, mit dem Task Scheduler gesteuert werden. Folgend zwei Screenshots wie der Task
  konfiguriert wurde:
</p>

<figure>
  <img src="images/thermoscanner/monitoring-task-configuration-1.png">
  <figcaption>Konfiguration Monitoring Ping Task Generell</figcaption>
</figure>

<figure>
  <img src="images/thermoscanner/monitoring-task-configuration-2.png">
  <figcaption>Konfiguration Monitoring Ping Task Trigger</figcaption>
</figure>

<p>
  Im Register Actions wurde eine Action hinzugefügt. Diese ist vom Typ "Start a program". Das Programm ist
  <code>powershell</code> und die Argumente sind
  <code>-NoLogo -File C:\repos\thermotrains\src\main\csharp\Scripts\ping.ps1 -WindowStyle Hidden"</code>.
</p>

<p>
  Das Ping Script <code>ping.ps1</code> ist in PowerShell geschrieben. Es liest die CPU Temperaturen, CPU
  Auslastungen und die Memory Auslastung vom OpenHardwareMonitor aus. Um diese Angaben zuverlässig und einfach
  auslesen zu können, gibt es von Windows leider keine vorgegebenen Befehle oder Services. Das Auslesen des
  verbleibenden Speicherplatzes ist jedoch mit PowerShell Hausmitteln möglich.
</p>

<p>
  Eine generell funktionierende Lösung, um die Sensoren der Hardware auszulesen, gibt es nicht. Jeder Hersteller
  programmiert in seinen Treibern eigene Interfaces, um die Werte auszulesen. Die Software OpenHardwareMonitor versucht
  diese verschiedenen Interfaces zusammen zu tragen und stellt diese im einfachen Tool dar. Zusätzlich bietet
  OpenHardwareMonitor die Option an, sich beim Systemstart selber im Tray auszuführen. Dies haben wir auf dem Mini-PC
  so konfiguriert. Wenn OpenHardwareMonitor läuft, kann über die angebotenen WMI Objekte (Windows Management
  Instrumentation) auf die Sensorwerte mit simplen Queries zugegriffen werden. Als Beispiel, hier ein Befehl welcher
  alle CPU Kern Temperaturen in Kelvin ausliest:
</p>

<figure>
  <pre><code>$temp = Get-WmiObject -Namespace "root/OpenHardwareMonitor" Sensor `
  | Where-Object {$_.SensorType -eq 'Temperature' -and $_.Name -like 'CPU Core #*'} `
  | Select-Object Value `
  | ForEach-Object {$_.Value}</code></pre>
  <figcaption>CPU Temperatur Auslesen PowerShell Script</figcaption>
</figure>

<figure>
  <img src="images/thermoscanner/screenshot-openhardwaremonitor.png">
  <figcaption>Screenshot OpenHardwareMonitor</figcaption>
</figure>

<h5>Error Monitoring</h5>
<p>
  In den jeweiligen Komponenten können Errors, Warnings oder gar nicht abgefangene Exceptions
  auftreten. Um diese nicht umständlich im Log suchen zu müssen, sollen diese einfacher eingesehen werden können. Zudem
  sollte bei neuen Fehler gleich eine E-Mail-Benachrichtigung versendet werden.
</p>

<p>
  Dazu haben wir Sentry integriert. Sentry ist ein Dienst, welcher selber gehostet werden kann oder auf sentry.io
  betrieben wird. Da die Gratis-Lizenz mit ihren Einschränkungen für uns ausreicht, müssen wir keine Kosten
  aufwenden. Wenn eine Warnung oder Fehler auftritt, wird dieses Event an die Sentry API gemeldet. Sentry aggregiert
  dann die Events zusammen und schickt bei neuen unentdeckten Events eine E-Mail an uns.
</p>

<figure>
  <img src="images/thermoscanner/screenshot-sentry.png">
  <figcaption>Screenshot Sentry</figcaption>
</figure>

<p>
  Der Dienst ermöglicht es uns die Fehler sofort zu beheben nachdem sie aufgetreten sind. Zudem können wir die
  Warnungen dazu benutzen, um zu sehen wie häufig und wann gewisse Meldungen auftauchen. Zum Beispiel schicken wir
  immer eine Warnung, wenn ein Zug entdeckt, wurde dieser aber "anscheinenden" nach zu kurzer Zeit wieder aus dem Bild
  gefahren ist, dass das eigentlich gar nicht möglich ist. Wenn wir dann sehen, dass dies immer während der Dämmerung
  passiert ist das ein guter Hinweis, was womöglich mit der Bildanalyse schiefläuft.
</p>

<p>
  Um Sentry einzubinden, mussten wir einen eigenen log4net Appender bauen. Die existierenden Appender sind zu alt und
  werden nicht mehr gewartet. Sollte sich in naher Zukunft nichts mehr in diesen Projekten tun, erwägen wir unseren
  Appender selber als NuGet Package zu veröffentlich, damit andere auch davon profitieren können.
</p>

<p>
  Der Sentry Account müsste im kommerziellen Betrieb von der SBB neu erstellt werden. Danach kann im Projekt ein
  neuer DSN (Data Source Name) generiert werden. Dieser muss dann auf dem Mini PC in der Umgebungsvariable mit dem
  Namen <code>SENTRY_DSN</code> abgelegt werden.
</p>

<h4>Interne Kommunikation</h4>
<p>
  Durch die Anbindung von verschiedenen Komponenten und die verteilte Entwicklung sowie möglicher Integration von
  weiteren Komponenten je nach Projektverlauf haben wir uns für eine Microservice-Architektur entschieden. In einer
  solchen Architektur braucht es in der Regel einen Message Broker beziehungsweise eine Message Queue. Um dies
  umzusetzen, haben wir auf Redis gesetzt. Der Redis Server läuft ständig auf dem Mini PC und fährt automatisch hoch
  sobald Windows startet.
</p>

<p>
  Redis kann zwar mehr als nur Broker spielen, wir brauchen diese Features allerdings nicht. Redis zeichnet sich aus
  durch die Einfachheit sowie die grosse Anzahl an production ready Bibliotheken zur Integration in alle gängigen
  und zum Teil auch nicht gängigen Technologien. Ausserdem gibt es ein Command Line Interface mit welchem bequem
  manuell Nachrichten in die Queue geschrieben und diese auch ausgelesen werden können.
</p>

<p>
  Der Einsatz von Redis bringt also das Pattern PubSub (Publish/Subscribe) hervor. Die entwickelten Komponenten
  laufen als eigenständige Prozesse und wissen nicht welche anderen Prozesse laufen. Will eine Komponente mit einer
  anderen kommunizieren, muss sie die Nachricht in die Redis Message Queue schreiben und somit publishen. Will sie
  Nachrichten lesen, muss sie sich auf eine Queue subscriben.
</p>

<p>
  Um auf der Entwicklungsumgebung nicht den Redis Server installieren zu müssen, gibt es ein Redis Docker Image
  welches den Server in einem Container startet. Somit kann eine Instanz ohne Konfiguration on Demand gestartet werden.
  Auf dem Mini PC haben wir zuerst auch mit Docker den Redis Server eingerichtet, weil die Windows Version des Redis
  Servers nicht offiziell von Seite Redis unterstützt wird. Jedoch ist zum Zeitpunkt dieser Arbeit Docker immer noch
  nicht sehr stabil, um Container auf einer Serverplattform laufen zu lassen. Für eine Entwicklungsumgebung ist es ok,
  aber auch nicht ideal. Deshalb haben wir schliesslich den Redis Server, der Microsoft offiziell unterstützt, als
  Windowsdienst installiert.
</p>

<p>
  Während der Entwicklung der Komponenten haben wir uns schlussendlich auf folgende Menge an Befehlen geeinigt.
</p>

<ul>
  <li><code>capture:start </code></li>
  <li><code>capture:stop</code></li>
  <li><code>capture:abort</code></li>
  <li><code>capture:pause</code> (obsolet)</li>
  <li><code>capture:resume</code> (obsolet)</li>
  <li><code>delivery:upload </code></li>
  <li><code>delivery:compress </code></li>
  <li><code>kill</code></li>
</ul>

<p>
  Im Diagramm unten ist dargestellt, wie eine Aufnahme des Zuges auf der Kommunikationsebene abläuft. Es sind sämtliche
  Nachrichten ersichtlich. Wie man sieht, geht jede Nachricht immer zuerst an den Redis Server, welche sie dann weiter
  an die Komponenten, die sich darauf subscribed haben, weiterleitet. Die einzelnen Komponenten werden später erklärt.
</p>

<figure class="full">
  <img src="images/thermoscanner/aufnahme-sequenz-diagramm.png">
  <figcaption>Sequenzdiagramm Aufnahme</figcaption>
</figure>
<!--
To generate the diagram: Go to https://www.websequencediagrams.com/ and paste the following:

title Aufnahme Sequenz

Trigger->Redis: capture:start "1"
Redis->IRReader: capture:start "1"
Redis->VisibleLightReader: capture:start "1"
Redis->TemperatureReader: capture:start "1"

Trigger->Redis: capture:stop
Redis->IRReader: capture:stop
Redis->VisibleLightReader: capture:stop
Redis->TemperatureReader: capture:stop

IRReader->Redis: delivery:compress "1-ir.seq"
Redis->IRCompressor: delivery:compress "1-ir.seq"
IRCompressor->Redis: delivery:upload "1-ir.mp4"
VisibleLightReader->Redis: delivery:upload "1-visible.mp4"
TemperatureReader->Redis: delivery:upload "1-temperature.txt"

Redis->Uploader: delivery:upload "1-ir.mp4"
Redis->Uploader: delivery:upload "1-visible.mp4"
Redis->Uploader: delivery:upload "1-temperature.txt"
-->

<h4>Komponenten Architektur</h4>
<p>
  Damit es einfach ist neue Komponenten zu erstellen, haben wir ein Common-Modul erstellt welches Code bereitstellt,
  der für alle Komponenten derselbe ist. Im Common Modul gibt es eine abstrakte Klasse
  <code>ThermoBoxComponent</code> von welcher konkrete Komponenten erben können. Eine Komponente kann dann definieren,
  welchen Kanälen sie folgen möchte und welchen Handler sie anstossen möchte, wenn eine Nachricht eintritt. Zudem gibt
  es eine Funktion um Nachrichten zu publishen.
</p>

<p>
  Die gewählte Architektur hat sich sehr bewährt. Es ermöglicht den selektiven Betrieb einzelner Komponenten so, dass
  diese isoliert getestet werden können. Zudem kann man auch mal eine Komponente ausschalten, wenn man das Feature
  gerade nicht braucht. Zum Beispiel können wir den Uploader nicht starten, wenn wir nicht möchten, dass Dateien auf den
  Server hochgeladen werden. Es ist keine Konfigurationsänderung notwendig.
</p>

<p>
  Das einzige Problem das wir mit dieser Architektur festgestellt haben ist, dass wir Kameras haben, auf welche mit
  nur einem Windows Prozess zugegriffen werden kann. Die erste Idee war, dass wir eine Komponente haben, welche die
  Farbkamera ausliest, um zu erkennen, ob gerade ein Zug in die Waschstrasse fährt. Eine zweite Komponente greift auf
  dieselbe Kamera zu um die Bilder in ein Video aufzunehmen sobald der Zug erkannt wurde. Dies war leider nicht
  möglich. Also mussten wir in diesem konkreten und einzigen Fall vom Microservice-Pattern abweichen und beide
  Funktionalitäten in derselben Komponente implementieren.
</p>

<h4>Reader Komponenten</h4>
<p>
  Unter den Reader Komponenten verstehen sich Komponenten die eine Aufnahme tätigen oder eben einen Sensor auslesen.
  Die Reader Komponenten subscriben sich auf die Channels <code>capture:start</code>, <code>capture:stop</code> und
  <code>capture:abort</code>.</p>
<p>
  Beim Start Befehl wird ein Timestamp im Format <code>YYYY-MM-DD@HH-mm-ss</code>
  übergeben. Diesen Timestamp verwenden die Reader, um einen eindeutigen Dateinamen zu erzeugen. Wenn diese Dateinamen
  später asynchron über diverse Wege wieder zusammengetragen werden, können sie wieder zusammen gruppiert werden und
  man weiss zu welcher Aufnahme sie gehören. Würden die Reader selber den Timestamp erzeugen, ist durch die asynchrone
  Natur der Redis Message Queue nicht gegeben, dass dieselben Timestamps zum Zuge kommen.
</p>

<p>
  Beim Stopp Befehl beenden die Reader ihre Aufnahme und schicken die erzeugte Datei an den jeweiligen Channel weiter.
</p>

<p>
  Beim Abbruch-Befehl werden die Reader die Aufnahme abbrechen und alle angefallenen Dateien aufräumen. Ein Abbruch
  kann unter anderem stattfinden, wenn die Trigger Komponente sich dafür entscheidet die Aufnahme zu starten, weil sie
  glaubt, der Zug sei eingefahren. Später wird dann aber gemerkt, dass es ein Fehlalarm war und die Aufnahme
  abgebrochen werden sollte. Würde der Trigger warten bis er sich ganz sicher ist, könnte bereits ein Teil des Zuges
  durch gefahren sein ohne dass er aufgenommen wurde.
</p>

<p>
  Um manuelle Befehle zu auszuführen, haben wir ein PowerShell Script <code>Control.ps1</code> erstellt mit welchem
  interaktiv die möglichen Befehle abgesendet werden können.
</p>

<h5>IRReader</h5>
<ul>
  <li>
    <b>Subscription:</b>
    <ul>
      <li><code>capture:start</code></li>
      <li><code>capture:stop</code></li>
      <li><code>capture:abort</code></li>
      <li><code>capture:pause</code></li>
      <li><code>capture:resume</code></li>
      <li><code>kill</code></li>
    </ul>
  </li>
  <li>
    <b>Publish:</b>
    <ul>
      <li><code>delivery:compress</code></li>
      <li><code>delivery:upload</code></li>
    </ul>
  </li>
</ul>
<p class="affix">
  Die Aufgabe des IRReaders ist, die Bilder FLIR Kamera aufzunehmen. Bevor die eigentliche Komponente gestartet wird,
  wird zuerst die Kamera gefunden. Die Suche nach der Kamera geschieht asynchron. Um die Testingzyklen sowie
  grundsätzlich den Start der Komponente zu beschleunigen haben wir folgendes Verfahren entwickelt, um möglichst
  schnell mit der Kamera zu verbinden.
</p>

<p>
  Die Kommunikation zur Kamera handeln wir mit dem FLIR ATLAS SDK ab. Im Hintergrund verwendet das SDK einen GenICam
  Treiber für die Kommunikation. FLIR hat aber weitere proprietäre Parameter in die Kamera verbaut, weswegen wir nicht
  ein Standard GenICam-SDK verwenden.
</p>

<p>
  Zuerst wird ein Discovery Thread gestartet der in einem Event Handler auf gefundene Kameras reagiert. Wird die
  richtige Kamera gefunden, welche dem Suchstring enspricht, wird der originale Thread mit dem Resultat der Kamera
  beendet und retourniert. Die Kamera die gefunden werden soll, ist in der Konfiguration unter dem Key
  <code>IR_CAMERA_NAME</code> abgelegt. Für unsere Kamera lautet dieser "FLIR AX5". Wird die Kamera nicht innerhalb von
  5 Sekunden gefunden, verwenden wir die Emulator-Kamera, die das FLIR ATLAS SDK anbietet. Das heisst, das
  Discovery-Verfahren dauert maximal 5 Sekunden oder weniger.
</p>

<p>
  Die Emulator-Kamera ist hilfreich, wenn man die Komponente testen will ohne, dass man die Kamera im Netzwerk in
  Betrieb nimmt. Das erzeugte Bild ist ein Test-Infrarotbild bei welchem sich ein Balken auf und ab bewegt, sowie einige
  Noise-Pixel die sich zufällig verteilen. Diese Noise Pixel stellen sicher, dass jedes Frame andere Statistikwerte
  aufweist. So können Entwickler testen ob Minimum, Maximum, Median und so weiter für jedes Frame neu und richtig
  berechnet werden. Diese Werte sind wichtig um das Bild dem Betrachter mit einer sinnvollen Temperaturspanne anzeigen
  zu können. Macht die anzeigende Software keine laufende Anpassung der Spanne sind die Noise-Pixel nicht immer
  sichtbar.
</p>

<figure>
  <img src="images/thermoscanner/kamera-emulator.jpg">
  <figcaption>Bild des FLIR ATLAS SDK Kameraemulators</figcaption>
</figure>

<figure>
  <pre><code>2018-01-11 10:51:40,520 IRReader.exe [INFO] Discovering cameras
2018-01-11 10:51:45,640 IRReader.exe [WARN] Fallback to emulator camera
2018-01-11 10:51:47,125 IRReader.exe [INFO] Stopping discovery
2018-01-11 10:51:47,125 IRReader.exe [INFO] Connecting to camera: Camera Emulator
2018-01-11 10:51:48,229 IRReader.exe [INFO] Connecting to redis on localhost
2018-01-11 10:51:48,893 IRReader.exe [INFO] Connected to redis
2018-01-11 10:51:48,894 IRReader.exe [INFO] Subscribing to cmd:capture:start
2018-01-11 10:51:48,904 IRReader.exe [INFO] Subscribing to cmd:capture:stop
2018-01-11 10:51:48,905 IRReader.exe [INFO] Subscribing to cmd:capture:abort
2018-01-11 10:51:48,905 IRReader.exe [INFO] Subscribing to cmd:capture:pause
2018-01-11 10:51:48,906 IRReader.exe [INFO] Subscribing to cmd:capture:resume
2018-01-11 10:51:48,907 IRReader.exe [INFO] Subscribing to cmd:kill</code></pre>
  <figcaption>IRReader Startup Log</figcaption>
</figure>

<p>
  Die Infrarot Kamera wird in der Regel eine Betriebstemperatur von ca. 40 - 50° C einnehmen. Die Kamera hat eine
  interne Mappingtabelle sowie ein Kalibrierungsverfahren, um die Eigentemperatur zu kompensieren damit diese keine
  Einflüsse auf das aufgenommene Bild haben. FLIR nennt den Prozess NUC (Non Uniformity Correction). Ein NUC dauert
  ca. 0.2 - 0.5 Sekunden und blockiert die Aufnahme. Zeichnet man Bilder auf, hat man während eines NUCs ein schwarzes
  Bild. Die Standardkonfiguration der Kamera ist, dass alle 4 Minuten ein NUC gemacht wird um eventuelle sich
  veränderte Temperaturen zu kompensieren. Zusätzlich startet die Kamera den NUC, wenn sich die Eigentemperatur um
  einen einstellbaren Threshold verändert hat. Gerade wenn man die Kamera in einem Büro in Betrieb nimmt, wird sie
  anfänglich ca. jede Minute einen NUC durchführen bis sie die normale Betriebstemperatur erreicht hat. Der NUC ist
  übrigens hörbar, weil der Verschluss mechanisch betätigt wird. Es hört sich wie ein Klicken an.
</p>

<p>
  Wir möchten in unseren Aufnahmen keine schwarzen Frames haben. Deshalb wird unmittelbar bevor eine Aufnahme
  gestartet wird ein NUC durchgeführt. Dann können wir davon ausgehen, sofern die Kamera länger als ein paar Minuten in
  Betrieb ist, dass der nächste NUC erst in 4 Minuten durchgeführt wird. Der anfängliche NUC dauert auch nicht zu lange,
  so, dass wir eventuell sogar den Anfang verpassen würden. Die NUC Einstellungen können via die Parameter
  <code>NUCMode</code>, <code>NUCTempDelta</code> und <code>NUCIntervalFrames</code> eingerichtet werden.
</p>

<p>
  Die Aufnahme steuern wir über den FLIR ATLAS SDK Recorder. Dieser kann Videos von der Kamera aufnehmen. Diese
  werden im FLIR proprietären Format SEQ abgespeichert. Das SEQ Format ist nichts anderes als aneinandergehängte TIFF
  Dateien, welche mit Rohdaten der Kamera befüllt sind. Zusätzlich gibt es EXIF Metadaten. Da die Kamera mit 14 Bit
  aufnimmt und keine Kompression stattfindet, sind die resultierenden Dateien sehr gross. Es gibt keine Möglichkeit
  die Bilder sofort zu komprimieren.
</p>

<figure>
  <img src="images/thermoscanner/seq-filesize-formula.png">
  <figcaption>Formel zur Berechnung der SEQ-Dateigrösse</figcaption>
</figure>

<p>
  Nehmen wir in der vollen Auflösung und Framerate auf, werden die Dateien schnell so gross, dass sie gar nicht mehr
  auf der Disk Platz hätten. Deshalb haben wir die Framerate auf ein Frame pro Sekunde verringert. Die Auflösung zu
  verringern macht keinen Sinn, weil wir dann Bilddetails verlieren.
</p>

<p>
  Die Kontrolle der Kamera über das FLIR ATLAS SDK ist etwas umständlich. Die Control-Library ist nicht sehr stabil. Es
  kann zu zufälligen Exceptions kommen, wenn ein Befehl abgesetzt wird. Um Abhilfe zu schaffen, haben wir eine Retry
  Methode gebaut, der ein Callback zur Befehlsausführung mitgegeben wird sowie ein Callback zur Überprüfung ob es
  erfolgreich war. Diese Retry Methode führt den Befehl so lange wiederholt aus, bis keine Exception geworfen wird
  und der zweite Callback sagt, dass es funktioniert hat. Leider können wir nicht in den Code hineinschauen, weil er
  proprietär und obfuscatet ist.
</p>

<figure>
  <pre><code>/// &lt;summary&gt;
/// Tries to execute an action. If an exception is thrown, it tries again until a
/// threshold is reached.
/// &lt;/summary&gt;
/// &lt;param name="action"&gt;Action with potential exception thrown&lt;/param&gt;
/// &lt;param name="testSuccess"&gt;Function to test the success of the action&lt;/param&gt;
private static void Retry(Action action, Func&lt;bool&gt; testSuccess)
{
  var tries = 0;
  const int maxTries = 5;

  while (tries &lt; maxTries)
  {
    try
    {
      action.Invoke();

      if (testSuccess.Invoke())
      {
        return;
      }

      throw new Exception("Camera state was not as expected");
    }
    catch (Exception ex)
    {
      Log.Warn("Exception while executing camera command", ex);
    }

    tries++;
    Log.Warn($"Failed executing camera command (try {tries} of {maxTries})");
    Thread.Sleep(10);
  }

  Log.Error($"Could not execute command after {tries} tries.");
}</code></pre>
  <figcaption>FLIR ATLAS SDK Code zur wiederholten Befehlsausführung</figcaption>
</figure>

<p>
  Wird die Aufnahme gestoppt, erzeugt der IRReader drei Dateien.
</p>

<ul>
  <li>SEQ Datei: Video-Rohdaten</li>
  <li>JPG Datei: Snapshot des ersten Frames des Videos</li>
  <li>JSON Datei: Aktuelle Device Parameter</li>
</ul>

<p>
  Die SEQ Datei wird via <code>delivery:compress</code> an den IRCompressor übergeben, welcher die Datei komprimiert,
  damit dieser übertragen werden kann. Das Übertragen der Rohdaten ist erstens zu langsam, weil Züge schneller
  vorbeifahren als wir hochladen könnten und zweitens haben wir nicht genügend Serverspeicherplatz im die Dateien
  abzuspeichern. Zu beachten ist, dass die SEQ Datei nur die Sensorwerte enthält. Es sind noch keine Temperaturwerte
  erzeugt worden. Um diese zu berechnen müssen weitere Parameter hinzugenommen werden.
</p>

<p>
  Die JPG Datei dient uns zum Vergleich des komprimierten Videos. Damit sehen wir, ob der Kompressionsalgorithmus
  keine offensichtlichen Fehler beinhaltet. Weiter dauern das Kompressionsverfahren sowie der Upload einige Zeit.
  Möchten wir trotzdem schon sehen was aufgenommen wurde, müssten wir warten. Dieser Snapshot kann allerdings sehr
  schnell und ohne langwierige Kompression hochgeladen werden. So sehen wir direkt, ob und welcher Zug sich im Bild
  befindet.
</p>

<p>
  Die JSON Datei enthält die 315 verfügbaren Geräteparameter der FLIR A65 Kamera die zur Zeit der Aufnahme
  konfiguriert waren. Je nach Einstellung resultieren andere Bilder. So gibt es zum Beispiel einen High Gain oder Low
  Gain Modus. Diese Angaben dienen uns als Kontext bei der Interpretation und Weiterverarbeitung der aufgenommenen
  Bilder. Wir speichern diese Informationen bei jeder Aufnahme mit ab. Enthalten sind auch Sensorinformationen der
  Kamera, wie z.B. Gehäuse- oder Sensortemperatur. Zur Serialisierung der Kamera-Parameter haben wir eigene Modelklassen
  geschrieben die wir befüllen und dann mit <a href="#ref-json-net">Json.NET</a> serialisieren.
</p>

<figure>
  <pre><code>{
  "Parameters": [{
    "Name": "SensorTemperature",
    "Description": "FPA temperature in Celsius",
    "Value": "28.4"
  }, ...]
}</code></pre>
  <figcaption>Ausgelesen Device Parameter in JSON Format</figcaption>
</figure>

<p>
  Hier ein Auszug aus den Parametern die für uns zum Teil relevant sind:
</p>

<figure>
  <code>DeviceVendorName, DeviceModelName, DeviceManufacturerInfo, DeviceVersion, DeviceID, DeviceUserID,
    DeviceScanType, DeviceTemperatureSelector, DeviceTemperature, CameraFirmwareVersion,
    SensorFirmwareVersion, SensorResolution, SensorFocalLength, SensorFrameRate, SensorTemperature,
    HousingTemperature, CameraOptions, SensorWidth, SensorHeight, SensorDigitizationTaps, WidthMax, HeightMax, Width,
    Height, OffsetX, OffsetY, BinningHorizontal, BinningVertical, DecimationHorizontal, DecimationVertical, PixelFormat,
    PixelCoding, PixelSize, PixelColorFilter, AcquisitionMode, AcquisitionFrameCount, LineSelector,
    LineMode, UserOutputSelector, LineDebounceFactor, SyncMode, SensorGainMode, NUCMode, NUCTempDelta,
    NUCIntervalFrames, BaudRate, ShutterPosition, CorrectionMask, R, Spot, B, F, O, FNumber,
    Transmission, OInt, TemperatureLinearMode, TemperatureLinearResolution, RThg, J1, OAShift, ObjectEmissivity,
    ReflectedTemperature, WindowTransmission, WindowTemperature, AtmosphericTransmission, AtmosphericTemperature,
    SensorVideoStandard, ImageAdjustMethod, VideoOrientation, PlateauLevel, MaxAGCGain</code>
</figure>

<h5>VisibleLightReader</h5>
<ul>
  <li>
    <b>Subscription:</b>
    <ul>
      <li><code>capture:start</code></li>
      <li><code>capture:stop</code></li>
      <li><code>capture:abort</code></li>
      <li><code>capture:pause</code></li>
      <li><code>capture:resume</code></li>
      <li><code>kill</code></li>
    </ul>
  </li>
  <li>
    <b>Publish:</b>
    <ul>
      <li><code>capture:start</code></li>
      <li><code>capture:stop</code></li>
      <li><code>capture:abort</code></li>
      <li><code>delivery:upload</code></li>
    </ul>
  </li>
</ul>
<p class="affix">
  Aus technischen Limitierungen erfüllt der VisibleLightReader zwei Aufgaben was gegen das Microservice Prinzip
  verstösst. Die Komponente liest zum einen den Bildstream der Basler Kamera, um festzustellen, ob sich ein Zug in das
  Bild hineinbewegt hat. Hat sie einen Zug detektiert, meldet sie mit <code>capture:start</code> dies an alle
  Komponenten weiter so wie auch sich selber. Selber startet sie nämlich dann auf den Befehl die Aufnahme in eine
  Videodatei. Der Name der Komponente kommt davon, dass wir damit die für das menschliche Auge sichtbaren Wellenlängen
  aufnehmen. Dass Licht und Wellenlängen nicht dasselbe ist, ist uns bewusst. Der Name ist also wissenschaftlich nicht
  ganz korrekt.
</p>

<p>
  Die Komponente benutzt zur Kommunikation das Basler Pylon SDK. Da die Firma Basler einer der führenden Hersteller
  von Industriekameras ist, ist ihr SDK auch weit aus durchdachter und stabiler als das Pendant von FLIR.
  Man merkt, dass FLIR einige Patterns vom Pylon SDK abgekupfert hat.
</p>

<p>
  Die technische Limitierung ist, dass man nur von einem Windows Prozess auf die über USB angeschlossene Basler
  Kamera zugreifen kann. Der Prozess blockiert die Kamera und kein zweiter kann mehr darauf zugreifen. Das heisst, wir
  mussten den Image Grab Loop so schreiben, dass die Bilder zum einen vom Erkennungsalgorithmus verarbeitet
  und zum anderen in eine Videodatei geschrieben werden. Das setzt natürlich voraus, dass die
  Erkennungsphase nicht länger dauert als die Aufnahme eines Frames. Ansonsten verlieren wir entweder Frames oder der
  Buffer wird unendlich gross.
</p>

<p>
  Das folgende Flow Chart erklärt, wie wir das gelöst haben. In der Implementation haben wir eine Klasse
  <code>Recorder</code> geschrieben, welche intern einen State hat, ob sie am Aufnehmen ist oder nicht. Der Image Grab
  Loop schickt die Bilder dann an eine Instanz der Klasse. Der Recorder weiss dann selber, ob er das Bild verwerfen
  oder aufnehmen soll.
</p>

<p>
  Die Prozesse, die gelb umrandet sind, werden später in einem weiteren Flow Chart erklärt. Den gesamten Flow in einem
  einzigen Flow Chart darzustellen, ist leider zu unübersichtlich.
</p>

<figure>
  <img src="images/thermoscanner/detection/flowchart-grab-loop.png">
  <figcaption>VisibleLightReader: Grab Loop Flow Chart</figcaption>
</figure>

<p>
  Zuerst finden wir die richtige Kamera mithilfe des Namens, welcher in der Konfiguration unter dem Key
  <code>VISIBLE_LIGHT_CAMERA_NAME</code> abgelegt ist. Dazu mussten wir glücklicherweise keinen asynchronen
  Discover-Service bauen. Im Pylon Framework gibt es einen synchronen Aufruf, der das Kameraobjekt liefert. Nach dem
  Öffnen konfigurieren wir die Kameraparameter für uns passend. Wir stellen ein, dass nur 1 Frame pro Sekunde
  aufgezeichnet werden kann. Das machen wir, weil wir ansonsten zu grosse Dateien erhalten, für welche wir kein
  Speicherplatz haben. Ausserdem haben wir in der Nacht sowieso nicht mehr als 1 FPS, weil die Beleuchtungszeit so
  hoch wird. Den Image Grabber stellen wir so ein, dass er immer das aktuellste Frame der Kamera holt. Somit baut
  sich kein versteckter Buffer unendlich auf. Den Grab Loop entwickeln wir wie erwähnt selber, weil wir mehr Kontrolle
  brauchen.
</p>

<p>
  In der Methode <code>HandleStateChange</code> verarbeiten wir Befehle die asynchron von extern hineingekommen sind.
  Das sind also Befehle der Message Queue.
</p>

<figure>
  <img src="images/thermoscanner/detection/flowchart-handle-state-change.png">
  <figcaption>VisibleLightReader: Handle State Change Flow Chart</figcaption>
</figure>

<p>
  Es geht nun weiter in den Detektionsalgorithmus. Dies ist eine eigene Klasse. Die 4 übergebenen Bilder von vorher
  sind bereits vorverarbeitet. Sie sind in Graustufen, verkleinert und enthalten nur den Ausschnitt des Bildes, auf
  welchem sich die Räder des Zuges befinden.
</p>

<figure>
  <img src="images/thermoscanner/detection/background.jpg">
  <figcaption>Gecroppter Background mit Gleis</figcaption>
</figure>

<figure>
  <img src="images/thermoscanner/detection/snapshot.jpg">
  <figcaption>Einfahrt Zug aufgenommen von Basler Kamera</figcaption>
</figure>

<p>
  Das Erkennen des Zuges mit dem ganzen Bild ist sehr schwierig, wie auch unnötig. Der Ausschnitt verändert sich nicht
  solange die Kabine nicht bewegt wird. Im gesamten Hintergrund gibt es ca. 10 weitere Gleise auf welchen Zügen
  vorbeifahren können. Zu Beginn haben wir den Ausschnitt nicht ausgeschnitten. Dass wir dann mehr aufnehmen, war uns
  klar. Da wussten wir aber noch nicht, dass wir an einer neuralgischen Stelle aufnehmen werden, wo jede Minute
  mindestens ein Zug im Hintergrund vorbeifährt.
</p>

<p>
  In der Detektionsklasse ist das Hintergrundbild abgelegt. Also das Bild der Kamera, auf welchem sich kein Zug
  befindet. Wir nennen es von nun an Background. Der Background wird beim Start sowie in regelmässigen Abständen immer
  wieder neu abgelegt. Wenn wir den Background ablegen, führen wir zuerst einen Blurfilter aus. Danach berechnen wir die
  arithmetische durchschnittliche Helligkeit. Wir speichern also nur einen Wert zwischen 0 und 255.
</p>

<p>
  Um auf dem Bild immer etwas zu sehen, müssen wir periodisch die Beleuchtungszeit korrigieren. In der Nacht
  haben wir Beleuchtungszeiten von einer Sekunde und am Tag von wenigen Millisekunden. Aber auch mit der
  Beleuchtungszeit von einer Sekunde haben wir, je nach eingeschalteten Flutlichtern, zu wenig gesehen. Deswegen weisen
  wir die Kamera an den Gain automatisch zu korrigieren. Diese automatische Korrektur dauert im Extremfall bis
  zu zwei Sekunden. Wir geben den Befehl zur automatischen Korrektur selber. Würden wir es der Kamera überlassen
  dies periodisch zu tun, könnten wir im falschen Moment eine Background-Momentaufnahme machen, was dann fälschlicherweise einen Zug detektiert.
</p>

<p>
  Auf dem folgenden Diagramm sieht man die von uns in der ersten Testphase aufgezeichneten Beleuchtungszeiten
  (links) und Gains (rechts) aggregiert auf einen Tagesverlauf.
</p>

<figure>
  <img src="images/thermoscanner/detection/exposure-gain.png">
  <figcaption>Basler Kamera Belichtungszeit und Gain</figcaption>
</figure>

<p>
  Haben wir alle diese State-Geschichten hinter uns, können wir die eigentlichen Bilder evaluieren.
  Nachfolgend das Flow Chart, mit den bis dort durchlaufenen Schritten:
</p>

<figure>
  <img src="images/thermoscanner/detection/flowchart-detection.png">
  <figcaption>VisibleLightReader: Detektion Flow Chart</figcaption>
</figure>

<p>
  Bisher haben wir also einen Background-Durchschnittswert und neu auch die Durchschnittswerte der aktuellen
  Bilder. Wir können jetzt vergleichen, ob der aktuelle Wert dunkler ist als der Backgroundwert. Zugräder sind nämlich
  dunkler als der Schotter und produzieren einen deutlich sichtbaren Einfall der Bildhelligkeit. Auf dieses einfache Verfahren
  erst mit der Zeit gekommen, nachdem viele andere Ansätze fehl schlugen. Die Umstände der echten Welt sind viel abstruser, als
  man sich das im Labor vorstellt. Der Kasten ist diversen Umständen ausgeliefert und muss damit umgehen können.
</p>

<ul>
  <li>Sonnenaufgang und Sonnenuntergang</li>
  <li>Regen und Schnee erzeugen Noise im Bild, welcher potentiell detektiert wird</li>
  <li>Wenn Schnee liegt, sieht das Bild anders aus</li>
  <li>Personen auf dem Fussweg vor der Kabine</li>
  <li>Regen kann an der Scheibe Tropfen bilden</li>
  <li>Nebel kann die Sicht verschlechtern</li>
  <li>Reflexionen vom Prime Tower können Kaustiken bilden</li>
  <li>In der Nacht vorbeifahrende Züge werfen Licht auf den Boden</li>
  <li>Unterschiedliche Fahrtrichtungen</li>
</ul>

<p>
  Unser erstes Verfahren war mit dem Backgroundbild die sich veränderten Pixel zu tracken und festzustellen, ob von
  links oder rechts ein genug grosses Objekt hineinfährt. Das hat schon gar nicht funktioniert, wenn man den ganzen
  Bildbereich analysiert. Man könnte zwar meinen man muss einfach einen genug hohen Raum festlegen. Anders gesagt
  die Bounding Box müsste eine gewisse Höhe haben. Nur steht im Hintergrund das Bahnviadukt von Zürich. Wenn
  gleichzeitig ein Zug auf einem der unteren Gleise und einem auf dem oberen Gleis durchfährt haben wir eine Bounding
  Box die genau gleich hoch ist wie ein Zug der durch die Waschstrasse fährt. Jetzt kann man sich fragen, wie häufig
  das vorkommt. Wir mussten feststellen, dass das ca. alle 5 Minuten passiert. Ansonsten hätte dieses Verfahren sehr
  gut funktioniert soweit wir es getestet haben.
</p>

<p>
  Nachdem wir den Analyseausschnitt auf das untere Gleis gelegt haben, mussten wir feststellen,
  dass in der Nacht vorbeifahrende Züge einen starken Lichteinfall auf den Boden werfen.
  Dies hat den Detektor eine Aufnahme auslösen lassen.
</p>

<figure>
  <img src="images/thermoscanner/detection/night-light.jpg">
  <figcaption>Von anderen Zügen einfallendes Licht</figcaption>
</figure>

<p>
  Wir mussten also ein einfacheres Verfahren entwickeln, welches nur die Helligkeit zum Background vergleicht.
  Mit fleissiger Überwachung des Detektionsalgorithmus und ständigem Nachschrauben sowie den praktischen Tools, die wir
  geschaffen haben, konnten wir die Thresholds ständig verbessern. Das nachfolgende Flow Chart zeigt auf was passiert,
  wenn der Threshold erreicht wurde oder nicht. Zur Übersicht wurde der End-State weggelassen. Wenn kein Pfeil von einem
  Kasten weggeht, endet der Flow dort.
</p>

<figure>
  <img src="images/thermoscanner/detection/flowchart-evaluate.png">
  <figcaption>VisibleLightReader: Evaluierungs-Flow Chart</figcaption>
</figure>

<p>
  Die Detektorklasse meldet die Befehle über C# Events an die Klasse die den Grab Loop beherbergt. Diese erzeugt dann
  asynchron den Befehl welchen sie an die Message Queue weiterleitet.
</p>

<p>
  Um den Detektionsalgorithmus herum entstanden eine ganze Menge an Konstanten die wir nach besten Kenntnissen
  vorkonfigurierten und dann während der Testphase ständig nachkorrigierten. Die aufgeführten Werte sind die
  schlussendlichen Resultate davon.
</p>

<figure>
  <table>
    <thead>
    <tr>
      <th>Konstante</th>
      <th>Beschreibung</th>
      <th>Einheit</th>
      <th>Wert</th>
    </tr>
    </thead>
    <tbody>
    <tr>
      <td>NoBoundingBoxBackgroundThreshold</td>
      <td>Solange sich nichts bewegt, wird der Background neu initialisiert</td>
      <td>Sekunden</td>
      <td>30</td>
    </tr>
    <tr>
      <td>MinTimeAfterExit</td>
      <td>Minimale Dauer zwischen zwei Aufnahmen</td>
      <td>Sekunden</td>
      <td>60</td>
    </tr>
    <tr>
      <td>MinTimeAfterEntry</td>
      <td>Minimale Dauer einer Aufnahme. Ist sie kürzer wird sie abgebrochen</td>
      <td>Sekunden</td>
      <td>60</td>
    </tr>
    <tr>
      <td>MaxRecordingDuration</td>
      <td>Maximale Dauer einer Aufnahme</td>
      <td>Minuten</td>
      <td>45</td>
    </tr>
    <tr>
      <td>AutoExposureTimeout</td>
      <td>Zeit, die die Kamera braucht, um die Belichtung einzustellen</td>
      <td>Sekunden</td>
      <td>2</td>
    </tr>
    <tr>
      <td>ExitThreshold</td>
      <td>Anzahl Bestätigungen, dass kein Zug mehr da ist</td>
      <td>Einheit</td>
      <td>8</td>
    </tr>
    <tr>
      <td>ExitScheduleTime</td>
      <td>Dauer des Auslaufens der Aufnahme (damit das Video das Wegfahren des Zuges aufnimmt)</td>
      <td>Sekunden</td>
      <td>20</td>
    </tr>
    </tbody>
  </table>
  <figcaption>Konstanten des Detektionsalgorithmus</figcaption>
</figure>

<p>
  Insgesamt sind wir mit der Erkennungsrate des Verfahrens zufrieden. Es detektiert zwar eindeutig zu viele Bilder als
  Zug, jedoch wird kein einziger Zug verpasst. Wenn man folgende Beispiele anschaut, ist es aber erstaunlich, wie gut es
  bei schlechten Sichtverhältnissen funktioniert.
</p>

<figure>
  <img src="images/thermoscanner/detection/rain-no-train.jpg">
  <figcaption>Fälschlicherweise Aufnahme gestartet</figcaption>
</figure>

<figure>
  <img src="images/thermoscanner/detection/rain-train.jpg">
  <figcaption>Aufnahme wurde gestartet</figcaption>
</figure>

<p>
  Die Erkennungsrate lag am Ende der ersten Testphase mit verbessertem Algorithmus bei 99%.
  27 Aufnahmen wurden gestartet und hochgeladen, auf welchen kein Zug war. In der zweiten Testphase
  wurden 178 von 178 Zügen erkannt. Die Anzahl falsch erkannter Züge lag bei 49. Als Grund dafür sehen wir das
  schlechte Wetter sowie die Wassertropfen, die sich auf dem Sichtglas festgesetzt haben. In einer
  neuen Version der Kabine wäre ein Glas, welches keine Tropfen ansetzt, sicher besser. Allenfalls würde auch
  ein kleines Dach Abhilfe schaffen (welches wir eigentlich ursprünglich bei Almatec bestellt hatten).
</p>

<p>
  Die Sensitivität liegt also bei 0.78, trotz dem schlechten Wetter mit Sturm Burglind. Die Spezifität ist
  schwierig zu beziffern, weil wir künstlich etwa ein Zeitintervall für richtig negativ erkannte Bilder festlegen
  müssten. Oder es müssten in sämtlichen Bildern Features als richtig negativ erkannt und gezählt werden, was unmöglich ist.
</p>

<p>
  Nachdem wir die erste Durchlaufwaschstrasse besucht haben, bemerkten wir, dass Züge zuerst an die Waschstrasse
  heranfahren und dann dort ein paar Minuten verweilen. Während dieser Zeit kommunizieren Sie mit dem
  Waschstrassenleiter oder bedienen die Steueranlage, je nach Typ der Anlage. Für uns heisst das, dass auf dem Video
  der Zug zu Beginn einfach steht und sich nicht bewegt. Das erzeugt unnötig grosse Videodateien. Deshalb hatten wir
  die Idee das Video zu pausieren und fortzusetzen, wenn sich der Zug in Bewegung setzt. Das hat bei gutem
  Wetter gut funktioniert. Wir haben die Bewegung auf dem Bild mit dem vorherigen Bild analysiert und konnten so sagen, ob sich etwas bewegt hat.
  Leider funktionierte das Verfahren bei Nacht nicht, da diverse Lichtquellen Bewegung
  verursachen. Dasselbe bei schlechtem Wetter, wenn Regentropfen oder Schnee ins Bild kommen. Deshalb haben
  wir die Pausierung und Fortsetzung wieder entfernt. Die Reader selber sind aber noch in der Lage zu pausieren und
  fortzusetzen, wenn ein manueller Befehl eintrifft.
</p>

<h5>TemperatureReader</h5>
<ul>
  <li>
    <b>Subscription:</b>
    <ul>
      <li><code>capture:start</code></li>
      <li><code>capture:stop</code></li>
      <li><code>capture:abort</code></li>
      <li><code>kill</code></li>
    </ul>
  </li>
  <li>
    <b>Publish:</b>
    <ul>
      <li><code>delivery:upload</code></li>
    </ul>
  </li>
</ul>
<p class="affix">
  Um unseren TEMPer2 Temperatursensor auszulesen, mussten wir das USB HID (Human Interface Device) konsumieren. Vom
  Hersteller gibt es nur eine Bedienersoftware ohne Code Beispiele oder Support für Entwickler. Glücklicherweise
  haben wir ein .NET Codebeispiel gefunden, welches jedoch etwas veraltet ist. Wir haben die Konstanten, die der
  Autor gefunden hat übernommen und den Code neu geschrieben. Im Artikel
  <a href="#ref-temper-in-csharp">Code Project: Temper USB Thermometer in C#</a> wurde allerdings von einem älteren
  TEMPer Gerät ausgegangen, welches nicht zwei Sensoren hat. Deswegen haben wir anfangs nicht gemerkt, dass wir
  eigentlich den falschen Sensor auslesen.
</p>

<p>
  Beim Befehl <code>capture:start</code> macht der TemperatureReader nichts anderes als sich den übermittelten
  Timestamp merken. Würde er nämlich bereits jetzt die Temperatur auslesen und mit <code>delivery:upload</code>
  weiterschicken, hat ein späterer Abbruch durch <code>capture:abort</code> keine Folgen mehr und die Datei wäre
  bereits auf dem Server. Erst bei <code>capture:stop</code> zeichnet der Sensor die Temperatur in eine Datei auf die
  er dann weiter schickt.
</p>

<h5>WeatherReader</h5>
<ul>
  <li>
    <b>Subscription:</b>
    <ul>
      <li><code>capture:start</code></li>
      <li><code>capture:stop</code></li>
      <li><code>capture:abort</code></li>
      <li><code>kill</code></li>
    </ul>
  </li>
  <li>
    <b>Publish:</b>
    <ul>
      <li><code>delivery:upload</code></li>
    </ul>
  </li>
</ul>
<p class="affix">
  In der zweiten Aufnahmephase haben wir uns entschieden den TemperatureReader nicht mehr zu verwenden, weil die
  Messdaten nicht brauchbar sind. Um die aufgenommenen IR Bilder trotzdem auswerten zu können haben wir den
  WeatherReader implementiert der aktuellen Wetterdaten von <a href="#ref-openweathermap">OpenWeatherMap</a>
  herunterlädt.
</p>

<p>
  OpenWeatherMap bietet eine gratis API an, um das aktuelle Wetter abzufragen. Um alte Daten abzufragen, müsste man
  bezahlen. Deswegen haben wir uns entschieden die Daten jeweils dann abzufragen, wenn die Aufnahme gemacht wird.
  Durch das müssen wir nicht bezahlen. Die Daten können über einen HTTP GET Request abgefragt werden:
</p>

<figure class="affix">
  <pre><code>http://api.openweathermap.org/data/2.5/weather?q=Zürich,CH&appid={openWeatherMapApiKey}</code></pre>
  <figcaption>OpenWeatherMap API Request URL</figcaption>
</figure>

<p>
  Die Antwort, die zurück kommt sieht dann ungefähr so aus:
</p>

<figure class="affix">
  <pre><code>{
  "coord": {"lon": 8.54, "lat": 47.37},
  "weather": [{
    "id": 802,
    "main": "Clouds",
    "description": "scattered clouds",
    "icon": "03n"
  }],
  "main": {
    "temp": 280.06,
    "pressure": 1016,
    "humidity": 71,
    "temp_min": 276.15,
    "temp_max": 284.15
  },
  "visibility": 10000,
  "wind": {"speed": 7.2, "deg": 240},
  "clouds": {"all": 40},
  "dt": 1514697600,
  "sys": {
    "sunrise": 1514704386,
    "sunset": 1514735106
  }
}</code></pre>
  <figcaption>OpenWeatherMap API Response</figcaption>
</figure>

<p>
  Der WeatherReader setzt eine Internetverbindung voraus. In der Zeit als das LTE Modul ausgefallen ist, hatten wir
  somit keine Wetterdaten sammeln können. Leider werden bei der kostenlosen Variante die Wetterdaten nur alle 2 Stunden aktualisiert.
  Etwas unschön ist, dass die Wetterdaten nur alle 2 Stunden aktualisiert werden, wenn man nicht
  zahlt.
</p>

<p>
  Der API Key wird auf dem Mini PC in der Umgebungsvariable <code>OPEN_WEATHER_MAP_API_KEY</code> abgelegt, damit
  dieser nicht in einer im Repository eingecheckten Datei gespeichert werden muss.
</p>

<h4>Delivery Komponenten</h4>

<h5>IRCompressor</h5>
<ul>
  <li>
    <b>Subscription:</b>
    <ul>
      <li><code>delivery:compress</code></li>
      <li><code>kill</code></li>
    </ul>
  </li>
  <li>
    <b>Publish:</b>
    <ul>
      <li><code>delivery:upload</code></li>
    </ul>
  </li>
</ul>
<p class="affix">
  Die Bilder von der FLIR Infrarotkamera können mithilfe des FLIR ATLAS SDKs leider nur als SEQ Datei abgespeichert werden.
  Diese Dateien sind zu gross, um sie in vernünftiger Zeit über eine Mobilfunkantenne zu übertragen.
  Eventuell käme es dann zu einem nicht aufholbaren Rückstand in der
  Übertragungsqueue. Der andere Grund ist der begrenzte Speicherplatz des Servers, auf den die Dateien übertragen werden.
  Dieser verfügt nur über 50 GiB Speicherplatz. Dies ist jedoch ein Soft-Limit wie wir bemerkt
  haben. Wir möchten also die SEQ Dateien in ein anderes Format komprimieren, um die Daten zu übertragen. Da es sich um
  aufeinander folgende Bilder handelt, würde sich ein Videoformat anbieten. Das proprietäre Format von FLIR SEQ mussten
  wir selber reverse engineeren, um die Inhalte zu verstehen und auslesen zu können. FLIR hat kein Interesse diese
  Innereien zu dokumentieren oder Gratissoftware anzubieten, die diese konvertieren kann. Es gibt nämlich Bezahltools von
  FLIR, die aber mehrere zehntausend Franken kosten. Deswegen haben wir es selber entwickelt.
</p>

<p>
  Das SEQ Format ist eigentlich nichts anderes als mehrere rohe TIFF Bilder hintereinander gehängt. Die FLIR Kamera
  nimmt Bilder mit 14 Bit Auflösung pro Pixel auf. Diese Pixel werden mit zwei Null-bits mit 16 Bit abgespeichert.
  Wir können mit dem FLIR ATLAS SDK auf diese rohen Daten lesend zugreifen. Allerdings erhalten wir ein Byte-Array.
  Das heisst, wir müssen dieses in ein ushort-Array umkonvertieren und jeweils zwei Bytes mergen. Das müssen wir für
  jedes Bild im Video machen. Jetzt haben wir Bilder mit 16 Bit pro Pixel in einem Format das wir weiterverarbeiten
  können. Das Problem ist nun, wenn wir aus diesen Graustufen Bildern ein Video machen möchten, müssten wir einen Codec
  finden der 16 bit Graustufen unterstützt. Das gibt es leider nicht in einem breit unterstützten Codec. Apple hat die
  <a href="#ref-apple-prores">ProRes Codec Familie</a> die bis zu 12 Bit sampeln kann. Wir müssen also die 16
  respektive 14 Bits auf die gängige Samplerate von 8 bits herunterbringen.
</p>

<p>
  Die Rohdaten aus der SEQ Datei sind absolute und linear verteilte Sensorwerte. Das heisst der Wert 0 entspricht dem
  niedrigsten Sensorwert und 16'384 (2<sup>14</sup>) entspricht dem höchsten. Wenn wir ein Zug aufnehmen, geht dessen
  Temperaturspanne nicht von 0 bis 16'384, sondern bewegt sich im Bereich von wenigen Celsius. Die kältesten Objekte
  sind so kalt wie die Aussentemperatur und die heissesten Objekte, die Bremsen und Achsen, bis zu 40°C. Züge fahren
  idealerweise nicht mit Vollgas auf die Waschstrasse zu machen eine Vollbremsung, was die Bremsen
  zum Glühen bringen würde.
</p>

<p>
  Mit dieser Erkenntnis haben wir ausprobiert, was es heissen würde, wenn wir die minimalen maximalen Pixelwerte auslesen
  und die Werte mit dem Offset "clampen". Nach dem Clamping nehmen wir die ganze Range und skalieren sie auf 8 Bit.
  Dabei können wir auch gleich den Präzisionsverlust berechnen.
</p>

<figure>
  <pre><code>Präzisonsverlust = Math.Max(0, (1 - (256f / (max - min))) * 100)</code></pre>
  <figcaption>Formel zur Berechnung des Präzisionsverlusts in Prozent</figcaption>
</figure>

<p>
  Sämtliche Verluste haben wir in diesem Histogramm beider Testphasen aufgezeigt. Die Y-Achse stellt die Anzahl Bilder
  dar.
</p>

<figure class="affix">
  <img src="images/thermoscanner/compression-precision-loss.png">
  <figcaption>IRCompressor Histogramm Präzisionsverlust in Prozent</figcaption>
</figure>

<p>
  Man sieht, dass der Verlust in der Regel zwischen 78% und 88% liegt. Wir gewinnen also nicht viel
  dabei. Man muss allerdings folgendes berücksichtigen. Die Kamera liefert im HighGainMode eine Temperaturgenauigkeit
  von 0.01°C (Temperaturspanne von 160 (-25° bis 135°C) geteilt durch 16'384). Bei einem Verlust von 100% können wir
  die Temperatur mit einer Genauigkeit von 0.625°C angeben. Bei einem Verlust von 80% bei <strong>0.125°C</strong>.
  Für unsere Auswertungen später ist diese Genauigkeit ausreichend.
</p>

<p>
  Nachdem wir die Bilder zu 8 Bit Graustufen konvertiert haben, können wir sie mit h.264 encodieren und erhalten somit
  eine sehr kleine Datei die auch direkt im Browser im Cockpit angeschaut werden kann. Den Offset- und
  Skalierungsfaktor den wir zum Clamping benutzt haben speichern wir in den Metadaten der MP4 Datei damit die SEQ
  Datei wieder rekonstruiert werden kann wenn auch mit Verlust.
</p>

<p>
  Ein Problem ergab sich während dem Betrieb. Wenn die aufgenommen SEQ Datei sehr gross war, hat der IRCompressor die
  CPU zu 100% ausgelastet um das Video zu encodieren. Das hatte zur Folge, dass der VisibleLightReader keine
  Aufnahmen mehr machen konnte, weil er nicht mehr genug schnell seine eigene Aufnahme encodieren konnte. Somit
  endeten diese Videos als schwarze Frames. Der Bildanalyseteil hat aber wie bis anhin funktioniert, weil das
  Betriebssystem dem Prozess genug Priorität gegeben hat. Das Encoding passiert jedoch in einem separaten Thread der
  über EMGU.CV zu OpenCV von FFmpeg gesteuert wird. Somit hatten wir keine Kontrolle über die Priorität dieser Threads.
  Als Gegenmassnahme haben wir zum einen mit dem C# Profiler den Code soweit wie möglich optimiert. Dies ergab ein
  merkbarer aber vernichtend kleinen Performance Boost. Wir haben uns dann entschlossen, die Framerate der IR Videos
  auf 1 Frame zu setzen. Danach konnten wir die Videos innerhalb einer Minute konvertieren. Da wir das Video sowieso
  nur zum Stitching verwenden, spielt dies keine Rolle. Wir haben es nicht von Beginn an so gemacht. Denn 30 FPS Videos
  bei Projektpartnern zu zeigen kommt besser an als ein Video mit 1 FPS. Die Hardwaelimitation hat uns also davon
  abgehalten.
</p>

<h5>Uploader</h5>
<ul>
  <li>
    <b>Subscription:</b>
    <ul>
      <li><code>delivery:upload</code></li>
      <li><code>kill</code></li>
    </ul>
  </li>
  <li>
    <b>Publish:</b>
    <ul>
      <li>keine</li>
    </ul>
  </li>
</ul>
<p class="affix">
  Die Uploader Komponente hört auf Nachrichten im Kanal <code>delivery:upload</code>. Die Dateien werden auf einen
  FTP Server hochgeladen und anschliessend in den Papierkorb verschoben. Die Zugangsdaten zum FTP Server sind in der
  Umgebungsvariable <code>THERMOBOX_FTP</code> abgelegt in der Form <code>username:password@server</code>.
</p>
<p>
  Die Dateien werden anschliessend nicht einfach gelöscht, sondern in den Papierkorb verschoben. Damit bietet sich
  die Möglichkeit Möglichkeit nachzuschauen, welche Dateien auf dem Mini PC existiert haben, falls vielleicht etwas mit
  dem Upload schiefgegangen ist. Auf Windows kann dann eingestellt werden, wie gross der Inhalt des Papierkorbs maximal
  sein darf. Ist das Limit überschritten, leert Windows automatisch die ältesten Dateien. So können wir den
  Festplattenspeicherplatz optimal ausnutzen und trotzdem sicherstellen, dass sie nie vollläuft. Ausser ein Programm
  erstellt eine einzige Datei, die den verfügbaren Speicherplatz alleine ausnutzt.
</p>

<p>
  Um Dateien von einer .NET Applikation in den Papierkorb zu verschieben, gibt es bedauerlicherweise kein einfaches
  Interface. Zum einen kann man ein Shell32 Objekt erstellen, welches auf den Papierkorb zeigt und die Datei dort hin
  verschieben. Damit das klappt, muss man aber zuerst in einem STA Thread (single-threaded apartment) sein, welches
  unsere Komponenten nicht sind.
</p>

<p>
  Die auf den FTP Server hochgeladenen Dateien lesen wir im Thermobox Cockpit aus und zeigen sie im Browser an.
  So kann live mitverfolgt werden, was gerade aufgenommen wurde.
</p>

<h4>CLI Utils</h4>
<p>
  Während der Projektarbeit sind wir immer wieder auf sich wiederholende Arbeiten gestossen, die wir automatisiert
  haben. Diese kleinen CLI Tools haben wir auch in die .NET Solution integriert damit diese auch vom gemeinsamen Code
  profitieren können.
</p>

<h5>SeqConverter</h5>
<p>
  Das SeqConverter CLI Programm dient zur Konvertierung von FLIR SEQ Dateien zu MP4 oder umgekehrt. Zudem kann es
  schwarzweisse MP4 Videos in Falschfarben Videos konvertieren.
</p>

<p>
  Im Modus <strong>SEQ zu MP4</strong> macht das CLI Tool genau dasselbe wie der IRCompressor.
</p>

<p>
  Im Modus <strong>MP4 zu SEQ</strong> werden die Metainformationen der MP4 Datei ausgelesen um die SEQ Datei zu
  rekonstruieren. Dies dient dazu, dass wir verifizieren können, ob unser Kompressionsverfahren funktioniert.
  Vorher und nachher sollten dieselben Temperaturwerte berechnet werden. Auch können wir so visualisieren, was der
  Präzisionsverlust effektiv ausmacht, indem wir die zwei Dateien in den FLIR Tools vergleichen können. Das FLIR ATLAS
  SDK bietet keine Möglichkeit an, SEQ oder auch nur einzelne Bilder mit Signaldaten zu erstellen, an. Das heisst
  wir mussten die SEQ Datei bit für bit selber erstellen. Damit wir das konnten mussten wir zuerst die essentiellen Tags
  der Metadaten finden, damit die SEQ Datei valide ist und von den FLIR Tools erkannt und gelesen wird. Mithilfe von
  ExifTool und einem Hex Editor und einer langen Nacht konnten wir das nötige Verständnis aufbauen.
</p>

<p>
  Als Einstiegspunkt diente das veraltete <a href="#ref-flir-interface">Spezifikationsdokument</a> von FLIR für einzelne
  Bilder mit angehängten Signaldaten. Das fehlende Puzzlestück war die Framerate des Videos. Ist diese gesetzt,
  erkennt FLIR Tools die Datei als Video. Fehlt sie, wird nur das erste Frame angezeigt. Die Kenntnis, welches Tag
  Feld und welchen Datentyp die Framerate hat, haben wir zurück ins ExifTool fliessen lassen. Phil Harvey, der
  Maintainer von ExifTool, hat kurz später eine neue Version erstellt die nun die Framerate von SEQ Dateien auslesen
  kann. Sebastian Häni wurde dankend in den Release Notes erwähnt.
</p>

<p>
  Das Verfahren zur Erstellung der SEQ Datei ist nun jedoch sehr einfach geraten nachdem wir das Format verstanden
  haben. Der Header der SEQ Datei enthält wichtige Informationen die von der Kamera respektive durch das FLIR ATLAS
  SDK geschrieben werden. Jedoch sind diese Informationen immer dieselben bei gleichbleibender Kamera. Das einzige
  was sich ändert sind dynamische Anzeigeparameter und Temperaturberechnungsparameter die in den FLIR Tools durch den
  Klick auf "Auto" sowieso neu berechnet werden. Wir können also getrost immer den binär identischen Header
  verwenden. Danach können wir jedes Frame einzeln an den Binärstream anhängen nachdem wir sie durch den
  Skalierungsfaktor geteilt und den Offset dazu addiert haben.
</p>

<p>
  Im Modus <strong>MP4 zu MP4</strong> muss ein weiterer Parameter zur Farbpalette angegeben werden. Der SeqConverter
  akzeptiert ein Graustufenvideo und wendet eine Falschfarbenpalette an. Innerhalb von OpenCV gibt es bereits eine
  ganze Reihe von <a href="#ref-opencv-color-map">Paletten</a> welche aber nicht wirklich gut aussehen. Deswegen
  unterstützen wir davon nur die Palette Hot. Neben dieser haben wir die Palette die schöne Palette von FLIR namens
  Iron implementiert. Weil es keine zu 100% genaue mathematische Definition der Palette gibt haben wir die 256
  Farbwerte in einer Map abgelegt. Somit können wir die nicht so ansehbaren Graustufenvideos in mit Farben versehen
  um diese an unsere Partner weiterleiten zu können damit sie diese in Präsentation aufnehmen können. Zudem können
  wir die Falschfarbenbilder auch weiter an das Thermoboard schicken.
</p>

<figure class="affix">
  <img src="images/thermoscanner/iron-colormap.png">
  <figcaption>FLIR Iron Color Palette</figcaption>
</figure>

<h5>ExtractFrames</h5>
<p>
  Das ExtractFrames diente uns während der Nachbearbeitung und zur Erstellung von kurzen Testvideos anhand unseres
  Testmaterials. Dieses Tool kann schnell n Frames aus einem Video extrahieren und in einem Ordner ablegen.
</p>

<h4>Thermobox Cockpit</h4>
<p>
  Die vom Uploader hochgeladenen Dateien möchten wir mit einfachen Mitteln einsehen können, um nachvollziehen zu
  können, ob die Datenakquise richtig funktioniert. Die Dateien werden asynchron auf den Server geladen was heisst,
  dass man partielle Aufnahmen bereits ansehen kann. Das Cockpit ist eine Webapplikation, die im Browser links eine
  Liste aller Artefakte darstellt. Diese werden zuerst nach Datum und dann nach Zeitpunkt zusammengefasst. Dies kann
  anhand der Dateinamen der Artefakte gemacht werden. Klickt man ein Artefakt an wird es rechts angezeigt.
</p>

<p>
  Es gibt eine simple REST API die die Artefakte auf die auf dem Dateisystem verfügbar sind sammelt und zurückgibt.
  Mit Javascript bauen wir dann die Liste der Artefakte auf. Auf ein Framework haben wir bewusst verzichtet, weil das
  Cockpit eigentlich nie mehr sein soll als ein Debug Tool.
</p>

<p>
  Um den Datenschutzanforderungen der SBB Folge zu leisten, haben wir die Webapplikation hinter einen simplen
  Kennwortschutz gestellt. Auf den Aufnahmen sind zum Teil auch Personen erkennbar die im Zug sitzen. Zudem sind
  die Zugnummern lesbar. Jemand könnte also eine Inventarliste aller Fahrzeuge der SBB aufbauen. Die Passwörter
  werden in einer Konfigurationsdatei abgelegt.
</p>

<figure class="affix">
  <img src="images/thermoscanner/screenshot-thermobox-cockpit.png">
  <figcaption>Screenshot Thermobox Cockpit</figcaption>
</figure>

<!-- References -->

<p id="ref-json-net" class="reference-item">
  <span class="ref">Json.NET: .NET API für JSON</span>
  <a href="https://www.newtonsoft.com/json" target="_blank" rel="noopener">https://www.newtonsoft.com/json</a>
  <span class="retrieved">11. Januar 2018</span>
</p>

<p id="ref-temper-in-csharp" class="reference-item">
  <span class="ref">Code Project: Temper USB Thermometer in C#</span>
  <a href="https://www.codeproject.com/Tips/1007522/Temper-USB-Thermometer-in-Csharp" target="_blank" rel="noopener">https://www.codeproject.com/Tips/1007522/Temper-USB-Thermometer-in-Csharp</a>
  <span class="retrieved">12. Januar 2018</span>
</p>

<p id="ref-openweathermap" class="reference-item">
  <span class="ref">OpenWeatherMap: Wetter API</span>
  <a href="http://openweathermap.org" target="_blank" rel="noopener">http://openweathermap.org</a>
  <span class="retrieved">10. Januar 2018</span>
</p>

<p id="ref-apple-prores" class="reference-item">
  <span class="ref">Apple ProRes</span>
  <a href="https://en.wikipedia.org/wiki/Apple_ProRes" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Apple_ProRes</a>
  <span class="retrieved">13. Januar 2018</span>
</p>

<p id="ref-flir-interface" class="reference-item">
  <span class="ref">FLIR Interface Requirements Specification for FLIR TIFF File Format</span>
  <a href="http://www.flir.com/uploadedFiles/sUAS/Products/Vue-Pro/FLIR-Interface-Requirements-TIFF.pdf" target="_blank"
     rel="noopener">http://www.flir.com/uploadedFiles/sUAS/Products/Vue-Pro/FLIR-Interface-Requirements-TIFF.pdf</a>
  <span class="retrieved">14. Januar 2018</span>
</p>

<p id="ref-opencv-color-map" class="reference-item">
  <span class="ref">OpenCV Color Maps</span>
  <a
    href="https://docs.opencv.org/3.4.0/d3/d50/group__imgproc__colormap.html#gga9a805d8262bcbe273f16be9ea2055a65aef83628a5b46e23bf823000ff0270304"
    target="_blank" rel="noopener">https://docs.opencv.org/3.4.0/d3/d50/group__imgproc__colormap.html</a>
  <span class="retrieved">14. Januar 2018</span>
</p>


  <h3>Anleitungen</h3>

  <p>
    Um den Thermo-Scanner aufzubauen muss man einige Dinge beachten und einrichten. Dazu haben wir eine Checkliste
    verfasst, welche eine Person Schritt für Schritt durch den Aufbau am Gleis führt. Wir haben diese Anleitung bei
    unserem Aufbau selber verwendet um nichts zu vergessen.
  </p>

  <p>
    Sollten irgendwelche Konfigurationen auf dem Thermo-Scanner geändert werden oder Server umgezogen werden, müssen
    einige Dinge beachtet werden. Dazu haben wir ein Manual zur Software des Thermo-Scanners geschrieben. Es enthält
    eine Beschreibung der Konfiguration auf dem Scanner, die Umgebungsvariabeln die gesetzt werden können und eine
    Anleitung wie man die Webservices installiert. Somit sollten bei einer Projektweiterführung keine Unklarheiten
    bestehen.
  </p>

  <h3>Aufnahmen</h3>

  <p>
    Der fertige Thermo-Scanner ist nun strategisch geschickt zu platzieren, um einerseits möglichst viele und anderseits
    möglichst gute Aufnahmen zu erhalten. Wie schon erwähnt, eignen sich hierfür Waschanlagen, da Züge hier oft
    (wöchentlich) und langsam durchfahren. Wichtige Bedingung ist ein Stromanschluss. Die möglichen Standorte
    waren also aufgrund der kleinen Anzahl schnell gefunden.
  </p>
  <p>
    Um beim Aufbau keine Überraschungen zu erleben, haben wir die nötigen Schritte in einer Checkliste festgehalten. Die
    Logistik ist aufwendig, deshalb sollten Fehler vermieden werden.
  </p>

  <h4>Standort Zürich Herdern</h4>
  <p>
    Der erste Standort in Zürich Herdern wurde von der SBB vorgeschlagen, da dort eine Aussensteckdose vorhanden ist.
    Zürich Herdern ist eine Serviceanlange, die auch eine Durchlaufreinigung betreibt. Die Voraussetzungen wären also
    erfüllt. Leider mussten wir bei der Besichtigung feststellen, dass der Mindestabstand von 5m zum Gleis nicht
    einzuhalten ist. Somit passte nicht der ganze Zug aufs Bild. Möglichkeiten für eine alternative Platzierung waren
    auch nicht gegeben, da Masten oder andere Gleise dies verunmöglichten. Und wir haben bereits erste Erfahrung mit dem
    Aufbau und der Inbetriebnahme des Thermo-Scanners gemacht. Auch die Checkliste konnten wir testen und für gut
    befinden. Bei den Testaufnahmen mussten wir auch feststellen, dass das Starten der Aufnahme noch nicht korrekt
    funktioniert. Für den nächsten Standort mussten wir also noch unsere Parameter anpassen (min. Aufnahmedauer, max.
    Aufnahmedauer, usw.).
  </p>

  <figure>
    <img src="images/thermoscanner/zurich_herdern.jpg"/>
    <figcaption>Links: Transport, Rechts: Thermo-Scanner zu nahe am Gleis</figcaption>
  </figure>

  <h4>Standort Zürich Altstetten</h4>

  <p>
    Glücklicherweise konnten wir zeitnah unseren definitiven Standort finden, unweit des ersten Standortes. Vor der
    Durchlaufreinigung in Zürich Altstetten konnten wir den Thermo-Scanner im idealen Abstand zum Gleis platzieren. Auch
    der Stromanschluss in einer nahe gelegenen Garage war vorhanden. Dank der Erfahrung aus dem ersten Aufbau, war das
    Aufstellen in wenigen Minuten erledigt.
  </p>

  <p>
    Aufgrund von Bauarbeiten an der Waschanlage musste der Thermo-Scanner bereits vor Ablauf der zwei Wochen
    Aufnahmezeit wieder abgebaut werden. Glücklicherweise durften wir ihn nach den Bauarbeiten nochmals über Weihnachten
    und Neujahr aufstellen.
  </p>

  <p>Am Standort Altstetten gab es also zwei Testphasen:</p>

  <figure class="full affix">
    <table>
      <thead>
      <tr>
        <th style="white-space: nowrap;">Phase Nr.</th>
        <th>Zeitraum</th>
        <th>Beschreibung</th>
        <th style="white-space: nowrap;">Anzahl Aufnahmen</th>
      </tr>
      </thead>
      <tbody>
      <tr>
        <td>1</td>
        <td style="white-space: nowrap;">20.11.2017 - 29.11.2017</td>
        <td>
          Schnee und Regen überstanden. Nach einigen Anpassungen an Software erfolgreich Bilder für Analyse mit SBB
          Experten gesammelt. Farbvideos in der Nacht nicht brauchbar.
        </td>
        <td>273</td>
      </tr>
      <tr>
        <td>2</td>
        <td style="white-space: nowrap;">19.12.2017 - 07.01.2018</td>
        <td>
          Sturm Burglind überstanden. Weitere gute Aufnahmen gesammelt. Ab 04.01.2018 15:08 Uhr offline, Grund LTE
          Modul abgestürzt.
        </td>
        <td>306</td>
      </tr>
      </tbody>
    </table>
    <figcaption>Testphasen Thermo-Scanner</figcaption>
  </figure>

  <p>
    In den ersten Tagen des Betriebs mussten wir noch zahlreiche Justierungen an der Aufnahmesoftware vornehmen. Das
    Starten und Stoppen der Aufnahme, sowie das Erkennen von einfahrenden Zügen hat in der Praxis noch ein wenig
    anders funktioniert als in der Theorie. Die grundsätzlichen Überlegungen und Methoden haben sich aber als brauchbar
    erwiesen. Die Monitoring Tools, der Remotezugang und die Website mit den hochgeladenen Videos haben sich in dieser
    Phase als sehr nützliche Werkzeuge behauptet.
  </p>
  <p>
    Die Outdoor-Tauglichkeit war ebenfalls überzeugend, Schnee und Regen konnten dem Thermo-Scanner nichts anhaben. Der
    integrierte Heizlüfter hat dafür gesorgt, dass kein Kondenswasser entstand.
  </p>

  <figure>
    <img src="images/thermoscanner/zurich_alstetten.jpg"/>
    <figcaption>Aufgebauter Thermo-Scanner in Zürich Altstetten</figcaption>
  </figure>

  <p>
    Nach der ersten Testphase haben wir unsere Resultate den Verantwortlichen der SBB präsentiert. Dabei waren der
    Projektleiter seitens SBB, der Chef Technik (zuständig bei Isolationsschäden) und ein Klimatechniker der Klimakammer
    Olten. Wir haben die Panoramas der gefilmten Züge vorgestellt und auch einige Videos in den FLIR Tools genauer
    analysiert.
  </p>
  <p>
    Unterhalb einiger Fenster sind uns seltsame helle Stellen aufgefallen. Wir vermuteten hier einen Schaden. Für uns
    sah das verdächtig nach einem Wassereintritt aus.
  </p>

  <figure>
    <img src="images/thermoscanner/car_highlight.png"/>
    <figcaption>Waggon mit auffälligen Stellen unterhalb Fenster im Oberdeck</figcaption>
  </figure>
  <figure>
    <img src="images/thermoscanner/car_normal.png"/>
    <figcaption>Waggon mit normalen Fenstern</figcaption>
  </figure>
  <figure style="display: inline-block;">
    <img src="images/thermoscanner/car_highlight_detail.png"/>
    <figcaption>Detailansicht auffällige Stelle</figcaption>
  </figure>
  <figure style="display: inline-block;">
    <img src="images/thermoscanner/car_normal_detail.png"/>
    <figcaption>Detailansicht normale Stelle</figcaption>
  </figure>

  <p>
    Im Gespräch mit dem Chef Technik stellte sich aber heraus, dass dies normal ist. Der dritte Waggon ist ein
    Niederflurwagen, der anders aufgebaut ist als der restliche Zug. Die auffälligen Stellen sind lediglich die
    Belüftungskanäle. Der Chef Technik versprach aber uns beim nächsten Isolationsschaden sofort zu informieren, damit
    wir den Zug filmen können.
  </p>

  <p>
    Die SBB ist sehr interessiert an den Aufnahmen und sieht grosses Potential für die Verwendung in verschiedenen
    Bereichen. Die Resultate wurden auch SBB intern an Teammeetings und an den Auftraggeber weitergegeben.
    Wärmebildaufnahmen existierten bis anhin nur von einzelnen Wagons unter kontrollierten Bedingungen und nicht im
    täglichen Betrieb. Ausserdem konnte die Qualität der Aufnahmen überzeugen, denn es waren viele Details sichtbar wie
    beispielsweise das feine Wagon-Skelet. Die Experten konnten auf den Aufnahmen aber keine Schäden feststellen, somit
    haben wir leider keinen schadhaften Zug erwischt.
  </p>

</div>

<!-- References -->

<div class="chapter">
  <h2 class="break"><a name="post-processing">Processing: Bildverarbeitung</a></h2>

  <h3>Übersicht</h3>

<p>
  In diesem Kapitel wird die von uns entwickelte Verarbeitungspipeline erläutert. Der Input der Pipeline ist ein Video
  eines vorbeifahrenden Zuges. Das Endartefakt der Pipeline ist pro Zugwagen ein Bild, welches den Wagon in einer nahezu
  orthografischen Projektion oder anders gesagt einer Planansicht darstellt. Ausserdem sollen unnötige Bildausschnitte
  wie Himmel oder Boden entfernt werden.
</p>

<figure class="full">
  <img src="images/post-processing/diagram.png">
  <figcaption>Pipeline Ablaufdiagramm</figcaption>
</figure>

<p>
  Artefakte in Form von <code>mp4</code> oder <code>jpg</code> Dateien sind rechteckig dargestellt. Die
  Processing-Schritte haben runde Ecken. Wie man erkennt, dienen ein Video eines Schachbretts und das eines Zuges als
  Eingabeartefakte. Davon wird das Schachbrettvideo genutzt, um das Kameramodell abzubilden. Das Resultat wird wiederum
  zur Entzerrung des Zug-Videos genutzt. Sind alle Schritte durchlaufen, ergibt sich ein Panoramabild des Zuges.
  Ausserdem gibt es pro Wagon ein Bild, welches auf den Wagon genau zugeschnitten ist.
</p>

<p>
  Die Verarbeitung der Farb- und Infrarotvideos erfolgt dabei praktisch identisch. Grundsätzlich sind die Infrarotbilder
  schwarz/weiss, haben eine kleinere Auflösung und sind teilweise sehr kontrastarm. Nichts desto trotz können die Infrarot-Panoramas
  mit denselben Methoden erstellt werden. Lediglich die Parameter (Schwellwerte, Fenstergrössen, etc.) sind anzupassen.
  Nennenswerte Abweichungen sind direkt in den jeweiligen Schritten beschrieben.
</p>

<h4>Implementation</h4>

<p>
  Die Pipeline haben wir mit Java und <a href="#ref-opencv">OpenCV</a> entwickelt. Zur Verifikation und zum Prototyping
  der von uns gewählten Verfahren, haben wir zudem Matlab angewendet.
</p>

<p>
  Es war uns von Anfang an klar, dass unsere Pipeline mindestens vier Schritte hat. Das hat sich dann ausgeweitet auf
  zehn Schritte. Die Software ist so designt, dass Schritte der Pipeline fast unabhängig voneinander entwickelt
  und getestet werden können. Der Grundbaustein der Pipeline ist der <code>PipelineRunner</code>. In ihm wird
  konfiguriert, welche Schritte auszuführen sind. Jeder Schritt hat mindestens ein Eingabeartefakt und mindestens ein Ausgabeartefakt.
  Die Daten lassen sich so von einem Schritt in den nächsten pipen. Dabei werden die Artefakte serialisiert auf der Festplatte abgelegt.
  In aller Regel sind das <code>jpg</code> Dateien. Folglich muss nicht immer die ganze Pipeline durchlaufen werden, um nur eine kleine
  Änderung an einem Schritt zu testen. Start- und Endschritt lassen sich im <code>PipelineRunner</code> einstellen.
  Das ermöglicht eine sehr flexible Arbeitsweise. In Zukunft ist klar, um einen produktiven Einsatz der Software zu unterstützen,
  dürfen die Artefakte nicht als <code>jpg</code> zwischen gespeichert werden. Dies sollte jedoch ohne grossen Aufwand lösbar sein.
</p>

<p>
  Die folgenden Unterkapitel erläutern die einzelnen Schritte der Pipeline genauer.
</p>

<p id="ref-opencv" class="reference-item">
  <span class="ref">OpenCV</span>
  <a href="http://opencv.org" target="_blank" rel="noopener">http://opencv.org</a>
  <span class="retrieved">15. Juni 2017</span>
</p>

  <h3>Frames extrahieren</h3>

<ul>
  <li><b>In:</b> Videodatei, Anzahl gewünschter Frames, Richtung, Längefaktor</li>
  <li><b>Out:</b> jpg Dateien</li>
</ul>

<p>
  Dieser Schritt erhält die Videodatei und extrahiert einzelne Frames daraus und speichert diese als <code>jpg</code>
  Datei ab. Der Richtungsparameter bestimmt, ob die Frames beginnend am Start oder am Ende des Videos extrahiert
  werden sollen. Dies ist hilfreich, um ein Video eines Zuges so zu transformieren, dass er aus einer anderen
  Richtung einfährt. Der Längefaktor bestimmt, wieviel der Länge des Videos berücksichtigt wird. Dies ist
  hilfreich, wenn nicht alle Frames benötigt werden. Zum Beispiel um die Fahrtrichtung des Zuges zu bestimmen.
</p>

<p>
  Momentan genügt es uns in regelmässigen Abständen Frames zu extrahieren. Sollten wir in Zukunft bessere Qualität in
  den Bildern brauchen, können wir den Algorithmus dahin optimieren, dass er nur <a href="#ref-video-compression">I-
  oder B-Frames</a> des Videos extrahiert.
</p>

<figure class="affix">
<pre><code>int frameCount = (int) (capture.get(Videoio.CAP_PROP_FRAME_COUNT) * lengthFactor);
int framesBetween = frameCount / framesToExtract;
int frameCounter = 0;

Predicate&lt;Integer&gt; termination = isForward ?
  i -&gt; i &lt; frameCount :
  i -&gt; i &gt; 0;
Function&lt;Integer, Integer&gt; increment = isForward ?
  i -&gt; i + 1 :
  i -&gt; i - 1;

int i = isForward ? 0 : frameCount;

while (termination.test(i) && frameCounter &lt; framesToExtract) {
  i = increment.apply(i);

  if (i == 0 || i % framesBetween != 0)
    continue;

  Mat frame = capture.read(frame);

  if (!isForward)
    flipHorizontal(frame);

  save(frame);
}</code></pre>
  <figcaption>Algorithmus: Frames extrahieren</figcaption>
</figure>


<p id="ref-video-compression" class="reference-item">
  <span class="ref">Wikipedia: Video Compression Picture Types</span>
  <a href="https://en.wikipedia.org/wiki/Video_compression_picture_types" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Video_compression_picture_types</a>
  <span class="retrieved">14. Juni 2017</span>
</p>

  <h3>Kamerakalibration</h3>

<ul>
  <li><b>In:</b> Schachbrett Frames</li>
  <li><b>Out:</b> JSON Datei mit Kalibrationsdaten</li>
</ul>

<p>
  Die Kalibration der Kamera lösen wir mithilfe eines Schachbretts. Die gut zu detektierenden Eckpunkte zwischen den Kacheln
  sind ein ideales Muster. In der Tat gibt es von OpenCV auch die Methode mit aussagekräftigem Namen
  <code>findChessboardCorners(...)</code>, welche die Eckpunkte findet. Vorgeben muss man die Anzahl Kacheln in Höhe
  sowie Breite.
</p>

<figure>
  <img src="images/post-processing/calibration.jpg">
  <figcaption>Gefundenes Schachbrett Muster</figcaption>
</figure>

<figure>
  <img src="images/post-processing/calibration_ir.jpg">
  <figcaption>Gefundenes Schachbrett Muster Infrarotbild</figcaption>
</figure>

<p>
  Wir brauchen nun viele Testbilder aus verschiedenen Blickwinkeln auf das Schachbrett. Je nachdem ist die Verzerrung
  der Kamera unten, oben, links oder rechts anders. Wir fügen sämtliche gefundenen Punkte in eine Liste ein. Nebenbei
  führen wir eine ideale Liste, in der die Punkte korrekt ausgerichtet sind. Die Methode
  <code>calibrateCamera(...)</code>
  findet dann die Kameramatrix mithilfe der zwei Listen. Weil wir wissen wie gross die Kacheln in Millimeter sind,
  können wir auch die Field of View ausrechnen. Mit unseren FLIR Aufnahmen berechneten wir ein vertikales Sichtfeld von
  33.2° mit einem Fehlerwert (RMS) von 0.64. Der echte Wert wäre eigentlich bei 37° gemäss der FLIR A65 Spezifikation.
</p>

<p>
  Um die Infrarotkamera zu kalibrieren haben wir das ausgedruckte Schachbrettmuster gegen das Sonnenlicht gefilmt.
  So wurden die schwarzen Kacheln aufgewärmt und sind auf dem Bild als weisse Kacheln sichtbar. Das Schachbrettmuster
  ist also invertiert. Der erste Versuch mit einem frisch gedruckten Schachbrett mit noch warmer Tinte, war nicht ausreichend
  genau um die Eckpunkte zu identifizieren.
</p>


<figure style="width: 48%;">
  <img src="images/post-processing/calibration_window.jpg">
  <figcaption>Schachbrett an warmem Fenster</figcaption>
</figure>

<figure style="width: 48%;">
  <img src="images/post-processing/chessboard_ir.jpg">
  <figcaption>Schachbrett an warmem Fenster (Infrarot)</figcaption>
</figure>

<p>
  Die gefundenen Daten serialisieren wir in eine JSON Datei.
</p>

<figure>
<pre><code>{
  "cameraMatrix": { "rows": 3, "cols": 3, "type": 6, "data": "..."},
  "distCoeffs": { "rows": 5, "cols": 1, "type": 6, "data": "..."},
  "imageSize": { "width": 1920.0, "height": 1080.0},
  "rvecs": [...]
  "tvecs": [...]
}</code></pre>
  <figcaption>Serialisierte Kalibrationsdaten</figcaption>
</figure>

<p class="affix">
  Der obige Ausschnitt aus der Datei zeigt den Aufbau. Zur Bestimmung des Kameramodells werden die Rotations- und
  Translations-Vektoren jedoch nicht gebraucht. Weil sie jedoch später einmal hilfreich sein könnten, serialisieren wir
  sie trotzdem.
</p>

<h4>Entzerren</h4>

<p>
  Die zuvor serialisierten Daten können wir nun einlesen und die optimale Kameramatrix berechnen. OpenCV bietet dazu
  die Methode <code>getOptimalNewCameraMatrix(...)</code> an, welche die Kameramatrix berechnet. Die einzelnen Frames
  lassen sich dann mit der Methode <code>undistort(...)</code> entzerren. Durch das Entzerren werden jedoch schwarze
  Ränder hinzugefügt. Wir schneiden diese Ränder anschliessend noch ab.
</p>

<figure>
  <img src="images/post-processing/flir_calibration.jpg">
  <figcaption>Links: Verzerrt, Rechts: Entzerrt</figcaption>
</figure>

<figure>
  <img src="images/post-processing/flir_calibration_diff.jpg">
  <figcaption>Absolute Differenz zum entzerrten Bild</figcaption>
</figure>

<p>
  Bei der FLIR Kamera ist die Verzerrung auf die aufgenommenen Distanz von Auge kaum zu erkennen. Nimmt man die absolute
  Differenz der beiden Bilder, kann man den Unterschied aber sichtbar machen.
</p>

  <h3>Motion Cropping</h3>

<ul>
  <li><b>In:</b> Zug Frames</li>
  <li><b>Out:</b> Bounding Boxes pro Frames in der sich der Zug befindet</li>
</ul>

<p>
  Unter Motion Cropping verstehen wir ein Frame auf den sich bewegenden Teil zuzuschneiden. Im Fall unseres Zuges
  heisst das, dass der Zug schön ausgeschnitten wird, weil er sich durch das Bild bewegt. Motion Cropping brauchen wir
  in zwei Schritten in der Pipeline. Zum einen erkennen wir damit die Bewegungsrichtung des Zuges um die Frames in
  normalisierter Form zu extrahieren. Zum anderen zum wortwörtlichen Motion Cropping.
</p>

<ul>
  <li>
    <b>Schritt 1: Hintergrund vorbereiten</b><br>
    Wir konvertieren ein Bild auf welchem nur der Hintergrund, also kein Zug ist, in ein Graustufenbild. Dieses
    verunschärfen wir mit einem Boxfilter und einer Kernelgrösse von 6. Ein Gaussfilter würde in diesem Fall nicht
    viel bringen ausser Performanceeinbussen.
  </li>
</ul>

<figure class="affix">
  <img src="images/post-processing/motion/1-bg.jpg">
  <figcaption>Vorbereitetes Hintergrundbild</figcaption>
</figure>

<ul>
  <li>
    <b>Schritt 2: Absolute Differenz</b><br>
    In dem wir das vorbereitete Hintergrundbild von einem Graustufenbild mit einem Zug abziehen und die absoluten
    Werte davon nehmen, erhalten wir ein neues Bild, auf welchem grosse Unterschiede heller sind. Also je weisser, desto
    grösser ist der Unterschied.
  </li>
</ul>

<figure class="affix">
  <img src="images/post-processing/motion/2-gray.jpg">
  <figcaption>Bewegender Zug</figcaption>
</figure>

<figure class="affix">
  <img src="images/post-processing/motion/3-diff.jpg">
  <figcaption>Absolute Differenz</figcaption>
</figure>

<ul>
  <li>
    <b>Schritt 3: Binarisieren</b><br>
    Das Differenzbild binarisieren wir mit einem Threshold von 40. Auf diesen Wert kamen wir stichprobenartig.
  </li>
</ul>

<figure class="affix">
  <img src="images/post-processing/motion/4-thresh.jpg">
  <figcaption>Binarisierter bewegender Zug</figcaption>
</figure>

<ul>
  <li>
    <b>Schritt 4: Erodieren und Dilatieren</b><br>
    Über das binarisierte Bild lassen wir nun eine Erosion und eine anschliessende Dilatation laufen. Dies entfernt
    kleine weisse Flächen und schliesst kleine schwarze Löcher. Was wir als Ergebnis sehen, ist die Fläche des Zuges.
  </li>
</ul>

<figure class="affix">
  <img src="images/post-processing/motion/6-dilate.jpg">
  <figcaption>Fläche der Bewegung</figcaption>
</figure>

<ul>
  <li>
    <b>Schritt 5: Kontur finden</b><br>
    Mithilfe der OpenCV Methode <code>findContours(...)</code> können wir nun von dieser Fläche die Kontur finden.
  </li>
</ul>

<figure class="affix">
  <img src="images/post-processing/motion/7-contours.jpg">
  <figcaption>Kontur des bewegenden Zuges</figcaption>
</figure>

<p>
  Nun haben wir die Kontur des Zuges. Daraus bauen wir ein Viereck zusammen. Wenn die Breite der Bounding Box im
  Vergleich mit der Breite des Frames nicht einen gewissen Anteil einnimmt, verwerfen wir die Kontur. Bei der Findung
  der Fahrtrichtung muss die Kontur 10% der Breite einnehmen. Wenn wir den Zug zuschneiden muss es 90% sein.
</p>

  <h3>Fahrtrichtung erkennen</h3>

<ul class="affix">
  <li><b>In:</b> Motion Bounding Boxes</li>
  <li><b>Out:</b> Vorwärts oder Rückwärts</li>
</ul>

<p class="affix">
  Während der Zug gerade ins Bild hereinfährt, nimmt die Bounding Box nicht die ganze Breite des zur Verfügung stehenden
  Platzes ein. Durch den Verlauf der Konturkoordinaten über mehrere Frames können wir feststellen, ob sich die
  Bounding Box von links nach rechts oder von rechts nach links bewegt hat.
</p>

<figure class="affix">
  <img src="images/post-processing/motion/contour-train-entering.jpg">
  <figcaption>Einfahrender Zug</figcaption>
</figure>

  <h3>Achsen ausrichten</h3>

<ul>
  <li><b>In:</b> Entzerrte Zug Frames</li>
  <li><b>Out:</b> An Gleis ausgerichtete Frames</li>
</ul>

<p>
  Das Ziel dieses Schritts ist es, die Fehleranfälligkeit der nachfolgenden Schritte zu vermindern. Würden wir das
  nicht machen, hätten wir beim Rektifizieren eine viel höhere Fehlerquote. Ausserdem könnten wir nicht das
  vereinfachte Verfahren zum Stitching verwenden.
</p>

<ul>
  <li>
    <b>Schritt 1: Binarisieren</b><br>
    Wir konvertieren das Frame zu einem Graustufenbild und binarisieren es mit einem Threshold von 40. Dies lässt
    dunkle Pixel als schwarz erscheinen und hellere Pixel als weiss.
  </li>
</ul>

<figure class="affix">
  <img src="images/post-processing/straighten/masked.jpg">
  <figcaption>Dunkle Pixel Threshold</figcaption>
</figure>

<ul>
  <li>
    <b>Schritt 2: Linien finden</b><br>
    In diesem Bild probieren wir nun die starken Linien zu finden. Dies tun wir im Hough Raum. OpenCV bietet dazu die
    Methode <code>HoughLinesP(...)</code>. Vorher verunschärfen wir das Bild jedoch noch und lassen den Canny
    Algorithmus die Kanten finden. Auch für den Canny Algorithmus gibt es in OpenCV eine Methode <code>Canny(...)</code>.
    Der Hough-Methode geben wir noch an, wie lange die Linien mindestens sein sollen und wie gross die Lücken
    dazwischen sein dürfen.
  </li>
</ul>

<figure class="affix">
  <img src="images/post-processing/straighten/lines.jpg">
  <figcaption>Gleislinien</figcaption>
</figure>

<p>
  Von den Linien berechnen wir dann die Steigung und somit den Winkel. Die gefundenen Winkel mitteln wir und rotieren
  dann das Bild um diesen Winkel zurück. Somit ist das Bild an den Gleisen ausgerichtet.
</p>

  <h3>Rektifizieren</h3>

<ul>
  <li><b>In:</b> Ausgerichtete Zug Frames</li>
  <li><b>Out:</b> Rektifizierter Zugausschnitt</li>
</ul>


<p>
  Die Rektifizierung oder räumliche Entzerrung des Zuges hat sich als äusserst schwierig herausgestellt. Nach vielen
  Versuchen sind wir auf ein Verfahren gestossen, welches im Grunde zwar simpel ist, aber trotzdem brauchbare
  Resultate erzeugt. Das Verfahren ist nicht perfekt und rektifiziert nicht gleichmässig.
</p>

<p>
  Die Idee ist die obere und untere Kante des Zuges zu finden. Dies in der Annahme, dass diese gerade verlaufen
  müssten. Mit den zwei Geraden spannen wir ein Viereck auf. Dieses verzerrte Viereck geben mir mit einem Rechteck in
  die OpenCV Methode <code>getPerspectiveTransform(...)</code> und erhalten so die Matrix, mit welcher wir die Bilder
  rektifizieren können.
</p>

<p>
  Folgend arbeiten wir mit dem V-Kanal (Value) von HSV bei den Farbbildern. Die Infrarotbilder sind bereits
  Graustufenbilder. Um die obere und untere Kante zu finden, unterteilen wir das Bild erst einmal, um die
  Wahrscheinlichkeit des Erfolgs zu erhöhen. Die obere Zugkante ist im oberen Drittel und die untere im unteren Drittel.
  In den beiden Ausschnitten suchen wir nun die maximale Frequenz in Y Richtung. Das heisst, wir wollen pro Spalte im
  Bild ein weisser Pixel. Diese finden wir mit einer Art vereinfachtem Sobel-Filter, der nur in eine Richtung agiert.
  Wir nehmen den Ausschnitt und verschieben ihn um ein Pixel nach unten. Dann bilden wir die absolute Differenz des
  Verschobenen und des Originals. Anschliessend suchen wir in jeder Spalte das Maximum, setzen es auf das Maximum (255)
  und alle übrigen auf das Minimum (0). Für die Infrarotbilder verwenden wir anstatt des vereinfachten Sobel-Filters den
  Canny-Edge Filter, dieser liefert die Kanten zuverlässiger.
</p>

<figure class="affix">
  <img src="images/post-processing/rectify/upper.jpg">
  <figcaption>Oberer Teil des Zuges</figcaption>
</figure>
<figure class="affix">
  <img src="images/post-processing/rectify/lines.jpg">
  <figcaption>Maximale Y Frequenz im oberen Teil</figcaption>
</figure>
<figure class="affix">
  <img src="images/post-processing/rectify/lower.jpg">
  <figcaption>Unterer Teil des Zuges</figcaption>
</figure>
<figure class="affix">
  <img src="images/post-processing/rectify/lower-line.jpg">
  <figcaption>Maximale Y Frequenz im unteren Teil</figcaption>
</figure>

<p>
  In den so erzeugten Bildern verwenden wir wieder die Hough Transformation um die Linie zu finden. Diese Linie
  erweitern wir dann, so dass sie die ganze Bildbreite überspannt. Dies weil nicht in jedem Bild die gesamte Zuglänge
  erkannt wird. Das ist zum Beispiel bei Wagonübergängen oder Stromabnehmern der Fall. Nun haben wir mit den Start- und
  Endpunkten der oberen und unteren Linie ein Viereck, das wir aufziehen können. Wie bereits beschrieben, lässt sich
  dadurch eine Transformationsmatrix errechnen, die das Bild rektifiziert.
</p>

<figure class="affix">
  <img src="images/post-processing/rectify/rectified.jpg">
  <figcaption>Rektifizierter Zug</figcaption>
</figure>

<p>
  Die Erfahrung hat gezeigt das ca. 80% der Bilder aus unseren Testdaten ein genügendes Resultat erzeugen. Jedoch
  brauchen wir jedes Bild. Wir nehmen an, dass wir dieselbe Transformationsmatrix für jedes Bild brauchen können.
  Deshalb bilden wir von allen verzerrten Vierecken den Median. Daraus berechnen wir die Matrix, um die Bilder
  schlussendlich zu rektifizieren.
</p>

<p>
  Die Matrix können wir nicht im voraus berechnen, weil wir nicht wissen ob sich der Zug über das parallel
  verlaufende Gleis oder über die Weiche fährt. Dies kann zum Teil zu ganz anderen Winkeln führen. Ausserdem kann so
  der Standort gewechselt werden ohne, dass man eine Kalibrierung vornehmen muss. Unten aufgezeigt ist eine
  Visualisierung, auf der ein Zug über die Weiche fährt. Gestrichelt ist das Sichtfeld der Kamera angedeutet. Ob der
  Zug vom oberen oder unterem Gleis kommt wissen wir nicht und kann immer ändern.
</p>

<figure class="affix">
  <img src="images/post-processing/rectify/gleis-winkel.png">
  <figcaption>Zug fährt über Weiche</figcaption>
</figure>

  <h3>Stitching</h3>

<ul>
  <li><b>In:</b> Rektifizierte Zug Frames</li>
  <li><b>Out:</b> Zug Panorama</li>
</ul>

<p class="affix">
  Durch die vorgängig angewandten Operationen auf den Frames können wir nun ein vereinfachtes Stitching
  Verfahren anwenden. Die Versuche mit handelsüblichem Panoramastitching brauchbare Resultate zu erzielen,
  schlugen fehl. Die <code>Stitcher</code> Funktionalität von OpenCV bietet zwei Modi an: Landschaftspanorama und Ebenenpanorama.
  Der Landschaftspanorama-Modus geht davon aus, dass die Kamera bei der Aufnahme geschwenkt wurde.
  Der Ebenen-Modus geht davon aus, man hat eine Karte oder Dokumente fotografiert, welche in einer Ebene
  liegen. Das wäre eigentlich unser Anwendungsfall.
</p>

<p>
  Das Hauptproblem besteht darin, dass es jeweils grosse Überlappungen zwischen den einzelnen Frames gibt und
  der Hintergrund immer gleich bleibt. Hinzu kommt, dass sich Wagons ziemlich ähnlich sind. Der erste Versuch
  war einfach mal drauf los zu stitchen. Wir haben sämtliche Frames der <code>Stitcher</code> Klasse übergeben, den
  Modus auf Ebene gestellt und beobachtet was passiert. Es gab einen Fehler, den wir auf keine Weise beheben konnten. Der
  Modus Landschaft hat ein sehr amüsantes Resultat geliefert, das allerdings weniger hilfreich war. Die Wagons waren
  wild übereinandergestapelt und verzerrt.
</p>

<p>
  Der zweite Ansatz war die Bilder gestaffelt zu stitchen. Das heisst wir würden inkrementell ein Bild mit dem
  bisherigen Resultat zusammenstitchen. Das war auch nicht erfolgreich. Man kann auch nicht spezifizieren, wo das
  nächste Bild hinsoll. Also hat OpenCV versucht das Bild irgendwo zu platzieren. Meistens gab es auch hier einen
  Fehler, weil zu wenig Bilder vorhanden waren.
</p>

<p>
  Danach haben wir versucht das Stitching selber zu implementieren mit der OpenCV Methode
  <code>findHomography(...)</code>. Dazu mussten wir Featurepunkte in den beiden Bildern finden, die wir
  zusammenstitchen wollten. Das hat aber auch nicht geklappt. Wahrscheinlich war die Qualität der Featurepunkte
  ungenügend.
</p>

<h4>Pattern Matching</h4>

<p>
  Das Verfahren, das uns dann die Augen geöffnet hat und einwandfrei funktioniert, ist verblüffend simpel. Wir finden in
  zwei aufeinanderfolgenden Frames ein Teilausschnitt von Frame A in Frame B. Dies tun wir mit der <a
  href="#ref-sqdiff-normed">Normalisierten Cross Correlation Funktion</a>. Wenn wir den Teilausschnitt gefunden haben, haben wir eine
  X-Koordinate und dort können wir die Bilder zusammenfügen.
</p>

<figure class="affix">
  <img src="images/post-processing/stitch/template.jpg">
  <figcaption>Template</figcaption>
</figure>
<figure class="affix">
  <img src="images/post-processing/stitch/match.jpg">
  <figcaption>Gefundenes Template</figcaption>
</figure>

<p>
  Fügen wir alle Zugframes zusammen erhalten wir so das gewünschte Resultat.
</p>

<figure class="affix">
  <img src="images/post-processing/stitch/pano.jpg">
  <figcaption>Zug Panorama</figcaption>
</figure>

<figure class="affix">
  <img src="images/post-processing/stitch/pano_ir.jpg">
  <figcaption>Zug Panorama Infrarot</figcaption>
</figure>

<figure class="affix">
  <img src="images/post-processing/stitch/pano_ir_false_color.jpg">
  <figcaption>Zug Panorama Infrarot False Color</figcaption>
</figure>

<p id="ref-sqdiff-normed" class="reference-item">
  <span class="ref">OpenCV: Template Match Modes</span>
  <a href="http://docs.opencv.org/3.2.0/df/dfb/group__imgproc__object.html#ga3a7850640f1fe1f58fe91a2d7583695d"
     target="_blank" rel="noopener">http://docs.opencv.org/3.2.0/df/dfb/group__imgproc__object.html</a>
  <span class="retrieved">15. Juni 2017</span>
</p>

  <h3>Wagons ausschneiden</h3>

<ul>
  <li><b>In:</b> Zug Panorama</li>
  <li><b>Out:</b> Pro Wagon ein Bild</li>
</ul>

<p>
  Nun möchten wir aus dem Panorama die einzelnen Wagons ausschneiden, so dass wir pro Wagon ein Bild haben.
  Unsere Analyse hat ergeben, dass die Übergänge der Wagons jeweils sehr viele dunkle Pixel aufweisen. Mit dieser
  Erkenntnis haben wir folgendes Verfahren entwickelt.
</p>

<p>
  Bei den Farbbildern schränken wir das Panorama Bild auf Pixel ein, die zwischen <code>HSV(0,0,0)</code> und <code>
  HSV(255,255,38)</code> sind. Danach führen wir eine Erosion durch um kleine Objekte zu entfernen. Anschliessend bauen
  wir ein Histogramm über die Spalten auf, welches die Anzahl der überlebenden Pixel aufsummiert. Das Resultat wird
  durch das folgende Diagramm dargestellt.
</p>

<figure class="affix">
  <img src="images/post-processing/split/split-histogram.png">
  <figcaption>Histogramm über dunkle Pixel in Spalten</figcaption>
</figure>

<p>
  Man kann sich vorstellen, dass dies das Panorama Bild überdeckt. Dort wo wir Peaks finden, sind Zugübergänge. Der
  folgende Algorithmus zeigt, wie die Peaks gefunden werden. Ein wichtiger Faktor spielt dabei die Mindestlänge eines
  Wagons in Pixel. Dadurch werden auch gleich die Peaks am Anfang und Ende eliminiert.
</p>

<figure>
  <pre><code>
    int lastPeak = -1;
    for (int i = 0; i &lt; hist.length; i++) {
      if (hist[i] &lt; max * PEAK_THRESHOLD) {
        hist[i] = 0;
      } else {
        hist[i] = 1;

        // Removes the plateau by flattening every element that
        // is 1 before the current one in a fixed distance.
        if (lastPeak &gt;= 0 && lastPeak + MIN_CAR_LENGTH_IN_PX &gt; i) {
          hist[lastPeak] = 0;
        }
        lastPeak = i;
      }
    }
  </code></pre>
  <figcaption>Algorithmus: Find Peaks</figcaption>
</figure>

<p>
  Das Resultat sieht so aus:
</p>

<figure class="affix">
  <img src="images/post-processing/split/wagon_1.jpg">
  <figcaption>Wagon 1</figcaption>
</figure>
<figure class="affix">
  <img src="images/post-processing/split/wagon_2.jpg">
  <figcaption>Wagon 2</figcaption>
</figure>
<figure class="affix">
  <img src="images/post-processing/split/wagon_3.jpg">
  <figcaption>Wagon 3</figcaption>
</figure>
<figure class="affix">
  <img src="images/post-processing/split/wagon_4.jpg">
  <figcaption>Wagon 4</figcaption>
</figure>

<p>
  Dieses Verfahren hat nicht für Infrarot Bilder funktioniert. In Matlab haben wir deshalb ein weiteres Verfahren
  entwickelt. Es ist aus Zeitgründen noch nicht in die Pipeline integriert, aber die Machbarkeit ist bewiesen.
  Die Idee bei den Infrarotbildern ist es die Räder zu finden und dazwischen abzuschneiden. Es sind nämlich immer
  zwei Räder am Anfang und am Ende eines Wagons. Somit kann man in der Mitte von vier nahe beieinanderliegenden
  Räder abschneiden und trifft so genau das Ende eines Wagons. Die Räder werden mit der <a href="#ref-phase-coding">Phase
  Coding Methode von Atherton and Kerbyson</a> gefunden. Alternativ wäre dies auch möglich über eine
  Hough-Transformation.
</p>

<figure>
  <img src="images/post-processing/split/wheel_detected_detail.jpg">
  <figcaption>Räder detektieren, Detailansicht</figcaption>
</figure>

<figure>
  <img src="images/post-processing/split/wagon_ir_1.jpg">
  <figcaption>Wagon Infrarot 1</figcaption>
</figure>
<figure class="affix">
  <img src="images/post-processing/split/wagon_ir_2.jpg">
  <figcaption>Wagon Infrarot 2</figcaption>
</figure>
<figure class="affix">
  <img src="images/post-processing/split/wagon_ir_3.jpg">
  <figcaption>Wagon Infrarot 3</figcaption>
</figure>
<figure class="affix">
  <img src="images/post-processing/split/wagon_ir_4.jpg">
  <figcaption>Wagon Infrarot 4</figcaption>
</figure>

<!-- References -->

<p id="ref-phase-coding" class="reference-item">
  <span class="ref">T.J Atherton, D.J. Kerbyson. "Size invariant circle detection." Image and Vision Computing. Volume 17, Number 11, 1999, pp. 795-803.</span>
</p>

  <h3>Metadaten extrahieren</h3>

<ul>
  <li><b>In:</b> MP4 Video, Panorama / Wagon Bilder</li>
  <li><b>Out:</b> Panorama / Wagon Bilder mit Metadaten</li>
</ul>

<p>
  Der letzte Schritt der Pipeline ist nur für Infrarotbilder notwendig. Beim Komprimieren der Rohdaten zu MP4 wenden wir
  eine Skalierung an. Die Skalierungsparameter sind in den Metadaten der MP4 Videodatei enthalten. In diesem letzten
  Schritt werden sie in die finalen zusammengesetzten Bilder geschrieben. Dies ist notwendig um später im Analysetool
  die richtigen Temperaturen berechnen zu können. Dafür sind logischerweise die unskalierten Rohdaten notwendig.
</p>

<p>
  Die Skalierungsparameter sind in den EXIF-Metadaten im Feld Comment gespeichert:
</p>

<figure>
  <pre><code>» exiftool -Comment 2017-11-23@23-13-01-IR.seq.mp4
Comment     : 1441/0.0827943086624146</code></pre>
  <figcaption>Skalierungsfaktor in EXIF Metadaten</figcaption>
</figure>


</div>

<div class="chapter">
  <h2 class="break">Analyse: Thermoboard</h2>

  <h3>Ziel</h3>
  <p>
    Um die akquirierten und verarbeiteten Sensordaten nun analysieren zu können, müssen wir eine geeignete Plattform
    finden. Diese Plattform soll einem Experten, der sich mit der Technik der Züge auskennt, ermöglichen die Daten in
    seinem Büro anzusehen, diese zu vergleichen und dadurch eventuell weitere Inspektionen von Fahrzeugen anzufordern.
  </p>

  <p>
    Eine automatische Analyse mit Bilderkennung oder gar Machine Learning können wir leider noch nicht implementieren,
    weil wir in unserem Projekt kein Schadensfall gefunden haben. Man könnte zwar sagen, man vergleicht Zugswagen vom
    selben Typ untereinander und markiert die gefundenen Unterschiede auf den Bildern automatisch. Dies bringt aber
    mit dem jetzigen Wissensstand auch eher wenig, weil der Wagen auf dem Infrarotbild deutlich anders aussehen kann,
    je nachdem was er vorher gemacht hat. Ist er auf dem Gleisfeld stillgestanden? Liefen die Heizungen? Strahlte ihn
    die Sonne an? All diese Gegebenheiten würden zu Unterschieden führen. Diese müssen zuerst in Erfahrung gebracht
    und festgehalten werden. Nur so kann man sagen, ob der Unterschied nun einen Schaden darstellt oder einfach eine
    andere Rahmenbedingung.
  </p>

  <p>
    Deshalb brauchen wir eine Applikation, die das Vergleichen der Aufnahmen erlaubt. Unsere Aufnahmen sind nach dem
    Post-Processing sehr breite Bilder mit einer Auflösung von bis zu 20'000 Pixeln in der Breite und 512 Pixel in
    der Höhe. Die erste Idee war, diese Bilder mit den rohen Sensordaten anzureichern, damit sie in den FLIR Tools
    angesehen werden können. Die FLIR Tools bieten alle Werkzeuge, um die Bilder zu analysieren, Kommentare zu
    erfassen und sogar Reports zu erstellen. Der Nachteil der FLIR Tools ist allerdings, dass diese lokal installiert
    werden müssen. Müssten jetzt alle Experten, die die Bilder ansehen möchten, bei ihrem SBB Büro-PC eine Software
    installieren, gäbe das einen grossen Aufwand für die interne IT.
  </p>

  <p>
    Das zweite Problem ist, dass die FLIR Tools auf dem FLIR ATLAS SDK basieren. Wenn ein Bild geöffnet wird, welches
    nicht einer Auflösung entspricht, zu der es auch eine FLIR Kamera gibt, stürzt die Software einfach mit einer
    Exception ab. Warum das passiert scheint uns ein Rätsel, kann aber durchaus so gewollt sein. FLIR
    hat keine Interessen Tools zu bauen, mit denen andere Bilder analysiert werden können. Jedenfalls haben wir
    bewiesen, dass es wirklich an der Auflösung liegt. Ansonsten würde der Ansatz funktionieren. Wir haben unsere
    Panoramabilder mit den Binärdaten und Metadaten angereichert und auf eine Auflösung herunterskaliert von 640 x
    512 Pixeln. Dann konnte man das Bild ansehen und die Temperaturen wurden korrekt berechnet. Auch haben wir im
    FLIR Produktekatalog alle Kameraauflösungen herausgesucht und festgestellt, dass alle diese Auflösungen
    funktionieren. Nun ist jedoch ein Bild, dass von 20'000 Pixeln Breite auf 640 Pixel Breite reduziert wurde, nicht
    mehr brauchbar. FLIR hat uns da also einen Stein in den Weg gelegt.
  </p>

  <p>
    Als Lösung haben wir unsere eigene Analyseplattform namens Thermoboard geschaffen. Die Webapplikation erlaubt es
    Experten über den Browser die Sensordaten anzusehen, die Züge zu suchen, die sie interessieren und Kommentare zu
    verfassen.
  </p>

  <h3>Architektur</h3>
  <p>
    Das Thermoboard ist eine Webapplikation die auf modernen Web Standards basiert.
  </p>
  <p>
    Der Technologie-Stack der Applikation enthält diese Kerntechnologien:
  </p>
  <ul>
    <li><a href="#ref-laravel">Laravel PHP Framework</a></li>
    <li><a href="#ref-mariadb">MariaDB</a></li>
    <li><a href="#ref-graphql">GraphQL Backend</a></li>
    <li><a href="#ref-angular">Angular Frontend</a></li>
    <li><a href="#ref-angular-material">Angular Material</a></li>
    <li><a href="#ref-apollo">Apollo GraphQL Client</a></li>
  </ul>

  <p>
    Die Idee hinter der Architektur ist, dass das Thermoboard nicht spezifisch nur für den Thermo-Scanner
    funktionieren kann. Es soll möglich sein irgendwelche Sensordaten die von irgendwo her kommen darin zu
    integrieren und auszuwerten. Deshalb haben wir die folgende Abstraktionslayers eingeführt mit dem jeweiligen
    Pendant aus diesem Projekt als Beispiel:
  </p>

  <ol>
    <li>Controller: Thermo-Scanner</li>
    <li>Sensoren: Infrarotkamera, Farbkamera, Temperaturmesser, Wettersensor</li>
    <li>Werte: Bild, Text, Zahl, Temperatur</li>
    <li>Orte: Zürich Altstetten</li>
  </ol>

  <p>
    Weiter gibt es <code>records</code> unter welchem wir eine Aufnahme verstehen. Die Aufnahme selber kann
    Sensorwerte haben. Zu einem <code>record</code> kann es auch <code>entity_records</code> geben, welche mit einem
    <code>entity</code> und einem <code>record</code> verknüpft sind und auch Sensorwerte haben. Ein Entity ist in
    unserem Fall ein Fahrzeug. Der Record muss zwingend an einem Ort gemacht worden sein, weswegen wir diesen dort
    auch verknüpfen. Ein Entity kann also in mehreren Records zu unterschiedliechen Zeit Sensorwerte von
    verschiedensten Sensoren an verschiedenen Orten erhalten. Also genau so wie das ein Zugwagen kann. Dieser kann in
    eine neue Zugkomposition aufgenommen werden. Meistens jedoch befinden er sich, gereade im Falle eines Triebzuges,
    immer in der selben Komposition.
  </p>

  <p>
    Hierzu das Datenbankschema. Administrative Tabellen wie Benutzer, Sessions, Migrationen und so weiter wurden
    ausgeblendet.
  </p>

  <figure class="full">
    <img src="images/thermoboard/database-diagram.png">
    <figcaption>Thermoboard Datenbankdiagramm</figcaption>
  </figure>

  <h3>Implementation</h3>
  <p>
    Das Thermoboard ist schlicht gehalten und setzt Material Design um. Die dazu verwendete Library
    <code>angular-material</code> hilft dabei die Komponenten gemäss Designspezifikation umzusetzen. Mithilfe von
    Bootstrap gestalten wir das Layout.
  </p>
  <p>
    Auf der Startseite sieht der Benutzer ein Dashboard mit interessanten Graphen, seinen zuletzt angesehenen
    Einträge, seine Favoriten und die neusten Aktivitäten. Die Graphen werden mit der Library <a
    href="#ref-chartjs">Chart.js</a> erzeugt. Die zuletzt angesehenen Einträge und die Favoriten werden im
    <code>localStorage</code> abgelegt.
  </p>

  <figure>
    <img src="images/thermoboard/dashboard.png">
    <figcaption>Thermoboard Screenshot Dashboard</figcaption>
  </figure>

  <p>
    Neben dem Dashboard kann der Benutzer entweder über die Seite "Fahrzeugtypen" oder "Aufnahmen" weiter eintauchen.
    Über die Seite Fahrzeugtypen, wählt er zuerst den Fahrzeugtyp aus und sieht dann eine Liste an Fahrzeugen. Diese
    kann er mit einem Freitextfeld weiter filtern und kann so zur Detailansicht des Fahrzeugs gelangen.
  </p>

  <figure>
    <img src="images/thermoboard/entity-kinds.png">
    <figcaption>Thermoboard Screenshot Fahrzeugtypen</figcaption>
  </figure>

  <p>
    Über die Seite "Aufnahmen" kann der Benutzer die letzten Aufnahmen einsehen. Auch hier gibt es ein Freitextfeld
    um die Liste zu filtern. So kann zum Beispiel nach Ort oder nach einer bestimmten Menge Fahrzeugtypen gefiltert
    werden.
  </p>

  <figure>
    <img src="images/thermoboard/recordings.png">
    <figcaption>Thermoboard Screenshot Aufnahmen</figcaption>
  </figure>

  <p>
    Auf der Detailansicht einer Aufnahme sieht der Benutzer die Sensorwerte der ganzen Aufnahme. Weiter unten sind
    alle Fahrzeugaufnahmen aufgelistet die zu dieser Aufnahmen gehören. Navigiert der Benutzer dort hin, sieht er die
    Werte des einzelnen Fahrzeuges sowie alle dessen anderen Aufnahmen.
  </p>

  <figure>
    <img src="images/thermoboard/record.png">
    <figcaption>Thermoboard Screenshot Aufnahme</figcaption>
  </figure>

  <h4>Image Viewer</h4>
  <p>
    Der eigens gebaute Image Viewer erlaubt es dem Benutzer in die sehr grossen Panoramabilder hineinzuzoomen und
    Details zu betrachten. Ausserdem kann der Viewer die Temperaturen aus den Infrarotbildern automatisch berechnen
    und zeigt sie dem Benutzer über der Maus an. Der Viewer ist ein Canvas Element, in welches wir das Bild an den
    bestimmten Koordinaten zeichnen. Mit der Maus kann der Benutzer das Bild verschieben. Mit dem Mausraud kann
    hineingezoomt werden.
  </p>

  <figure>
    <img src="images/thermoboard/image-viewer.png">
    <figcaption>Thermoboard Screenshot Aufnahme</figcaption>
  </figure>

  <p>
    Um die Temperatur berechnen zu können, müssen wir als erstes die Falschfarbe wieder zurück auf eine Graustufe
    führen. Den Pixelwert können wir auslesen, weil wir ein Canvas verwenden. Dann könnte man meinen, man kann einfach
    eine Lookup Table der Color Map anfertigen und diese abfragen. Leider gibt es in JPG Bildern immer kleine
    Abweichungen durch Kompressionsartefakte. Das heisst, wir müssen eine Nearest Neighbor Search durchführen und die
    naheste Farbe im RGB Raum in der Lookup Table finden. Zum Glück ist die JavaScript Welt voll mit Libraries. Die
    Library <a href="#ref-nearest-color">nearest-color</a> macht genau das.
  </p>

  <p>
    Danach können wir mit <a href="#ref-exif-js">Exif.js</a> die Exifdaten auslesen um den von uns
    definierten Skalierungs- und Offsetparameter zu erhalten. Den gelesenen Wert geben wir in die Temperaturformel
    hinein und heraus kommt die Temperatur in Grad Celsius. Die Formel haben wir soweit wie möglich mit sinnvollen
    Konstanten ausgerechnet damit diese live, während die Maus bewegt wird, berechnet werden kann.
  </p>

  <figure>
    <pre><code>private calculateTemp(value: number): number {
  const obj = (value / 0.95 - 154.2268539477985 + -207);
  const log = Math.log(16556 / (0.046952017 * obj) + 1);
  const celsius = 1428 / log - 273.15;

  return celsius;
}
</code></pre>
    <figcaption>Code Temperaturberechnung im Browser</figcaption>
  </figure>

  <h3>Reflexion</h3>
  <p>
    Das Thermoboard zeigt wie sich eine Predictive Maintenance Plattform aus Endbenutzersicht anfühlt. Uns ist klar,
    dass die Plattform nicht Feature-Komplett ist und weitere Anforderungen umgesetzt werden müssen, die vor allem das
    kollaborative Zusammenarbeiten auf der Plattform fördern. Zudem müssen die Daten mit weiteren
    Kontextinformationen aus anderen SBB System angereichert werden können. So soll zum Beispiel nachvollzogen werden
    können, welche Route ein Fahrzeug vor der Aufnahme gefahren ist. Eventuell ist auch eine
    Integration in ein anderes Messsystem anzudenken, welches die Passagierraumtemperatur aufzeichnet. All diese
    Informationen sind ein Puzzlestück des Ganzen und helfen am Schluss eine saubere Analyse zu machen.
  </p>

</div>

<!-- References -->

<p id="ref-laravel" class="reference-item">
  <span class="ref">Laravel PHP Framework</span>
  <a href="https://laravel.com/" target="_blank" rel="noopener">https://laravel.com</a>
  <span class="retrieved">15. Januar 2018</span>
</p>
<p id="ref-mariadb" class="reference-item">
  <span class="ref">MariaDB</span>
  <a href="https://mariadb.org/" target="_blank" rel="noopener">https://mariadb.org</a>
  <span class="retrieved">15. Januar 2018</span>
</p>
<p id="ref-graphql" class="reference-item">
  <span class="ref">GraphQL</span>
  <a href="http://graphql.org/" target="_blank" rel="noopener">http://graphql.org</a>
  <span class="retrieved">15. Januar 2018</span>
</p>
<p id="ref-angular" class="reference-item">
  <span class="ref">Angular Frontend Framework</span>
  <a href="https://angular.io/" target="_blank" rel="noopener">https://angular.io</a>
  <span class="retrieved">15. Januar 2018</span>
</p>
<p id="ref-angular-material" class="reference-item">
  <span class="ref">Angular Material</span>
  <a href="https://material.angular.io/" target="_blank" rel="noopener">https://material.angular.io</a>
  <span class="retrieved">15. Januar 2018</span>
</p>
<p id="ref-apollo" class="reference-item">
  <span class="ref">Apollo GraphQL Client</span>
  <a href="https://www.apollographql.com/" target="_blank" rel="noopener">https://www.apollographql.com</a>
  <span class="retrieved">15. Januar 2018</span>
</p>
<p id="ref-chartjs" class="reference-item">
  <span class="ref">Chart.js Graph Library</span>
  <a href="http://www.chartjs.org/" target="_blank" rel="noopener">http://www.chartjs.org</a>
  <span class="retrieved">16. Januar 2018</span>
</p>
<p id="ref-nearest-color" class="reference-item">
  <span class="ref">JavaScript Library nearest-color</span>
  <a href="https://github.com/dtao/nearest-color" target="_blank"
     rel="noopener">https://github.com/dtao/nearest-color</a>
  <span class="retrieved">16. Januar 2018</span>
</p>
<p id="ref-exif-js" class="reference-item">
  <span class="ref">JavaScript Library Exif.js</span>
  <a href="https://github.com/exif-js/exif-js" target="_blank" rel="noopener">https://github.com/exif-js/exif-js</a>
  <span class="retrieved">16. Januar 2018</span>
</p>

<div class="chapter">
  <h2 class="break">Research</h2>

  <p>
    Für das Lösen diverser Probleme mussten wir Theorie aufarbeiten, recherchieren oder Prototypen bauen.
    In diesem Kapitel sind die Erkenntnisse und das notwendige Wissen zusammengefasst. Bei den Prototypen handelt
    es sich meist um Matlab-Skripts im Zusammenhang mit dem Post-processing oder der Daten-Akquise.
  </p>

  <h3><a name="infrarot-theorie">Infrarot Theorie</a></h3>

<p>
  Das folgende Kapitel basiert auf dem Leitfaden <a href="#ref-flir-theory">FLIR: Infrared Theory</a>, das von FLIR
  für alle Anwender ihrer Infrarotkameras bereitgestellt wird. Darin ist die grundlegende Theorie zum Thema Infrarot
  beschrieben sowie die Messformel, die für die Temperaturberechnung benötigt wird. Zusätzlich ist das Kapitel mit
  Informationen aus Wikipedia und eigenen Erkenntnissen angereichert.
</p>

<h4>Einleitung</h4>

<p>
  Eine Infrarotkamera misst die ausgestrahlte Infrarotstrahlung eines Objekts. Da die Strahlung eine Funktion der
  Oberflächentemperatur des Objekts ist, kann man daraus die Temperatur berechnen. Der von der Kamera gemessene Wert
  spiegelt aber nicht die wirkliche Temperatur des Objekts wieder, sondern ist auch von der Umgebung abhängig. Die
  Atmosphäre absorbiert einen gewissen Teil der Strahlung und auch reflektierte Strahlung von anderen Objekten
  beeinflusst die gemessene Strahlung.
</p>

<p>
  FLIR Kameras bereinigen die Messungen bei der Aufnahme in Echtzeit. Dafür muss die Kamera aber richtig parametrisiert
  sein, damit die verschiedenen Strahlungsquellen berücksichtig werden können. Diese Parameter sind:
</p>

<ul>
  <li>Emissionsgrad</li>
  <li>scheinbar reflektierte Temperatur</li>
  <li>Distanz zum Objekt</li>
  <li>relative Luftfeuchtigkeit</li>
  <li>atmosphärische Temperatur</li>
</ul>

<h4>Kamera Parameter</h4>

<h5>Emissionsgrad</h5>
<p>
  Der Emissionsgrad ist der wichtigste Parameter. Er sagt aus wieviel Strahlung ein Objekt emittiert im Vergleich zu
  einem perfekten Schwarzkörper (bei derselben Temperatur). Typische Werte liegen zwischen 0.1 und 0.95. Einige
  Beispiele:
</p>

<figure>
  <table>
    <thead>
    <tr>
      <th>Material</th>
      <th>Emissionsgrad</th>
    </tr>
    </thead>
    <tbody>
    <tr>
      <td>hochpolierte Oberfläche (Spiegel)</td>
      <td>&lt; 0.1</td>
    </tr>
    <tr>
      <td>Ölfarbe</td>
      <td>> 0.9</td>
    </tr>
    <tr>
      <td>menschliche Haut</td>
      <td>~ 0.97</td>
    </tr>
    <tr>
      <td>nicht-oxidierte Metalle</td>
      <td>tiefer Emissionsgrad der mit Temperatur ansteigt (opak und spiegelnd)</td>
    </tr>
    </tbody>
  </table>
</figure>

<p>Der Emissionsgrad ist für viele Materialien bekannt, kann aber auch im Experiment bestimmt werden.</p>

<h5>Scheinbar reflektierte Temperatur</h5>
<p>
  Dient dazu die Strahlung anderer Quellen zu kompensieren, die das Objekt in die Kamera reflektiert.
  Bei tiefem Emissionsgrad und wenn sich die effektive Temperatur von der reflektierten stark unterscheiden,
  ist dieser Parameter wichtig.
</p>

<h5>Distanz zum Objekt</h5>
<p>
  Ist die Distanz vom Objektiv bis zum Objekt. Dieser Parameter ist wichtig, weil die Atmosphäre Strahlung absorbiert
  und selbst Strahlung erzeugt.
</p>

<h5>Relative Luftfeuchtigkeit</h5>
<p>
  Die Übertragung der Strahlung ist auch von der Luftfeuchtigkeit abhängig. Bei kurzen Distanzen und normaler
  Luftfeuchtigkeit kann als Standardwert 50% genommen werden.
</p>

<h5>Atmosphärische Temperatur</h5>
<p>
  Temperatur die zwischen Kamera und Objekt herrscht.
</p>

<h4>Infrarot im elektromagnetischen Spektrum</h4>
<p>
  Das elektromagnetische Spektrum ist in Bänder aufgeteilt. Die Aufteilung kommt von den Methoden mit denen man die
  Strahlung der jeweiligen Bänder messen oder produzieren kann.
</p>

<figure>
  <img src="images/research/elek_spectrum.png" width="700">
  <figcaption>Elektromagnetisches Spektrum</figcaption>
  <span class="source">Quelle: <a href="#ref-flir-theory">FLIR: Infrared Theory</a></span>
</figure>

<p>Elektromagnetisches Spektrum: 1: Röntgen, 2: UV, 3: Sichtbar, 4: IR, 5: Mikrowellen, 6: Radiowellen</p>

<p>Thermographie beschränkt sich auf das Infrarotband. Typischerweise im Wellenlängenbereich von 2 - 13μm. Der
  Infrarotbereich kann
  noch weiter in folgende Bänder unterteilt werden:
</p>
<ul>
  <li>near infrared (0.75 - 3μm)</li>
  <li>middle infrared (3 - 6μm)</li>
  <li>far infrared (6 - 15μm)</li>
  <li>extreme infrared (15 - 100μm)</li>
</ul>

<h4>Schwarzer Körper</h4>
<p>
  Ein schwarzer Körper ist ein theoretischer Körper, der die gesamte eintreffende Strahlung unabhängig der Wellenlänge
  absorbiert. Der Begriff wurde durch Gustav Robert Kirchhoff geprägt, der folgendes Gesetz entdeckt hat:
</p>
<ul>
  <li>Für jede Wellenlänge absorbiert der schwarze Körper das Maximum an Strahlung, d.h. er emittiert auch das Maximum
    an Strahlung für alle Wellenlängen.
  </li>
</ul>
<p>
  Die Strahlung des Schwarzkörpers wird Schwarzkörperstrahlung oder auch Hohlraumstrahlung genannt. Die Intensität und
  das Spektrum der Schwarzkörperstrahlung hängen nur von der Temperatur ab. Der schwarze Körper stellt die ideale
  Strahlungsquelle dar und wird als Referenz für reale Köper genommen.
</p>

<p>
  Ein Schwarzkörper kann simuliert werden, in dem man ein kleines Loch in einen Hohlraum bohrt. Alle einfallende
  Strahlung wird innerhalb des Hohlraums reflektiert und schliesslich praktisch vollständig absorbiert. Die kleine
  Öffnung verhält sich somit annähernd wie ein Schwarzkörper. Wird der Hohlraum nun erhitzt, kann man einen sogenannten
  Hohlraumstrahler erzeugen. Damit werden Wärmebildkameras kalibriert. Was man mit einem solchen Hohlraum auch
  beobachten kann, ist der Fakt, dass bei einer Temperatur &gt;525°C die austretende Strahlung sichtbar wird. Das Licht
  ändert die Farbe von Rot über Orange, Gelb bis zu Blauweiss. Die sogenannte Farbtemperatur ist nichts anderes als die
  Temperatur, auf die ein Schwarzkörper erhitzt werden muss, um gleich auszusehen.
</p>

<p>Zur Hohlraum- resp. Schwarzkörperstrahlung gibt es drei wichtige Gesetze:</p>

<h5>Planck'sches Strahlungsgesetz</h5>

<figure>
  <img src="images/research/planck_law.svg" style="height:4em;">
  <figcaption>Planck'sches Strahlungsgesetz</figcaption>
  <span class="source">Quelle: <a href="#ref-planck-wiki">Planck'sches Strahlengesetz, Wikipedia</a></span>
</figure>

<figure>
  <table>
    <thead>
    <tr>
      <th>Variable</th>
      <th>Bedeutung</th>
    </tr>
    </thead>
    <tbody>
    <tr>
      <td>B<sub>λ</sub></td>
      <td>Spektrale Ausstrahlung [W/sr m<sup>3</sup>]<br>(Strahlungsleistung pro Flächeneinheit pro Raumwinkel)</td>
    </tr>
    <tr>
      <td>e</td>
      <td>eulerische Zahl</td>
    </tr>
    <tr>
      <td>h</td>
      <td>Planck'sches Wirkungsquantum [J s]</td>
    </tr>
    <tr>
      <td>c</td>
      <td>Lichtgeschwindigket [m/s]</td>
    </tr>
    <tr>
      <td>λ</td>
      <td>Wellenlänge [m]</td>
    </tr>
    <tr>
      <td>k</td>
      <td>Boltzmann Konstante</td>
    </tr>
    <tr>
      <td>T</td>
      <td>Objekt Temperatur [K]</td>
    </tr>
    </tbody>
  </table>
</figure>

<p>
  Die von Max Planck (1858 - 1947) entwickelte Formel beschreibt die Verteilung der
  elektromagnetischen Abstrahlung eines Schwarzkörpers in Abhängigkeit der Wellenlänge.
</p>
<p>
  Randbemerkung: Die Herleitung des Strahlengesetzes war mit der damaligen Physik nicht
  möglich, weshalb Planck Quanten (Photonen) einführte. Dies gilt als Geburtsstunde der
  Quantenphysik.
</p>
<p>
  Die graphische Darstellung für verschiedene Temperaturen ergibt eine Familie von Kurven. Bei jeder
  dieser Planckkurven ist die Ausstrahlung null bei λ = 0, steigt danach stark an bis zum
  Maximum bei λ<sub>max</sub> und flacht danach wieder Richtung null ab.
  Je höher die Temperatur, desto schmaler wird die Kurve im Bereich des Maximums. Bei einer Temperatur
  von 1000K ist ein Schwarzkörper also am besten bei einer Wellenlänge von ca. 3μm im middle
  infrared Bereich sichtbar/messbar.
</p>

<figure>
  <img src="images/research/planck_spectrum.png" width="400px">
  <figcaption>Planck'sches Strahlungsspektrum</figcaption>
  <span class="source">Quelle: <a href="#ref-planck-wiki">Planck'sches Strahlengesetz, Wikipedia</a></span>
</figure>

<p>
  <b>Wien'sches Verschiebungsgesetz</b>
</p>
<p>
  Das wiensche Verschiebungsgesetz besagt, dass die Wellenlänge λ<sub>max</sub> (siehe oben)
  umgekehrt proportional zur Temperatur ist.
</p>

<figure>
  <img src="images/research/wien_law.svg">
  <figcaption>Wien'sches Verschiebungsgesetz</figcaption>
  <span class="source">Quelle: <a href="#ref-wien-wiki">Wien'sches Verschiebungsgesetz, Wikipedia</a></span>
</figure>

<p>
  Wie vorhin bereits beschrieben, ist das der Grund wieso z.B. Metall zuerst
  rot, dann orange usw. anfängt zu glühen. Oder die Sonne, die ca. 6000K heiss ist, emittiert
  gelbes Licht mit einem λ<sub>max</sub> von ca. 0.5μm.
</p>

<p>
  <b>Stefan-Boltzmann-Gesetz</b>
</p>
<p>
  Das Stefan-Boltzmann-Gesetz beschreibt die Abstrahlung eines Schwarzkörpers in
  Abhängigkeit der Temperatur. Graphisch gesehen ist das die Fläche unter einer Planckkurve.
  Das Gesetz sagt aus, dass die Strahlungsleistung eines Schwarzkörpers proportional zur
  vierten Potenz seiner absoluten Temperatur ist. D.h. eine Verdoppelung der Temperatur
  lässt die abgestrahlte Leistung um das 16-fache ansteigen.
</p>
<figure>
  <img src="images/research/stefan-boltzmann-law.svg">
  <figcaption>Stefan-Boltzmann-Gesetz</figcaption>
  <span class="source">Quelle: <a href="#ref-stefan-boltzmann-wiki">Stefan-Boltzmann-Gesetz, Wikipedia</a></span>
</figure>


<h4>Nicht-Schwarze Strahler</h4>
<p>
  Das Verhalten eines idealen Schwarzkörpers ist nun klar. Wie sieht es aber mit realen Objekten aus?
  Diese Verhalten sich, über grössere Wellenlängenbereiche gesehen, praktisch nie wie ein Schwarzkörper.
  Es gibt drei Faktoren, die einen realen Körper beeinflussen:
</p>

<ul>
  <li><b>spektrale Absorption α<sub>λ</sub></b>: absorbiert Strahlungsleistung durch den Körper</li>
  <li><b>spektrale Reflexion ρ<sub>λ</sub></b>: reflektierte Strahlungsleistung durch den Körper</li>
  <li><b>spektrale Transmission τ<sub>λ</sub></b>: durchgelassene Strahlungsleistung</li>
</ul>

<p>
  Die Summe dieser drei Faktoren muss immer eins ergeben.
</p>

<figure>
  <img src="images/research/non_black_body_emitters.svg" style="height:1.5em;">
  <figcaption>Zusammensetzung Abstrahlung (Emission)</figcaption>
  <span class="source">Quelle: <a href="#ref-flir-theory">Infrared Theory, FLIR</a></span>
</figure>

<p>
  Ein weiterer Faktor, der bereits viel erwähnte Emissionsgrad ε ist nötig um die Abstrahlung im Verhältnis zu einem
  Schwarzkörper bei derselben Temperatur anzugeben. Der spektrale Emissionsgrad ε<sub>λ</sub> ist das Verhältnis
  der spektralen Strahlungsleistung eines Köpers zu dem eines Schwarzkörpers bei gleicher Temperatur und Wellenlänge λ.
</p>

<figure>
  <img src="images/research/spectral_emissivity.svg" style="height:4em;">
  <figcaption>spektraler Emissionsgrad</figcaption>
  <span class="source">Quelle: <a href="#ref-flir-theory">Infrared Theory, FLIR</a></span>
</figure>

<p>
  Man kann Strahlungsquellen in drei Typen aufteilen und zwar je nachdem wie ihre Abstrahlung in
  verschiedenen Wellenlängen variiert.
</p>
<ul>
  <li>Schwarzkörper mit ε<sub>λ</sub> = ε = 1</li>
  <li>Graukörper mit ε<sub>λ</sub> = ε &lt; 1</li>
  <li>Selektiver Strahler mit ε<sub>λ</sub> variierend je nach Wellenlänge</li>
</ul>

<p>
  Gemäss dem Kirchhoff-Gesetz ist die spektrale Abstrahlung gleich der spektralen Absorption bei einer bestimmten
  Temperatur und Wellenlänge. Somit gilt:
</p>

<figure>
  <img src="images/research/flir_1.svg" style="height:1.5em;">
  <figcaption>Emissionsgrad</figcaption>
  <span class="source">Quelle: Infrared Theory, FLIR [30]</a></span>
</figure>

<p>
  Für opake Materialien ergibt das folgende Beziehung (gemäss der Formel α<sub>λ</sub> + ρ<sub>λ</sub> + τ<sub>λ</sub> =
  1)
</p>

<figure>
  <img src="images/research/flir_2.svg" style="height:1.5em;">
  <figcaption>Opakes Objekt: Emission und Reflexion</figcaption>
  <span class="source">Quelle: Infrared Theory, FLIR [30]</a></span>
</figure>

<p>
  Für hochpolierte Materialen, bspw. ein perfekter Spiegel, bei dem ε<sub>λ</sub> gegen Null geht gilt nur noch:
</p>

<figure>
  <img src="images/research/flir_3.svg" style="height:1.5em;">
  <figcaption>Perfekter Spiegel: nur Reflexion</figcaption>
  <span class="source">Quelle: Infrared Theory, FLIR [30]</a></span>
</figure>

<p>
  Für reale Körper, seien das nun Graukörper oder selektive Strahler, muss man das Stefan-Boltzmann-Gesetz mit dem
  Faktor ε erweitern. Auf der folgenden Grafik sieht man schön die Unterschiede der drei Typen. Zuoberst der
  Schwarzkörper, darunter der Graukörper der mit ε proportional zum Schwarzkörper verläuft und rot der selektive
  Strahler.
</p>

<figure>
  <img src="images/research/emitter_types.png">
  <figcaption>Spektrale Abstrahlung der drei Strahlungsquellen</figcaption>
  <span class="source">Quelle: <a href="#ref-flir-theory">Infrared Theory, FLIR</a></span>
</figure>

<p>1: spektrale Abstrahlung, 2: Wellenlänge, 3: Schwarzkörper, 4: selektiver Strahler, 5: Graukörper</p>

<figure>
  <img src="images/research/emissionsgrade.png">
  <figcaption>Spektrale Emissionsgrade der drei Strahlungsquellen</figcaption>
  <span class="source">Quelle: <a href="#ref-flir-theory">Infrared Theory, FLIR</a></span>
</figure>

<p>1: spektraler Emissionsgrad, 2: Wellenlänge, 3: Schwarzkörper, 4: Graukörper, 5: selektiver Strahler</p>

<h4>Die Mess-Formel</h4>

<p>
  Wie bereits erwähnt, misst eine Infrarotkamera nicht nur die Strahlung des Objekts selber sondern auch reflektierte
  und atomsphärische Strahlung. Zudem wird die Messung von der Absorption der Atmosphäre beeinflusst. Die generelle
  Messsituation ist in untenstehender Grafik dargestellt. Dabei wird angenommen, dass durch anpassen der Messsituation
  grössere Störquellen (wie z.B. Hitzequellen in der Umgebung), eliminiert werden.
</p>

<figure>
  <img src="images/research/mess_situation.png">
  <figcaption>Generelle Messsituation</figcaption>
  <span class="source">Quelle: <a href="#ref-flir-theory">Infrared Theory, FLIR</a></span>
</figure>

<p>1: Umgebung, 2: Objekt, 3: Atmosphäre, 4: Kamera</p>

<p>
  Ein perfekter Schwarzkörper mit Abstrahlung W bei Temperatur T<sub>source</sub> erzeugt über kurze Distanz auf der
  Kamera die Ausgangsspannung U<sub>source</sub>. Dies ergibt folgende Formel, wobei C eine Konstante ist.
</p>

<figure>
  <img src="images/research/flir_4.svg" style="height:2em;">
  <figcaption>Gleichung 1: Ausgangsspannung</figcaption>
  <span class="source">Quelle: Infrared Theory, FLIR [30]</a></span>
</figure>

<p>oder vereinfacht:</p>

<figure>
  <img src="images/research/flir_5.svg" style="height:2em;">
  <figcaption>Gleichung 2: Ausgangsspannung vereinfacht</figcaption>
  <span class="source">Quelle: Infrared Theory, FLIR [30]</a></span>
</figure>

<p>
  Wenn das Objekt ein Graukörper mit Emissionsgrad ε ist die Abstrahlung W<sub>source</sub> mit εW<sub>source</sub>
  zu ersetzen. Die Abstrahlung W<sub>source</sub> setzt sich zusammen aus:
</p>
<ol>
  <li>
    Abstrahlung vom Objekt = ετW<sub>obj</sub>, wobei ε der Emissionsgrad des Objekts und τ die Durchlässigkeit der
    Atmosphäre ist. Die Objekttemperatur ist T<sub>obj</sub>.
  </li>
  <li>
    Reflektierte Abstrahlung aus der Umgebung = (1-ε)τW<sub>refl</sub>, wobei (1-ε) der Reflexionsgrad des Objektes ist.
    Die Strahlungsquellen aus der Umgebung haben die Temperatur T<sub>refl</sub>. Die Annahme ist, dass T<sub>refl</sub>
    für alle Strahlungsquellen, in derselben Halbsphäre wie das Objekt, die gleiche ist.
  </li>
  <li>
    Abstrahlung aus der Atmosphäre = (1-τ)τW<sub>atm</sub>, wobei (1-τ) der Emissionsgrad der Atmosphäre ist. Die
    Atmosphärentemperatur ist T<sub>atm</sub>
  </li>
</ol>

<p>
  Die total gemessene Abstrahlung kann somit wie folgt definiert werden:
</p>

<figure>
  <img src="images/research/flir_6.svg">
  <figcaption>Gleichung 3: Gemessene Abstrahlung</figcaption>
  <span class="source">Quelle: <a href="#ref-flir-theory">Infrared Theory, FLIR</a></span>
</figure>

<p>
  Multiplizieren wir jedes Element mit der Konstanten C aus Gleichung 1 und nennen das Produkt U erhalten wir
  folgende Gleichung:
</p>

<figure>
  <img src="images/research/flir_7.svg">
  <figcaption>Gleichung 4: Gemessene Ausgangsspannung</figcaption>
  <span class="source">Quelle: Infrared Theory, FLIR [30]</a></span>
</figure>

<p>
  Das ist die generelle Messformel die in allen FLIR Thermographiekomponenten angewandt wird.
  U<sub>tot</sub> ist dabei die von der Kamera gemessene Spannung und U<sub>obj</sub> die berechnete Spannung
  für einen Schwarzkörper mit Temperatur T<sub>obj</sub>. D.h. daraus kann direkt die effektive Temperatur
  des Objekts berechnet werden.
</p>

<p>
  All die notwendigen Parameter dienen also dazu einerseits diese Gleichung aufzustellen
  und daraus die richtige Objekttemperatur zu berechnen.
</p>

<h3>Implementation Temperaturberechnung</h3>

<p>Die Berechnung der Temperatur erfolgt in zwei Schritten:</p>

<ul>
  <li>
    Messformel nach der spektralen Ausstrahlung des Objekts W<sub>obj</sub> umformen und fehlende Variablen berechnen.
  </li>
  <li>Aus W<sub>obj</sub> mit dem planckschen Strahlungssatz die Objekttemperatur T<sub>obj</sub> bestimmen</li>
</ul>

<h5>Umrechnen gemessene Strahlung in Objekt Strahlung</h5>

<p>
  Der erste Schritt ist das Extrahieren der Ausstrahlung des Objekts W<sub>obj</sub> aus der gemessenen totalen
  Ausstrahlung W<sub>tot</sub>. Das erfolgt anhand der oben behandelten Messformel. Es muss also nach W<sub>obj</sub>
  aufgelöst werden.
</p>

<figure>
  <img src="images/research/object_emissivity.svg" style="height:4em;">
  <figcaption>Ausstrahlung Objekt</figcaption>
</figure>

<p>Von ε, τ, W<sub>refl</sub> und W<sub>atm</sub> ist momentan nur der Emissionsgrad ε bekannt. Er ist durch die
  Beschaffenheit des Objekts gegeben.
  Die restlichen Parameter sind zu berechnen.
</p>

<p>
  τ ist die Durchlässigkeit der Atmosphäre und berechnet sich aus der relativen Luftfeuchtigkeit H und der
  Atmosphärentemperatur T.
</p>

<figure>
  <img src="images/research/vpw.svg">
  <figcaption>P: Druck Wasserdampf</figcaption>
  <span class="source">Quelle: <a href="#ref-minkina-dudzik">Infrared Thermography: Errors and Uncertainties</a></span>
</figure>

<figure>
  <img src="images/research/tau.svg">
  <figcaption>τ: Durchlässigkeit Atmosphäre</figcaption>
  <span class="source">Quelle: Infrared Thermography: Errors and Uncertainties [34]</a></span>
</figure>

<figure>
  <table>
    <thead>
    <tr>
      <th>Variable</th>
      <th>Bedeutung</th>
    </tr>
    </thead>
    <tbody>
    <tr>
      <td>A<sub>trans x</sub></td>
      <td>Atmospheric Trans X: Konstante für die Berechnung der Transmission aufgrund der Luftfeuchtigkeit</td>
    </tr>
    <tr>
      <td>A<sub>trans A1</sub></td>
      <td>Atmospheric Trans Alpha 1: Konstante für die Berechnung der Transmission aufgrund der Luftfeuchtigkeit</td>
    </tr>
    <tr>
      <td>A<sub>trans B1</sub></td>
      <td>Atmospheric Trans Beta 1: Konstante für die Berechnung der Transmission aufgrund der Luftfeuchtigkeit</td>
    </tr>
    <tr>
      <td>A<sub>trans A2</sub></td>
      <td>Atmospheric Trans Alpha 2: Konstante für die Berechnung der Transmission aufgrund der Luftfeuchtigkeit</td>
    </tr>
    <tr>
      <td>A<sub>trans B2</sub></td>
      <td>Atmospheric Trans Beta 2: Konstante für die Berechnung der Transmission aufgrund der Luftfeuchtigkeit</td>
    </tr>
    <tr>
      <td>d</td>
      <td>Distanz zum Objekt [m]</td>
    </tr>
    <tr>
      <td>P</td>
      <td>Druck Wasserdampf [mmHg]</td>
    </tr>
    </tbody>
  </table>
</figure>

<p>
  W<sub>obj</sub>, W<sub>tot</sub>, W<sub>refl</sub> und W<sub>atm</sub> sind alles spektrale Ausstrahlungen. Also eine
  Strahlungsleistung pro Flächenelement pro Raumwinkel. Sie berechnen sich mit dem Planckschen Strahlungsgesetz. Mit
  diesem Gesetz kann man direkt aus der spektralen Ausstrahlung die Temperatur berechnen und umgekehrt.
</p>

<figure>
  <img src="images/research/planck_law.svg" style="height:4em;">
  <figcaption>Planck'sches Strahlungsgesetz</figcaption>
  <span class="source">Quelle: <a href="#ref-planck-wiki">Planck'sches Strahlengesetz, Wikipedia</a></span>
</figure>

<figure>
  <table>
    <thead>
    <tr>
      <th>Variable</th>
      <th>Bedeutung</th>
    </tr>
    </thead>
    <tbody>
    <tr>
      <td>B<sub>λ</sub></td>
      <td>Spektrale Ausstrahlung [W/sr m<sup>3</sup>]<br>(Strahlungsleistung pro Flächeneinheit pro Raumwinkel)</td>
    </tr>
    <tr>
      <td>e</td>
      <td>eulerische Zahl</td>
    </tr>
    <tr>
      <td>h</td>
      <td>Planck'sches Wirkungsquantum [J s]</td>
    </tr>
    <tr>
      <td>c</td>
      <td>Lichtgeschwindigket [m/s]</td>
    </tr>
    <tr>
      <td>λ</td>
      <td>Wellenlänge [m]</td>
    </tr>
    <tr>
      <td>k</td>
      <td>Boltzmann Konstante</td>
    </tr>
    <tr>
      <td>T</td>
      <td>Objekt Temperatur [K]</td>
    </tr>
    </tbody>
  </table>
</figure>

<p>
  Da jede FLIR Kamera unterschiedlich misst, werden sie auf einen Schwarzkörper kalibriert. Das Resultat sind
  untenstehende Werte. Die Werte aus der Kalibrierung substituieren die Konstanten Teile des planckschen
  Strahlungsgesetz (siehe Tabelle unten). Für alle Berechnungen müssen diese neuen Konstanten verwendet werden. Sie
  sind in den EXIF-Metadaten der Videodatei zu finden.
</p>

<figure>
  <img src="images/research/planck_substitute.svg" style="height:4em;">
  <figcaption>Planck'sches Strahlungsgesetz mit FLIR Konstanten</figcaption>
</figure>

<br>

<figure>
  <table width="300">
    <thead>
    <tr>
      <th>Konstante</th>
      <th>Bedeutung</th>
    </tr>
    </thead>
    <tbody>
    <tr>
      <td>Planck R1</td>
      <td><img src="images/research/planck_R1.svg" style="height:1em;"></td>
    </tr>
    <tr>
      <td>Planck R2</td>
      <td><img src="images/research/planck_R2.svg" style="height:1em;"></td>
    </tr>
    <tr>
      <td>Planck R</td>
      <td><img src="images/research/planck_R.svg" style="height:2em;"></td>
    </tr>
    <tr>
      <td>Planck B</td>
      <td><img src="images/research/planck_B.svg" style="height:2em;"></td>
    </tr>
    <tr>
      <td>Planck F</td>
      <td>Ersetzt die Konstante 1 in der Planck Formel mit einem Wert im Bereich [0.5, 2].</td>
    </tr>
    <tr>
      <td>Planck O</td>
      <td>Offset Rohdaten.</td>
    </tr>
    <tr>
      <td>S</td>
      <td><img src="images/research/planck_O.svg" style="height:1em;"> 16 Bit A/D Counts.</td>
    </tr>
    </tbody>
  </table>
</figure>

<p>
  Aufgelöst nach Temperatur ergibt das folgende Formel:
</p>

<figure>
  <img src="images/research/planck_flir_temp.svg" style="height:4em;">
  <figcaption>Planckgesetz umgeformt nach Temperatur mit FLIR Konstanten</figcaption>
</figure>

<p>
  Diese Formel findet sich auch in den <a href="#ref-flir-temp-doc">FLIR Unterlagen</a> und bestätigt somit die
  Korrektheit dieser Herleitung. Die Herleitung, von den User im <a href="#ref-exiftool-forum">ExifTool Forum</a>
  konnten wir so ebenfalls nachvollziehen und bestätigen. Auch fanden wir eine <a href="#ref-raw2temp">Implementation in
  R von Glenn J. Tattersall</a> die sogar noch die Atmosphäre korrekt in der Temperaturberechnung berücksichtigt. Mit
  einem Prototypen in Matlab haben wir stichprobenartig für eine Infrarotaufnahme die Temperatur mehrerer Punkte
  gemessen und mit den Temperaturen im offiziellen Analysetool von FLIR (FLIR Tools) verglichen. Die Abweichungen lagen
  unter 1%.
</p>

<p>
  Mit diesen beiden Formeln kann nun W<sub>refl</sub> und W<sub>atm</sub> und schlussendlich die Temperatur
  T<sub>obj</sub> berechnet werden.
</p>

<figure>
  <img src="images/research/planck_wrefl.svg" style="height:4em;">
  <figcaption>Gleichung W<sub>refl</sub></figcaption>
</figure>

<br>
<br>

<figure>
  <img src="images/research/planck_watm.svg" style="height:4em;">
  <figcaption>Gleichung W<sub>atm</sub></figcaption>
</figure>

<p>
  Wenn wir uns die ursprüngliche Messformel nochmals ansehen, sehen wir, dass nun alle Variablen bekannt sind und
  W<sub>obj</sub> berechnet werden kann.
</p>

<figure>
  <img src="images/research/object_emissivity.svg" style="height:4em;">
  <figcaption>Ausstrahlung Objekt</figcaption>
</figure>

<h5>Berechnung Temperatur aus Ausstrahlung</h5>

<p>
  Die Objekttemperatur T<sub>obj</sub> kann nun ganz einfach mit dem planckschen Strahlungsgesetz berechnet werden.
</p>

<figure>
  <img src="images/research/planck_Tobj.svg" style="height:4em;">
  <figcaption>Gleichung T<sub>obj</sub></figcaption>
</figure>

<!-- References -->

<p id="ref-flir-theory" class="reference-item">
  <span class="ref">FLIR Systems Inc.: Infrared Theory, 04.02.2015</span>
  <a href="http://www.flir.com" target="_blank" rel="noopener">http://www.flir.com</a>
</p>

<p id="ref-planck-wiki" class="reference-item">
  <span class="ref">Planck'sches Strahlungsgesetz</span>
  <a href="https://de.wikipedia.org/wiki/Plancksches_Strahlungsgesetz" target="_blank" rel="noopener">https://de.wikipedia.org/wiki/Plancksches_Strahlungsgesetz</a>
  <span class="retrieved">19. Oktober 2017</span>
</p>

<p id="ref-wien-wiki" class="reference-item">
  <span class="ref">Wien'sches Verschiebungsgesetz</span>
  <a href="https://de.wikipedia.org/wiki/Wiensches_Verschiebungsgesetz" target="_blank" rel="noopener">https://de.wikipedia.org/wiki/Wiensches_Verschiebungsgesetz</a>
  <span class="retrieved">19. Oktober 2017</span>
</p>

<p id="ref-stefan-boltzmann-wiki" class="reference-item">
  <span class="ref">Stefan-Boltzmann-Gesetz</span>
  <a href="https://de.wikipedia.org/wiki/Stefan-Boltzmann-Gesetz" target="_blank" rel="noopener">https://de.wikipedia.org/wiki/Stefan-Boltzmann-Gesetz</a>
  <span class="retrieved">19. Oktober 2017</span>
</p>

<p id="ref-minkina-dudzik" class="reference-item">
  <span class="ref">Prof. Minkina W., Dudzik S: Infrared Thermography: Errors and Uncertainties, Wiley, 2009</span>
</p>

<p id="ref-flir-temp-doc" class="reference-item">
  <span class="ref">FLIR A65 Kamerasteuerung Dokumentation</span>
  <a href="http://80.77.70.144/DocDownload/Assets/dl/t559775-en-us.pdf" target="_blank" rel="noopener">http://80.77.70.144/DocDownload/Assets/dl/t559775-en-us.pdf</a>
  <span class="retrieved">6. Januar 2018</span>
</p>

<p id="ref-exiftool-forum" class="reference-item">
  <span class="ref">ExifTool Forum: FLIR Temperaturberechnung</span>
  <a href="http://u88.n24.queensu.ca/exiftool/forum/index.php/topic,4898.msg23944.html#msg23944" target="_blank"
     rel="noopener">http://u88.n24.queensu.ca/exiftool/forum/index.php/topic,4898.msg23944.html#msg23944</a>
  <span class="retrieved">6. Januar 2018</span>
</p>

<p id="ref-raw2temp" class="reference-item">
  <span class="ref">Tattersall, G.J.: Thermimage: Thermal Image Analysis, raw2temp.R</span>
  <a href="https://rdrr.io/cran/Thermimage/src/R/raw2temp.R" target="_blank" rel="noopener">https://rdrr.io/cran/Thermimage/src/R/raw2temp.R</a>
  <span class="retrieved">14. Januar 2018</span>
</p>

  <h3>FLIR Formate</h3>

<p>
  Da wir mit den Rohdaten der FLIR Kamera arbeiten, mussten wir uns zuerst umfangreich über deren Format informieren.
  Die Informationen des Herstellers sind nur spärlich und verstreut vorhanden. Einige Erkenntnisse erhielten wir durch
  Reverse-Engineering. In diesem Kapitel sind die wichtigen Erkenntnisse festgehalten.
</p>

<h4>Format</h4>

<h5>Rohdaten</h5>

<p>
  Die Rohdaten sind im sogenannten Mono14 Format. Jeder Pixelwert ist LSB aligned und in den Bits 0-13 enthalten. Bits
  14 und 15 sind immer 0. Die Kamera nimmt also 16-Bit Graustufen Bilder auf. Die Pixelwerte wiederum repräsentieren
  eine Strahlungsleistung, wie das im Kapitel <a href="#infrarot-theorie">IR-Theorie</a> beschrieben ist. Durch die
  Infrarotstrahlung wird in der Kamera eine Ausgangsspannung erzeugt. Diese wird in A/D Counts umgewandelt. Mit den
  Rohdaten erhalten wir diese A/D Counts. In der Temperaturberechnung von FLIR <code>Temperature (in Kelvin) = B /
  log(R / (S - O) + F)</code> sind die A/D Counts mit S bezeichnet. Die A/D Counts haben ein Offset O, der in den
  Metadaten des Videos enthalten ist.
</p>

<h5>Sensor Gain Modes</h5>

<p>
  Die FLIR Kamera hat zwei Sensor Gain Modes. Diese bestimmen den Temperaturbereich in welchem gemessen wird. Für
  unseren Anwendungsfall wählen wir den HighGainMode. Da der Temperaturbereich in diesem Mode kleiner ist, haben wir
  eine höhere Auflösung in den A/D Counts.
</p>

<figure>
  <pre><code>Auflösung = 160°C / 2<sup>16</sup> = 0.01°C (exemplarische Darstellung)</code></pre>
</figure>

<figure>
  <table>
    <thead>
    <tr>
      <th>Sensor Gain Mode</th>
      <th>Temperaturbereich</th>
    </tr>
    </thead>
    <tbody>
    <tr>
      <td>HighGainMode</td>
      <td>–25 to +135°C</td>
    </tr>
    <tr>
      <td>LowGainMode</td>
      <td>–40 to +550°C</td>
    </tr>
    </tbody>
  </table>
  <figcaption>FLIR Sensor Gain Modes</figcaption>
</figure>

<h5>Temperatur Linear Mode</h5>

<p>
  Eine wichtige Einstellung die vorzunehmen ist, ist die TemperaturLinearMode. Die Kamera kann nämlich lineare Signal-
  oder Temperaturdaten liefern. Die Signaldaten sind die wirklichen Rohdaten, die die Strahlungsleistung repräsentieren.
  Die linearen Temperaturdaten dagegen wären schon umgerechnete Temperaturen aus den Rohdaten. Dies bedingt das
  Konfigurieren von Parameter für die Temperaturberechnung auf der Kamera selber. Wir präferieren die wirklichen
  Rohdaten, da uns das mehr Kontrolle über die Berechnung der Temperatur gibt. Dies vor allem, weil wir unser eigenes
  Analyse-Tool / Viewer für die IR-Aufnahmen schreiben. So können wir die Parameter zur Temperaturberechnung dynamisch
  anpassen.
</p>

<h4>Header</h4>


<p>
  Glücklicherweise sind wir nicht die ersten die sich damit beschäftigen. Im <a href="#ref-exiftool">ExifTool von Phil
  Harvey</a> gibt es bereits einige Tags die aus den Metadaten ausgelesen werden.
</p>

<figure>
  <table>
    <tbody>
    <tr>
      <td>ExifTool Version Number</td>
      <td>10.75</td>
    </tr>
    <tr>
      <td>Emissivity</td>
      <td>1.00</td>
    </tr>
    <tr>
      <td>Object Distance</td>
      <td>0.00 m</td>
    </tr>
    <tr>
      <td>Reflected Apparent Temperature</td>
      <td>21.9 C</td>
    </tr>
    <tr>
      <td>Atmospheric Temperature</td>
      <td>21.9 C</td>
    </tr>
    <tr>
      <td>Relative Humidity</td>
      <td>0.0 %</td>
    </tr>
    <tr>
      <td>Planck R1</td>
      <td>16556</td>
    </tr>
    <tr>
      <td>Planck B</td>
      <td>1428</td>
    </tr>
    <tr>
      <td>Planck F</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Atmospheric Trans Alpha 1</td>
      <td>0.006569</td>
    </tr>
    <tr>
      <td>Atmospheric Trans Alpha 2</td>
      <td>0.012620</td>
    </tr>
    <tr>
      <td>Atmospheric Trans Beta 1</td>
      <td>-0.002276</td>
    </tr>
    <tr>
      <td>Atmospheric Trans Beta 2</td>
      <td>-0.006670</td>
    </tr>
    <tr>
      <td>Atmospheric Trans X</td>
      <td>1.900000</td>
    </tr>
    <tr>
      <td>Planck O</td>
      <td>-207</td>
    </tr>
    <tr>
      <td>Planck R2</td>
      <td>0.046952017</td>
    </tr>
    <tr>
      <td>Frame Rate</td>
      <td>30</td>
    </tr>
    <tr>
      <td>Raw Thermal Image Width</td>
      <td>640</td>
    </tr>
    <tr>
      <td>Raw Thermal Image Height</td>
      <td>512</td>
    </tr>
    <tr>
      <td>Raw Thermal Image Type</td>
      <td>TIFF</td>
    </tr>
    <tr>
      <td>Raw Thermal Image</td>
      <td>(Binary data 655564 bytes, use -b option to extract)</td>
    </tr>
    </tbody>
  </table>
  <figcaption>ExifTool Output einer SEQ Datei (Auszug)</figcaption>
</figure>

<p>
  Diese Angaben werden zum Teil auch in den FLIR Tools dargestellt, wenn man die Datei öffnet. Je nach Wert
  erzeugt dies andere Temperaturwerte.
</p>

<p>
  Das Tag "FrameRate" ist nur für SEQ Dateien relevant. Die anderen Tags erscheinen genauso bei einzelnen Bildern.
  Das ExifTool kann dank den Erkenntnissen von unserem Reverse Engineering diese jetzt auch auslesen. Die das
  FrameRate Tag nicht präsent, kann FLIR Tools die Datei nicht als Video erkennen und abspielen.
</p>

<figure>
  <img src="images/screenshot-flir-tools.jpg">
  <figcaption>Screenshot FLIR Tools</figcaption>
</figure>

<p>
  Man sieht auf der rechten Seite die einstellbaren Parameter, die in die Temperaturgleichung einfliessen. Diese
  sind auch in den Metadaten vorhanden.
</p>

<!-- References -->

<p id="ref-exiftool" class="reference-item">
  <span class="ref">ExifTool von Phil Harvey</span>
  <a href="https://www.sno.phy.queensu.ca/~phil/exiftool/" target="_blank" rel="noopener">https://www.sno.phy.queensu.ca/~phil/exiftool/</a>
  <span class="retrieved">13. Januar 2018</span>
</p>

  <h3>Zugnummererkennung</h3>
<p>
  Während dieser Arbeit kam die Problematik auf, dass wir die Wagen mit der Zugsidentifikationsnummer identifizieren
  möchten. Diese Aufgabe hätte der RFID Reader übernommen, hätten wir einen erhalten. Wir kamen dann auf die Idee,
  die auf den Fahrzeugen aufgedruckte Nummer auf dem Bild zu erkennen und so den Wagen zu identifizieren.
</p>

<p>
  Jeder Wagen muss zwingend eine aufgedruckte Nummer nach UIC Norm haben. Die Länge der Nummer ist unterschiedlich.
  Es gibt Nummern wie 50 85 2673 106-4, 94 85 0500 037-2 oder auch verkürzt nur 802 710-4. Alle Nummern haben am
  Schluss eine Prüfziffer.
</p>

<figure>
  <img src="images/research/ocr/uic-kennzeichnung.png">
  <figcaption>UIC Kennzeichnungsnummer</figcaption>
  <span class="source">Quelle:
    <a href="#ref-wikipedia-uic-kennzeichnung">Wikipedia: UIC-Kennzeichnung der Triebfahrzeuge</a></span>
</figure>

<p>
  Die Berechnung der Prüfziffer haben wir anhand einer <a href="#ref-eisenbahn-katalog">Eisenbahn-Fan-Website</a>
  rekonstruiert. In C# sieht der Code dazu wie folgt aus:
</p>

<figure class="affix">
  <pre><code>public static int TrainNumberCheckSum(string number)
{
    var sum = 0;
    var multiplier = 2;

    for (var i = number.Length - 1; i != -1; i--)
    {
        var c = number[i];

        if (c >= '0' && c <= '9')
        {
            var zsum = ushort.Parse("" + c) * multiplier;
            sum += zsum % 10 + zsum / 10;
        }

        if (c >= '0' && c <= '9')
        {
            multiplier = 3 - multiplier;
        }
    }

    var checkDigit = (10 - sum % 10) % 10;

    return checkDigit;
}</code></pre>
  <figcaption>Code UIC Prüfzifferberechnung</figcaption>
</figure>

<p>
  Um die Kennzeichnung auf den Bildern zu finden, wenden wir Template Matching an. Die Kennzeichnungen sind in der
  Schrift Helvetica geschrieben. Deswegen haben wir eine Font Map von Helvetica erstellt in dem wir Bilder der einzelnen
  Zeichen in ein grosses Bild eingefügt haben.
</p>

<figure class="affix">
  <img src="images/research/ocr/font-map-helvetica.png">
  <figcaption>UIC Kennzeichnungsnummer</figcaption>
</figure>

<p>
  Als erstes müssen wir die Nummer im Bild finden, damit wir danach die einzelnen Zeichen identifizieren können. Unsere
  Idee war, nach dem Text "SBB" zu suchen, weil dieser, zumindest bei SBB Fahrzeugen, meistens nach der Nummer steht und
  gut gefunden werden kann. Deshalb bauen wir unser Template aus "S", "B" und "B" zusammen und suchen es in der Szene.
  Die Szene, also das Gesamtbild des Fahrzeugs, haben wir vorher in Graustufen umgewandelt und so zugeschnitten, dass es
  in etwa den unteren Wagenbereich abdeckt. Zudem wenden wir ein adaptives Thresholding an.
</p>

<figure class="affix">
  <img src="images/research/ocr/matched-sbb-template.png">
  <figcaption>UIC Kennzeichnungsnummer</figcaption>
</figure>

<p>
  Es kann sein, dass der Text schwarz auf weiss oder weiss auf schwarz gedruckt ist. Deshalb suchen wir das Template
  immer normal und invertiert. Die höhere Trefferwahrscheinlichkeit gewinnt. Danach merken wir uns für den nächsten Schritt,
  ob der Text schwarz oder weiss ist.
</p>

<p>
  Nun können wir die Nummer ausschneiden:
</p>

<figure class="affix">
  <img src="images/research/ocr/cropped-number.jpg">
  <figcaption>Ausgeschnittene Kennzeichnung</figcaption>
</figure>

<p>
  Dadurch sieht man jetzt wie schwierig es ist, diese Nummern zu lesen. Menschen haben bereits Mühe diese zu
  entziffern. Wir schieben nun alle Zahlen über die Nummer um die Templates zu matchen. Wir erhalten für jedes
  Zeichen eine Tabelle mit der Wahrscheinlichkeit pro Zeichen. Mit dieser Tabelle können wir nun überprüfen ob wir eine
  korrekte Nummer gefunden haben. Ist dies nicht der Fall, kommt unser Guesstimator-Algorithmus zum Zug. Dieser
  probiert anhand der Wahrscheinlichkeiten so lange Ziffern auszutauschen bis er eine gültige Nummer gefunden hat.
</p>

<p>
  Die Resultate sind leider ernüchternd. Meistens stimmen eine oder zwei Ziffern nicht. Mit dem Guesstimator
  verschlimmert sich meist das Resultat noch. Auch haben wir bemerkt, dass die meisten Züge in der Nacht oder
  zumindest bei Dunkelheit gewaschen werden. Unsere Aufnahmen mit langer Belichtungszeit lassen kein Lesen der Nummer
  zu. Hinzu kommen Regentropfen auf dem Sichtglas, die die Sicht noch weiter verschlechtern. Deswegen haben wir
  diesen Code auf Eis gelegt und werden diesen Ansatz nicht weiterverfolgen.
</p>

<p>
  Die Aufnahmen-Zeitstempel mit den Waschstrassentabellen abzugleichen wird der bessere Ansatz sein. Jede Waschstrasse
  führt ein Logbuch wann welcher Zug mit welchen Wagen gewaschen worden ist.
</p>

<!-- References -->

<p id="ref-wikipedia-uic-kennzeichnung" class="reference-item">
  <span class="ref">Wikipedia: UIC-Kennzeichnung der Triebfahrzeuge</span>
  <a href="https://de.wikipedia.org/wiki/UIC-Kennzeichnung_der_Triebfahrzeuge" target="_blank" rel="noopener">https://de.wikipedia.org/wiki/UIC-Kennzeichnung_der_Triebfahrzeuge</a>
  <span class="retrieved">17. Januar 2018</span>
</p>

<p id="ref-eisenbahn-katalog" class="reference-item">
  <span class="ref">Berechnung der UIC-Prüfziffern</span>
  <a href="http://www.eisenbahn-webkatalog.de/lexikon/prueffziffer.html" target="_blank" rel="noopener">http://www.eisenbahn-webkatalog.de/lexikon/prueffziffer.html</a>
  <span class="retrieved">17. Januar 2018</span>
</p>


</div>

<!-- References -->

<div class="chapter">
  <h2 class="break">Schlussbemerkung</h2>

  <p>
    Ziel dieser ersten Phase war das Abklären der Machbarkeit und die Evaluation einer geeigneten IR-Kamera. Unser
    eigentliches Ziel, das thermographische analysieren von Zugwagons war aber noch nicht erreicht. Wir haben allerdings ein
    klares Bild unserer Problemstellung erhalten und mit der FLIR A65 IR-Kamera die nötige Hardware, um unsere Pläne für
    die Bachelorthesis zu verwirklichen.
  </p>

  <p>
    Ziel der Bachelorarbeit war das automatisierte Beschaffen von Infrarotdaten und deren Aufarbeitung. Wir konnten auf
    der Arbeit der Voranalyse aufbauen und unserem Prototypen zur Reife verhelfen. Wir haben ein einsatzfähiges Gerät
    entwickelt, dass automatisch vorbeifahrende Züge mit einer Infrarot- und einer Farbkamera aufnimmt. Nach dem
    Hochladen in die Cloud werden die Videos zu entzerrten und rektifizierten Zugpanoramas zusammengesetzt. Experten der
    SBB können sich die Aufnahmen danach im eigens entwickelten Analyse-Tool ansehen, und die Isolation und andere
    Wärmebrücken studieren. Die qualitativ hochwertigen Bilder bilden eine Grundlage für weitere Analysen und können für
    eine Klassifikation von Schäden verwendet werden.
  </p>

  <p>
    Der Auftraggeber, die SBB, ist daran interessiert das Projekt nach der Bachelorarbeit weiterzuführen. In den
    Aufnahmen steckt grosses Potential für sinnvolle Anwendungen im Bereich Predictive Maintenance und der
    Energieeinsparung. Die Vision ungewollte Wärmeverluste mit einem automatischen Verfahren zu klassifizieren scheint
    in greifbarer Nähe und reizt uns sehr mit unserer Arbeit weiter zu fahren.
  </p>
</div>


<h2 class="break">Appendix</h2>
<div class="chapter">
  <h3>Quellenverzeichnis</h3>

<div id="references"></div>

  <h3 class="break">Abbildungsverzeichnis</h3>

<ul id="figures" class="allow-page-break"></ul>

  <h3 class="break">Software</h3>

<p>
  Um die Resultate dieser Arbeit zu erzeugen wurde folgende Software eingesetzt:
</p>

<h4>Services</h4>
<ul>
  <li>Google Drive</li>
  <li>GitHub</li>
  <li>OpenWeatherMap</li>
  <li>Travis</li>
  <li>draw.io</li>
  <li>Metanet Hosting</li>
</ul>
<h4>IDE</h4>
<ul>
  <li>Matlab</li>
  <li>IntelliJ IDEA</li>
  <li>Visual Studio</li>
</ul>
<h4>Tools</h4>
<ul>
  <li>Git</li>
  <li>OpenHardwareMonitor</li>
  <li>Teamviewer</li>
  <li>ExifTool</li>
  <li>FLIR Tools</li>
  <li>Microsoft Office Excel</li>
  <li>Microsoft Office Powerpoint</li>
  <li>Microsoft Office Visio</li>
  <li>Microsoft Project</li>
  <li>PrinceXML</li>
  <li>MariaDB</li>
  <li>Frhed</li>
</ul>
<h4>Frameworks</h4>
<ul>
  <li>OpenCV</li>
  <li>EMGU.CV</li>
  <li>FFmpeg</li>
  <li>FLIR ATLAS SDK</li>
  <li>Basler Pylon SDK</li>
  <li>Laravel</li>
  <li>Angular</li>
  <li>Apollo</li>
  <li>reveal.js</li>
</ul>
<h4>Programmiersprachen / Plattformen</h4>
<ul>
  <li>C#.NET</li>
  <li>Java</li>
  <li>JavaScript</li>
  <li>TypeScript</li>
  <li>nodeJS</li>
  <li>Matlab</li>
  <li>PHP</li>
  <li>SQL</li>
  <li>Windows Batch</li>
  <li>PowerShell</li>
</ul>
<h4>Patterns</h4>
<ul>
  <li>TDD</li>
  <li>Microservices</li>
  <li>PubSub</li>
  <li>GraphQL</li>
</ul>

  <h3 class="break">Glossar</h3>

<!-- this list is alphabetically sorted, if you add to it, make sure it stays sorted -->
<dl>
  <dt>AKW</dt><dd>Atomkraftwerk</dd>
  <dt>API</dt><dd>Application Programming Interface</dd>
  <dt>BFH</dt><dd>Berner Fachhochschule</dd>
  <dt>CLI</dt><dd>Command Line Interface</dd>
  <dt>CV</dt><dd>Computer Vision: Interdisziplinäres Feld das sich mit high-level Verständnis von digitalem Bildmaterial beschäftigt</dd>
  <dt>DPZ</dt><dd>Zürich S-Bahn, Doppelstock-Pendelzug</dd>
  <dt>DTZ</dt><dd>Zürich S-Bahn, Doppelstock-Tribzug</dd>
  <dt>EXIF</dt><dd>Exchangeable Image File Format</dd>
  <dt>FFHS</dt><dd>Fernfachhochschule Schweiz</dd>
  <dt>FLIR</dt><dd>Forward Looking Infrared — aber auch ein amerikanischer Infrarot Kamera Hersteller</dd>
  <dt>FOV</dt><dd>Field Of View</dd>
  <dt>FPA</dt><dd>Focal Plane Array: Sensortyp</dd>
  <dt>FPS</dt><dd>Frames Per Second</dd>
  <dt>FTP</dt><dd>File Transfer Protocol</dd>
  <dt>GenICam</dt><dd>Generic Interface for Cameras Standard</dd>
  <dt>GSM</dt><dd>Global System for Mobile Communications: Mobile Kommunikation Standard</dd>
  <dt>GWh</dt><dd>Gigawatt Stunden, Masseinheit für Energie</dd>
  <dt>HD</dt><dd>High Definition</dd>
  <dt>HFOV</dt><dd>Horizontales FOV</dd>
  <dt>HSV</dt><dd>Hue, Saturation, Value</dd>
  <dt>ICE</dt><dd>Deutsche Bahn, Intercity-Express</dd>
  <dt>IDE</dt><dd>Integrated Development Environment</dd>
  <dt>IP</dt><dd>Internet Protocol</dd>
  <dt>IR</dt><dd></dd>
  <dt>IR</dt><dd>Infrarot</dd>
  <dt>JNI</dt><dd>Java Native Interface: Ermöglicht Anbindung von nativem Code in Java</dd>
  <dt>LTE</dt><dd>Long-Term Evolution: Mobile Kommunikation Standard</dd>
  <dt>NUC</dt><dd>Non Uniformity Correction: Kalibirierungsprozess der FLIR Kamera</dd>
  <dt>OCR</dt><dd>Optical Character Recognition</dd>
  <dt>ORM</dt><dd>Object Relational Model</dd>
  <dt>PubSub</dt><dd>Publish / Subscribe</dd>
  <dt>REST</dt><dd>Representational State Transfer</dd>
  <dt>RFID</dt><dd>Radio-Frequency Identification</dd>
  <dt>RMS</dt><dd>Root Mean Square</dd>
  <dt>SBB</dt><dd>Schweizerische Bundesbahnen</dd>
  <dt>SDK</dt><dd>Software Development Kit</dd>
  <dt>SEQ</dt><dd>Sequence Datei: FLIR Format für rohe Videos</dd>
  <dt>SMA</dt><dd>SubMiniature version A: Ein Antennenanschluss</dd>
  <dt>STA</dt><dd>Single Threaded Apartment: Ermöglicht nachrichtenbasiertes Paradigma für mehrere gleichzeitig laufende Prozesse mit Objekten</dd>
  <dt>SW</dt><dd>Software</dd>
  <dt>TDD</dt><dd>Test Driven Development</dd>
  <dt>TIFF</dt><dd>Tagged Image File Format</dd>
  <dt>UV</dt><dd>Ultraviolet</dd>
  <dt>VFOV</dt><dd>Vertikales FOV</dd>
  <dt>WMI</dt><dd>Windows Management Instrumentation: Infrastruktur für Management von Daten und Operation auf Windows-Systemen</dd>
</dl>

  <h3 class="break">Selbständigkeitserklärung</h3>
<p>
  Wir bestätigen, dass wir die vorliegende Arbeit selbstständig und ohne Benutzung anderer als der im
  Literaturverzeichnis angegebenen Quellen und Hilfsmittel angefertigt haben. Sämtliche Textstellen, die nicht von uns
  stammen, sind als Zitate gekennzeichnet und mit dem genauen Hinweis auf ihre Herkunft versehen.
</p>

<p>
  <br>
  Ort, Datum:
  <br>
  <br>
  <br>
  <br>
  <br>
  Unterschriften:
</p>

</div>



</body>
</html>
